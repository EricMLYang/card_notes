---
tags:
  - my-article
Checkbox 1: true
---
【大數據雜談 - Parquet】

—— 當數據量大，資料格式好壞都嚴重影響處理效率



軟體產業的介面與技術棧概念一直讓我很著迷。



雖然軟體變化很快，但很多技術都是長期演化堆疊而來。



大數據領域就是一個例子。



當數據量大到需要好幾台大電腦才能擺放。



數據到底是怎麼樣的格式放到硬碟裡，就影響重大。



因為你不會希望每次找數據就要等 10 分鐘以上。



而 Parquet 就是一個被廣泛應用的高效檔案格式。



以下快速介紹 Parquet 的基本概念。



之後會想針對比較底層的數據格式做更深的瞭解。



▋大數據工具發展起源

一個企業因應不同目的會有多個系統。



例如人資管理、倉儲系統、製造管理..等。



而為了更近一步優化企業的營運。



以前會建立資料倉儲來進行報表為主的應用。



讓分析師可以結合不同系統的數據來回答重要問題。



但是隨著時間演進。



現在大家希望能更即時、更多分析、處理更多種類數據。



因此發展了很多新工具來因應，其中就包含 Parquet。





 ▋Parquet 基本內涵

你可以把 Parquet 想成一個可以放在硬碟的檔案。



其當然也可以放在最靈活的雲端物件形式的儲存裝置。



首先，Parquet 是 columnar 儲存格式。



其設計上就是讓你可以方便一次取一整欄數據來分析。



因為每欄數據形式通常是差不多的。



因此非常好進行壓縮。



而他也不是真的把一大欄檔案都全放一起。



他還是有層次的分段 (Row Group, column,  Page )。



每段都有一些資訊讓你方便查找。



可以把 Parquet 想成是大圖書館的管理系統。



書分區、分類、分櫃放，同一櫃的書都類似主題。



當你要找書時可以靠這些資訊。



▋Apache Arrow 崛起

因為 Parquet 在大數據領域的成功。



多種上層的分析處理工具都支援 Parquet 檔案。



這導致了一個需求產生。



就是能不能讓不同分析工具的轉換更快一點。



當我用 spark datagrame 做完基本處理後。



想要轉到比較方便做時間序列分析的工具。



因為數據量大導致每次轉換都會造成時間和運算資源浪費。



Apache Arrow 因應而生。



可以先把 Apache Arrow 想成跟 Parquet 很像的數據格式。



但精確來說 Arrow 是一種記憶體內（in-memory）的列式資料格式。



專為高效能跨系統數據交換設計。



只是資料是放在記憶體裡，而不是硬碟。



這樣的搭配讓大數據的分析更加高效。



▋現代的數據平台架構 - Lakehouse

雲端存儲裝置為了達到分佈支援、跟彈性。



通常是用 Object-Base 的儲存裝置。



而這種儲存裝置就跟海一樣可以放任何數據，做任何分析處理。



因此我們稱這樣儲存為 Data Lake。



但缺點就是太過靈活彈性而本質上不像一般系統資料庫可靠。



為了解決這缺點，目前很多會在原端儲存上放 Parquet 檔。



然後又在 Parquet 檔上加上一層儲存層。



主要是讓數據可以有 ACID，可進行類 SQL 的操作。



Lakehouse (Data Lake + Warehouse) 因應而生。



目前我在公司處理數據的主要工具是 Databricks。



其就是用這種架構。



—— 

以上有很多底層細節都很值得去深入了解。



在 AI 當道的世代，惟多了解一些才能好好駕馭 AI。




















