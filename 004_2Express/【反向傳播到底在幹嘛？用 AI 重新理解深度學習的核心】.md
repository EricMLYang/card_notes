---
tags:
  - my-article
Checkbox 1: true
---
【反向傳播到底在幹嘛？用 AI 重新理解深度學習的核心】



最近想幫團隊成員說明什麼是 AI，

久違地重看了深度學習的文章。

自從 LLM 問世後，大家都在拼應用，

已經很少在訓練模型了。



回想起當初工作好幾年後，毅然決然回到學校，

去上當時最紅的林軒田、李宏毅的課（上課跟開演唱會一樣）。

非資電本科加上久沒讀書，當初跟這些概念比拼時真的很痛苦。

近期 AI 能力大幅提升，個人有個野心：

透過 AI 把以前沒讀好的書重讀一遍，

甚至想挑戰量子力學、高等微積分等科目。

理論上靠 AI 應該可以比以前純 K 書快很多。

想起以前跟反向傳播（Backpropagation）這個觀念戰了好久，

如今也忘得差不多了。



就來試試 AI 是否能讓我快速重新掌握這個概念。



▋反向傳播（Backpropagation）到底在做什麼？

深度學習的模型有幾百層、上億甚至上千億個參數。

但它們其實只做一件事：

調整參數，讓模型變得比較準。



但怎麼知道每個參數該怎麼調？往哪裡調?調多少？

不能亂改，因為：

．參數太多，亂改一下都會出事

．模型太深，一點點的改動都可能放大到很糟糕

．每次重新試一次模型都要非常昂貴的計算

所以現代深度學習的關鍵問題是：

如何知道某個參數改一點點，會讓結果變好還是變壞？





▋微分（Gradient）是答案

微分告訴我們：當某個參數 W 改一點點，錯誤（Loss）會怎麼變。

．微分為正 → 往這方向改會變更糟

．微分為負 → 往這方向改會變更好

．數值越大 → 這個參數越該調整

這個「對所有參數計算微分」的結果，就叫**梯度（gradient）**。





▋暴力計算不可行

聽起來很簡單，但有個大麻煩：參數可能有幾億個。

要是用最暴力的方式：把每個參數微調一下，

重跑模型， 看 Loss 怎麼變，

這意味著：

．有 10 億個參數 → 要重跑 10 億次模型

．每次都要重新計算好幾百層、上千個矩陣乘法

．任何 GPU、TPU 甚至量子電腦都受不了

這完全不可能。





▋反向傳播的解法

反向傳播不是魔法，它的本質其實非常務實：

用一個聰明的方法，把「所有參數的微分」一次算完。

重點是「一次」。



▋步驟一：正向傳播（Forward Pass）

也就是模型通常做推論那樣：

輸入 → 第一層 → 第二層 → … → 最後一層 → 預測結果，再算 Loss。

但真正重要的是：過程中每一層的中間結果都會被記住。



這些中間值包含：

．每層的輸入（x）

．每層的線性組合（Wx+b）

．每層 activation 的輸出（ReLU、sigmoid…）

．dropout 隨機 mask

．batchnorm 的 mean 和 variance

．attention 的 softmax 結果

為什麼要記住？ 因為微分（梯度）需要用到這些值。

例如 ReLU：

．a > 0 → derivative = 1

．a ≤ 0 → derivative = 0

這就需要 forward 時的 a。





▋步驟二:從 Loss 往回跑（Backward Pass）

反向傳播利用的是**（Chain Rule）**來更高效的算梯度，

∂L/∂W = ∂L/∂h · ∂h/∂a · ∂a/∂W

每一層的導數全部都依賴 forward 的中間值。

反向跑的步驟是：

從 Loss 開始，算 ∂L/∂h

1\.用 forward 中的 h 算 activation 的導數

2\.用 forward 中的 x 算線性層的導數

3\.把「錯誤訊號」往前一層傳

4\.重複直到第一層

過程中，每一層都只做：**local gradient × upstream gradient**

這樣就能：**一次把所有參數的梯度算出來，成本只比 forward 多一倍。**



▋為什麼這麼重要？

如果沒有反向傳播，

深度學習根本無法訓練，

GPT-3、GPT-4、AlphaGo、ImageNet 模型全部都不會存在，

這沒有這一波 AI 潮了**。**



▋最白話的總結

反向傳播就是：先把模型的所有中間計算記起來，

再用鏈式法則從後往前，

把每個參數對錯誤的影響一次算出來。



這讓訓練一個百億參數的模型變成可能。


