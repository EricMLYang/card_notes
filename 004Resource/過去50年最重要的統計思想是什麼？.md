---
tags:
  - my-article
Checkbox 1: false
---
過去50年最重要的統計思想是什麼？

<https://www.facebook.com/share/p/15UhUjSsCo/>

筆記：過去50年最重要的統計思想是什麼？

統計學家Andrew Gelman跟他的合作者Aki Vehtari先前寫了一篇文章叫過去50年最重要的統計思想是什麼？（What are the most important statistical ideas of the past 50 years?》。

本來Andrew Gelman投稿時是待討論的稿件，希望跟編輯群再多聊些，結果陰錯陽差，編輯給他們直接刊出去了（big name lol..)。最近他部落格聊到這篇文章，我就把文章找出來看一下。

在這篇論文中，作者 Andrew Gelman 和 Aki Vehtari 回顧了幾個研究領域，認為他們從根本上塑造了統計學和資料科學（甚至是今日的AI)。

儘管這些思想早有起源，受過去半世紀計算能力進步的推動，它們已經成熟並成為該領域的核心。

這些成果了包括了：

1\.Counterfactual causal inference (反事實因果推斷)：

此框架超越了「相關不等於因果」的傳統觀念。

從Rubin等人以來，提供了一個結構化的方法，透過以潛在結果（potential outcomes---平行世界裡的你在幹麼），來嚴謹地定義因果問題，從而思考並估計來自實驗數據和觀測數據的因果效應。

Andrew Gelman沒提到的是，經濟學家將這概念拿來，用在「自然實驗」上，因而走了非常遠，包含了最近的諾貝爾獎得主Daron Acemoglu 跟James Robinson等人用「工具變數法」來談制度的長期效果，或我老闆Melissa Dell利用制度邊界來做斷點迴歸，都是承接自這一浪潮。

有趣的是，我另一個老闆Ed Glaeser是最早反對Daron Acemoglu 跟James Robinson的研究的人，是為後話。

2\. Bootstrapping and simulation-based inference (自助法與模擬為本的推論)：

從Bradley Efron於1979年在《Annals of Statistics》上發表Bootstrapping後，要計算標準差但硬剛不出來？就Bootstrapping。直接boot不出來？要不要試試更狂野一點的wild bootstrapping?

不過作者們認為這觀念之所以重要，不是因為可以標準差變得好算，而是指在觀念上，人類可是用密集的計算來取代數學分析。自助法是其中的一個關鍵例子，該方法透過重複對數據集進行重抽樣來近似抽樣分佈。此領域還包括其他模擬方法，使得分析複雜模型成為可能。

不過Andrew沒有提到，如果是這觀念的轉移的話，或許更多是馮紐曼在發明蒙地卡羅法的時候？

之前讀到一本書提到，因為要直接得到核反應的的closed-form太困難，於是乾粹用蒙地卡羅來算，馮紐曼因而留下那句有名的“I am thinking about something much more important than bombs; I am thinking about computers.”

3\. Overparameterized models and regularization (過多參數化模型與正規化)：

這算是機器學習的起源之一，這概念是悠關於「讓我們用大量的參數來fit巨量資料吧。但這麼多的參數，哪些重要？我們會不會又過度fitting一部份的資料？

這裡延伸出來的方法，比方說，Lasso regression，就是要透過懲罰項來避免overfitting。

作為一種高度彈性模型的實踐，極端狀況下，有時模型的參數比數據還多，於是我們可以透過正規化技術來控制模型以防止過度擬合。這種方法催生了許多強大的預測模型，如類神經網絡、lasso 等等。

正規化在我哈佛的lab也扮演了重要的角色。

許多深度學習模型的訓練，可以在一樣的資料跟運算，以及幾乎一樣的演算法之下，提升十倍以上的精確程度---可能只是因為我們對loss function做了某些小小的正規化。 算進階通靈。

4\. Bayesian multilevel models (貝氏多層次模型)：

這類也稱為階層式模型（hierarchical models），這些模型用於結構化數據，例如panel data或統合分析（meta-analyses）的數據，應將「經驗（實證）貝氏」(empirical Bayes) 的想法形式化，透過整合資訊和部分共享（partially pooling）估計值，從而得到更穩定和合理的推斷結果。

文章沒提到太多應用，我這邊講一下：經濟學上常見的應用，就是做Meta-Analysis，比方說你要統整20篇不同RCT的結果，每個RCT都有500個受試者，你有RCT之間的差異（m=20)，給定一個RCT，你也有受試者間的差異（n=500)，這種資料就可以做階層式模型，因為你有不同RCT間的統計差異，也有RCT內部受試者的統計差異。

5\. Generic computation algorithms (一般泛用的計算演算法)：

作者認為，功能強大且在各種脈絡下都通用的演算法（如 Gibbs sampler、Hamiltonian Monte Carlo 和變分推斷）的發展，是現代統計學的關鍵推動力。這些演算法使從業人員能夠擬合各種複雜的模型，而無需為每個模型開發客製化的計算方法。

6\. Adaptive decision analysis (適應性決策分析)：

這裡的觀念就是邊收新樣本，調整實驗設計（Adaptive），然後一邊做出決定（比方說，平台商決定該不該調漲價格。）

這裡的觀念是是此領域將統計模型與決策過程連結起來--我這一輪應該多搜集幾個資料、多做幾個實驗？

這概念在商業上被普遍應用到了科技公司（像 A/B 測試），也影響了強化學習和貝氏最適化等方法。

7\. Robust inference (穩健推斷)：

Robust大概是最不robust的術語了，經濟學家跟統計學家對於什麼是Robust的理解可能完全不同。

Andrew這邊所指的Robust inference，是指那些「開發和使用那些對其假設的違反不過度敏感的統計方法。」

其重點是創建即使在面對離群值或模型假設錯誤等問題時，依然能良好運作的模型或統計程序。 

Andrew等人有提到經濟學上最大的應用，就是White's Robust Standard errors （等於STATA的reg y x, robust)。

8 Exploratory data analysis (EDA, 探索性資料分析)：

這觀念由 John Tukey 在1960提倡，也間接催生了S軟體（R軟體的前身）的誕生。在那之前，很多圖形都是手刻的。（事實上，我最近才知道一些哈佛數學系的老教授可能還是傾向手刻................他們版本更新最多到了用Power Point製圖。)

EDA 強調使用圖形化方法來探索數據、發現模式並產生假說。這種方法推動了統計領域朝向一個更開放、更注重新發現的方向發展，圖形化或資料視覺化，已成為理解數據與複雜模型之間關係不可或缺的一部分。

以上這八個觀念：反事實因果推斷、自助法與模擬為本的推論、過參數化模型與正規化、貝氏多層次模型、一般泛用的計算演算法、適應性決策分析、穩健推斷、探索性資料分析（特別是資料視覺化），是Andrew Gelman跟他的合作者Aki Vehtari認為過去半世紀最重要的統計觀念。

我自己是覺得統計研究雖然少，但fat tail的觀念很重要，因為fat tail 可以用來解釋財富不均（皮凱提的二十一世紀資本論）、生產力的分布、甚至是金融危機的黑天鵝事件，至少fat tail不會比Adaptive decision analysis不重要就是了。

---

## 卡片拆解結果

### A. 主脈絡
Andrew Gelman 與 Aki Vehtari 列舉過去50年統計學的八個關鍵思想，核心論證是：這些思想之所以重要，不僅因為它們提供了新技術，而是從根本上改變了統計思維模式——從「精確數學推導」轉向「計算密集的模擬」、從「單一模型假設」轉向「正規化的高維空間」、從「描述相關」轉向「因果推斷」。這些轉變由計算能力推動，但更關鍵的是它們為現代資料科學、AI、決策系統奠定了方法論基礎。作者認為這些概念的價值在於它們的「組裝性」——可以在不同領域反覆應用、互相結合。

---

### B. 卡片（Zettel）

#### 序號 1
- **標題**：反事實因果推斷從「相關不等於因果」到「潛在結果框架」
- **類型**：Model
- **概念**：反事實因果推斷（Counterfactual causal inference）透過「潛在結果」（potential outcomes）概念，將因果問題形式化為「平行世界的你會怎樣」。這不僅是方法論（如工具變數法、斷點迴歸），更是思維框架的轉變——從「X 與 Y 相關」進化到「如果改變 X，Y 會如何變化？」經濟學家將此應用於自然實驗（如制度邊界研究），嚴謹地從觀測資料估計因果效應。關鍵在於定義清楚「反事實」（什麼沒發生但可能發生），而非只看到相關性。
- **重要性**：這是從描述性統計到決策性分析的核心橋樑，可與實驗設計、A/B測試、政策評估等主題組裝。
- **邊界/反例**：需要強假設（如排除性約束、平行趨勢），且在複雜系統（多重干預、長期效應）下難以識別真實因果。工具變數可能失效、自然實驗的邊界條件常被質疑。
- **可連結關鍵詞**：#因果推斷 #實驗設計 #決策科學 #潛在結果 #自然實驗 #工具變數法 #A/B測試 #政策評估

---

#### 序號 2
- **標題**：用計算取代數學——從 Bootstrap 到蒙地卡羅的思維轉變
- **類型**：Principle
- **概念**：Bootstrapping（1979 Efron）和蒙地卡羅法的核心不是「讓標準差好算」，而是一個更深層的轉變：**用密集計算取代解析解**。當數學推導太複雜（如核反應、複雜模型的抽樣分佈），不硬剛 closed-form，改用模擬來逼近。馮紐曼的名言「電腦比炸彈重要」正體現這思維——計算能力重新定義了統計學的可能邊界。這開啟了現代統計學從「數學證明驅動」到「計算實驗驅動」的典範轉移。
- **重要性**：這是方法論「槓桿點」——解鎖了過去無法處理的複雜模型（如深度學習、貝氏推斷），是後續多個統計思想的基礎。
- **邊界/反例**：計算成本仍是瓶頸；模擬結果的收斂速度不一定可控；某些情況下解析解更可解釋、更快速（如簡單線性模型）。
- **可連結關鍵詞**：#計算統計 #模擬方法 #Bootstrapping #蒙地卡羅 #計算密集 #典範轉移 #電腦運算

---

#### 序號 3
- **標題**：正規化是「通靈」——在高維空間中控制模型複雜度
- **類型**：Heuristic
- **概念**：過參數化模型（參數比資料還多）聽起來瘋狂，但透過正規化（如 Lasso、Dropout、L2 penalty）可防止過度擬合。正規化的本質是「對 loss function 加入先驗信念」，告訴模型「哪些參數應該小、哪些結構應該簡單」。原文提到實務經驗：同樣資料、演算法，僅調整正規化設定，精確度可提升 10 倍以上——這是「進階通靈」，也是深度學習模型訓練的核心技術。關鍵是找到過擬合與欠擬合之間的平衡點。
- **重要性**：這是「槓桿型調整」——小改動（正規化項）帶來巨大效果，適用於模型調優、特徵選擇、神經網路訓練。
- **邊界/反例**：正規化參數（如 lambda）需要調參；不當正規化會損失重要訊號；某些問題（如因果推斷）不適合過度正規化，會扭曲因果關係。
- **可連結關鍵詞**：#正規化 #過度擬合 #Lasso #深度學習 #模型調優 #高維統計 #先驗信念 #通靈 #Loss_Function

---

#### 序號 4
- **標題**：階層式模型（Hierarchical Models）的「部分共享」智慧
- **類型**：Model
- **概念**：貝氏多層次模型（或稱階層式模型）處理「有結構的資料」（如 panel data、Meta-Analysis）——既有群組間差異，也有群組內差異。核心機制是「partial pooling」（部分共享）：不完全獨立估計每個群組（太不穩定），也不完全合併所有群組（忽略差異），而是讓群組間「借力」。例如 20 篇 RCT 的 Meta-Analysis，可同時估計「RCT 間差異」與「受試者內差異」，得到更穩定、更合理的推斷。
- **重要性**：這是「資訊整合」的系統化方法，可應用於 A/B 測試、多市場分析、個人化推薦（群組與個體的平衡）。
- **邊界/反例**：需要群組間有某種相似性（否則 pooling 會誤導）；模型設定錯誤（如錯估變異來源）會傳播偏差；計算成本較高。
- **可連結關鍵詞**：#階層模型 #貝氏統計 #Meta分析 #Panel_Data #Partial_Pooling #經驗貝氏 #資訊整合

---

#### 序號 5
- **標題**：通用演算法讓統計學「工程化」
- **類型**：Principle
- **概念**：Gibbs sampler、Hamiltonian Monte Carlo、變分推斷等「一般泛用的計算演算法」的重要性在於：它們讓從業者能擬合複雜模型，**無需為每個模型客製化計算方法**。這如同軟體工程的「框架」——你不需要每次都重新寫底層邏輯，直接套用通用演算法即可。這讓統計建模從「手工藝」變成「可規模化的工程流程」，大幅降低了應用門檻，也是 Stan、PyMC 等工具興起的基礎。
- **重要性**：這是「平台化思維」在統計學的體現——通用演算法是可重複使用的基礎設施，讓複雜分析變得可工程化。
- **邊界/反例**：通用演算法未必最快（客製化方法可能更高效）；某些特殊模型仍需專門設計；收斂診斷、調參仍需專業知識。
- **可連結關鍵詞**：#通用演算法 #工程化 #平台思維 #MCMC #變分推斷 #Stan #PyMC #可規模化

---

#### 序號 6
- **標題**：適應性決策分析——邊收資料邊調整實驗的系統思維
- **類型**：Heuristic
- **概念**：適應性決策分析（Adaptive decision analysis）將統計模型與決策過程連結：**這一輪應該多收集幾個資料？多做幾個實驗？還是直接做決定？** 這不是一次性的「先收資料→再分析→最後決策」，而是動態調整——根據當前結果調整實驗設計（如多臂老虎機的 Explore vs Exploit）。應用於 A/B 測試、強化學習、貝氏最適化。核心是「資訊價值」概念：每個新樣本的價值取決於它能減少多少決策不確定性。
- **重要性**：這是「決策系統」的核心思維，可與 DecisionOps、實驗平台、資料驅動決策等主題組裝。
- **邊界/反例**：需要清楚的效用函數（business metric）；實務中組織流程常無法快速調整實驗；短期最佳化可能損害長期探索。
- **可連結關鍵詞**：#適應性設計 #決策科學 #A/B測試 #強化學習 #貝氏最適化 #資訊價值 #DecisionOps #Explore_Exploit

---

#### 序號 7
- **標題**：穩健推斷——對錯誤假設的防禦性設計
- **類型**：Warning
- **概念**：穩健推斷（Robust inference）的核心是「開發對假設違反不過度敏感的方法」——即使模型假設錯了（有離群值、分佈不正態、異質變異數），推斷結果仍然大致可用。經濟學上最知名的應用是 White's Robust Standard Errors（STATA 的 `reg y x, robust`）。這是一種「防禦性設計」思維：不假設完美世界,而是假設現實總有雜訊、異常、模型誤設，建構仍能運作的方法。
- **重要性**：這是「反脆弱」在統計學的體現——不追求完美模型，而是追求「就算錯了也不會完全失效」的韌性。
- **邊界/反例**：穩健方法通常犧牲效率（更保守的估計）；某些 robust 方法仍對特定違反敏感（如極端離群值）；過度 robust 可能掩蓋真實訊號。
- **可連結關鍵詞**：#穩健推斷 #防禦性設計 #反脆弱 #Robust_SE #離群值 #模型誤設 #異質變異數

---

#### 序號 8
- **標題**：EDA（探索性資料分析）從手工繪圖到程式化視覺化
- **類型**：Principle
- **概念**：John Tukey 在 1960 年代提倡 EDA，強調「用圖形探索資料、發現模式、產生假說」，而非一開始就套模型。這催生了 S 語言（R 的前身），讓統計從「手刻圖表」進化到「程式化視覺化」。EDA 的價值在於它是「開放式探索」而非「驗證式分析」——你不知道會找到什麼,但透過視覺化快速掃描資料,能發現意外模式、離群值、資料品質問題。現代資料科學的第一步幾乎都是 EDA。
- **重要性**：這是「資料驅動發現」的起點,可與資料品質檢查、特徵工程、假說生成等主題組裝。
- **邊界/反例**：過度依賴 EDA 可能導致「事後找規律」（p-hacking）；視覺化可能誤導（如軸線調整、櫻桃挑選）；高維資料難以視覺化。
- **可連結關鍵詞**：#探索性分析 #資料視覺化 #EDA #John_Tukey #假說生成 #資料品質 #R語言 #開放式探索

---

### C. 連結建議（組裝藍圖）

**因果與決策鏈**：序號 1（反事實因果推斷）+ 序號 6（適應性決策分析）→ 可組裝成「從因果識別到動態決策」的完整框架（如何用因果模型指導實驗設計）。

**計算典範轉移鏈**：序號 2（計算取代數學）+ 序號 5（通用演算法）→ 說明統計學如何從「數學推導」演化到「工程化流程」，這也是現代 AI/ML 的方法論基礎。

**模型複雜度管理鏈**：序號 3（正規化）+ 序號 7（穩健推斷）→ 都在處理「模型與現實的落差」,前者是過擬合的防禦,後者是假設違反的防禦。

**資料探索到建模鏈**：序號 8（EDA）+ 序號 4（階層式模型）→ 從探索資料結構（發現群組差異）到建模群組關係（partial pooling）。

**實務應用整合**：序號 3（正規化）+ 序號 6（適應性決策）+ 序號 7（穩健推斷）→ 可組裝成「ML 模型在生產環境的完整生命週期」：正規化調優 → 線上 A/B 測試 → 穩健評估。