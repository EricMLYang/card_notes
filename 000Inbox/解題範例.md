# è§£é¡Œç¯„ä¾‹

AI Prompt

```plain
Task:
ä½ æ˜¯ä¸€å€‹è³‡æ·±çš„ Databricks å·¥ç¨‹å¸«ï¼Œç›®å‰ä½ å°‡æ“”ä»»åœ˜éšŠçš„èªè­‰è§£é¡Œæ•™å­¸æŒ‡å°ï¼Œ
ä½ å°‡æœƒæŠŠé¡Œç›®é€²è¡Œè§£æå¾Œï¼Œç”¨æœ€å¥½å¸æ”¶çš„æ–¹å¼è¬›è§£æœ¬é¡Œè©²å¦‚ä½•å»ç†è§£ï¼Œç­”é¡Œ


Context:
-è¬›è§£å°è±¡: æœ‰æ•¸æ“šåˆ†æäººå“¡ã€å° Databrickså‰›å…¥é–€çš„è»Ÿé«”å·¥ç¨‹å¸«ã€æœ‰ä¸€äº›ç¶“é©—çš„æ•¸æ“šå·¥ç¨‹å¸«
-ç¾æ³: å‰›é–‹å§‹æº–å‚™è®€æ›¸æœƒï¼Œå¤§å®¶ç®—æ˜¯æº–å‚™ç†±èº«ä¸­

Reference:
- å…ˆèªªæ˜æœ¬é¡Œæƒ³è€ƒä»€éº¼ï¼Œè¦æ€éº¼ç­”é¡Œï¼Œé å‚™çŸ¥è­˜æ˜¯ä»€éº¼
- ç­”é¡Œèªªæ˜ç”±æ·ºå…¥æ·±ï¼ŒçŸ¥è­˜ä¸è¶³çš„åŒäº‹å¯ä»¥çœ‹å‰é¢å…§å®¹ï¼Œå…¶ä»–ä¾ç…§è‡ªå·±é€²åº¦å¾€ä¸‹é–±è®€

[ä»¥ä¸‹é¡Œç›®é–‹å§‹]

```



## Q1

An upstream system has been configured to pass the date for a given batch of data to the Databricks Jobs API as a parameter. The notebook to be scheduled will use this parameter to load data with the following code: `df = spark.read.format("parquet").load(f"/mnt/source/{date}")`

Which code block should be used to create the date Python variable used in the above code block?

A. `date = spark.conf.get("date")`
B. `input_dict = input()`
`date= input_dict['date']`
C. `import sys`
`date = sys.argv[1]`
D. `date = dbutils.notebooks.getParam("date")`
E. `dbutils.widgets.text("date", "null")`
`date = dbutils.widgets.get("date")`





This looks like a discussion thread related to the multiple-choice question from the previous image. Here is the text content from the screenshot:

---

**Isio05** 1 month ago

**Selected Answer: E**
Widgets are used to create parameters in notebook that can then be utilized by e.g. jobs
upvoted 1 times

---

**imatheushenrique** 1 month, 1 week ago

E.
E. `dbutils.widgets.text("date", "null")`
`date = dbutils.widgets.get("date")`
upvoted 1 times

---

**AziLa** 2 months, 2 weeks ago

correct ans is E
upvoted 1 times

---

**Sosicha** 2 months, 3 weeks ago

Are you reading the question? It asks about an upstream system that has been configured to pass the date for a given batch of data to the Databricks Jobs API as a parameter. Upstream system usually don't use widgets. Widgets they are made for humans. Only C and D are correct but better is so D.
upvoted 1 times



## A1

### ğŸ“ æœ¬é¡Œæƒ³è€ƒä»€éº¼ï¼Ÿ

é€™é¡Œçš„æ ¸å¿ƒæ¦‚å¿µæ˜¯ï¼š**ã€Œä¸€å€‹è‡ªå‹•åŒ–çš„ Databricks Job æ˜¯å¦‚ä½•æ¥æ”¶åƒæ•¸çš„ï¼Ÿã€**

- **æƒ…å¢ƒæ‹†è§£ï¼š**

   1. **ã€Œä¸Šæ¸¸ç³»çµ± (Upstream system)ã€**ï¼šé€™æŒ‡çš„æ˜¯ä»»ä½•è‡ªå‹•åŒ–è§¸ç™¼ Job çš„æ±è¥¿ï¼Œä¾‹å¦‚ Airflowã€Azure Data Factoryï¼Œæˆ–æ˜¯ Databricks è‡ªå·±çš„æ’ç¨‹å™¨ã€‚

   2. **ã€ŒDatabricks Jobs APIã€**ï¼šé€™æ˜¯ä¸Šæ¸¸ç³»çµ±ç”¨ä¾†ã€Œå‘¼å«ã€Databricks Job çš„æ¨™æº–ä»‹é¢ã€‚

   3. **ã€Œä½œç‚ºåƒæ•¸ (as a parameter)ã€**ï¼šä¸Šæ¸¸ç³»çµ±åœ¨å‘¼å« API æ™‚ï¼ŒæœƒæŠŠä¸€å€‹ `date` è®Šæ•¸ï¼ˆä¾‹å¦‚ "2025-11-10"ï¼‰å‚³éçµ¦é€™å€‹ Jobã€‚

   4. **ã€ŒNotebook è¦ä½¿ç”¨é€™å€‹åƒæ•¸ã€**ï¼šä½ çš„ Notebook ç¨‹å¼ç¢¼éœ€è¦ã€Œè®€å–ã€é€™å€‹è¢«å‚³é€²ä¾†çš„ `date` è®Šæ•¸ã€‚

- **æ‰€ä»¥ï¼Œæœ¬é¡Œçš„çœŸæ­£å•é¡Œæ˜¯ï¼š** åœ¨ Notebook ä¸­ï¼Œç”¨å“ªä¸€è¡Œ Python ç¨‹å¼ç¢¼ï¼Œæ‰èƒ½æ­£ç¢ºã€Œè®€å–ã€åˆ° Job API å‚³éé€²ä¾†çš„åƒæ•¸ï¼Ÿ

### ğŸ’¡ é å‚™çŸ¥è­˜

è¦å›ç­”é€™å€‹å•é¡Œï¼Œä½ è‡³å°‘éœ€è¦çŸ¥é“ï¼š

1. **`dbutils` æ˜¯ä»€éº¼ï¼Ÿ**

   - `dbutils` (Databricks Utilities) æ˜¯ Databricks æä¾›çš„ä¸€å€‹ç‰¹æ®Šå·¥å…·åº«ï¼Œè®“ä½ çš„ Notebook å¯ä»¥è·Ÿ Databricks çš„å¾Œå°ç’°å¢ƒäº’å‹•ï¼ˆä¾‹å¦‚æ“ä½œæª”æ¡ˆç³»çµ± DBFSã€å‘¼å«å…¶ä»– Notebookï¼Œä»¥åŠæˆ‘å€‘è¦è«‡çš„ã€Œåƒæ•¸ã€ï¼‰ã€‚

2. **Widgets (å°å·¥å…·) æ˜¯ä»€éº¼ï¼Ÿ**

   - åœ¨ Databricks Notebook ä¸­ï¼ŒWidgets æ˜¯ä½ åœ¨ Notebook é ‚ç«¯å»ºç«‹çš„åœ–å½¢åŒ–è¼¸å…¥å…ƒä»¶ï¼ˆä¾‹å¦‚æ–‡å­—æ¡†ã€ä¸‹æ‹‰é¸å–®ï¼‰ã€‚

   - **é—œéµèªçŸ¥ï¼š** Widgets ä¸åªæ˜¯çµ¦ã€Œäººã€æ‰‹å‹•è¼¸å…¥ç”¨çš„ï¼Œå®ƒåŒæ™‚ä¹Ÿæ˜¯ Databricks ç”¨ä¾†æ¥æ”¶ã€Œè‡ªå‹•åŒ– Job åƒæ•¸ã€çš„**å”¯ä¸€æ¨™æº–æ©Ÿåˆ¶**ã€‚

---

### layered (ç”±æ·ºå…¥æ·±) ç­”é¡Œèªªæ˜

#### Level 1: çµ¦æ–°æ‰‹çš„å¿«é€Ÿç­”æ¡ˆ

- **æ­£ç¢ºç­”æ¡ˆï¼šE**

- **ç­”é¡Œå£è¨£ï¼š** ã€Œè¦çµ¦ Job åƒæ•¸ï¼Œå°±ç”¨ Widgetsã€ã€‚

åœ¨ Databricks çš„ä¸–ç•Œè£¡ï¼Œç•¶ä½ å¸Œæœ›ä¸€å€‹ Notebook èƒ½æ¥æ”¶å¤–ä¾†çš„åƒæ•¸ï¼ˆç„¡è«–æ˜¯äººæ‰‹å‹•è¼¸å…¥ï¼Œé‚„æ˜¯ API è‡ªå‹•å‚³éï¼‰ï¼Œæ¨™æº–åšæ³•å°±æ˜¯ã€ŒWidgetsã€ã€‚

é¸é … **E** åšäº†å…©ä»¶äº‹ï¼Œé€™æ˜¯ä¸€å€‹æ¨™æº–çš„çµ„åˆæŠ€ï¼š

1. `dbutils.widgets.text("date", "null")`

   - **å‹•ä½œï¼šå®£å‘Š/å»ºç«‹**ä¸€å€‹å«åš "date" çš„æ–‡å­— Widgetã€‚

   - **ç›®çš„ï¼š** é€™è¡Œç¨‹å¼ç¢¼ç­‰æ–¼æ˜¯å‘Šè¨´ Databricksï¼šã€Œå˜¿ï¼æˆ‘é€™å€‹ Notebook æº–å‚™å¥½è¦æ¥æ”¶ä¸€å€‹å«åš 'date' çš„åƒæ•¸äº†ã€‚å¦‚æœæ²’æœ‰äººå‚³çµ¦æˆ‘ï¼Œå°±å…ˆç”¨ "null" ç•¶ä½œé è¨­å€¼ã€‚ã€

2. `date = dbutils.widgets.get("date")`

   - **å‹•ä½œï¼šå–å¾—** "date" é€™å€‹ Widget çš„ç›®å‰å€¼ã€‚

   - **ç›®çš„ï¼š**

      - å¦‚æœé€™å€‹ Notebook æ˜¯è¢« **Jobs API** åŸ·è¡Œçš„ï¼Œ`get()` å°±æœƒæŠ“åˆ° API å‚³é€²ä¾†çš„å€¼ï¼ˆä¾‹å¦‚ "2025-11-10"ï¼‰ã€‚

      - å¦‚æœé€™å€‹ Notebook æ˜¯ä½ **æ‰‹å‹•æ‰“é–‹**ä¾†åŸ·è¡Œçš„ï¼Œ`get()` å°±æœƒæŠ“åˆ°ä½ åœ¨ä¸Šæ–¹ Widget UI è¼¸å…¥çš„å€¼ï¼Œæˆ–æ˜¯é è¨­å€¼ "null"ã€‚

æ‰€ä»¥ï¼Œ**E** æ˜¯å”¯ä¸€èƒ½åŒæ™‚æ»¿è¶³ã€Œæ‰‹å‹•åŸ·è¡Œã€å’Œã€ŒAPI è‡ªå‹•åŸ·è¡Œã€çš„æ¨™æº–åšæ³•ã€‚

#### Level 2: çµ¦æœ‰åŸºç¤çš„å·¥ç¨‹å¸« (é¸é …åˆ†æ)

è®“æˆ‘å€‘ä¾†åˆ†æä¸€ä¸‹ç‚ºä»€éº¼å…¶ä»–é¸é …æ˜¯éŒ¯çš„ï¼Œé€™æœ‰åŠ©æ–¼ä½ é‡æ¸…è§€å¿µï¼š

- **A. `date = spark.conf.get("date")`**

   - **ç”¨é€”ï¼š** é€™æ˜¯ç”¨ä¾†è®€å– **Spark çš„å…§éƒ¨è¨­å®š** (Spark Configuration)ã€‚

   - **ç‚ºä»€éº¼éŒ¯ï¼š** é›–ç„¶ Jobs API ä¹Ÿå¯ä»¥è¨­å®š Spark confï¼Œä½†é‚£é€šå¸¸æ˜¯ç”¨ä¾†èª¿æ•´ Spark æ•ˆèƒ½çš„ï¼ˆä¾‹å¦‚ `spark.sql.shuffle.partitions`ï¼‰ã€‚å®ƒä¸æ˜¯ç”¨ä¾†å‚³éã€Œå•†æ¥­é‚è¼¯åƒæ•¸ã€ï¼ˆå¦‚æ—¥æœŸæˆ–ç’°å¢ƒåç¨±ï¼‰çš„æ¨™æº–é€”å¾‘ã€‚æŠŠ `date` é€™ç¨®é‚è¼¯åƒæ•¸å¡åˆ° Spark conf è£¡ï¼Œæ˜¯ä¸€ç¨®ä¸æ­£ç¢ºçš„ç”¨æ³•ã€‚

- **B. `input_dict = input()`...**

   - **ç”¨é€”ï¼š** é€™æ˜¯æ¨™æº– Python åœ¨ã€Œçµ‚ç«¯æ©Ÿ (Terminal)ã€ä¸­ç­‰å¾…ä½¿ç”¨è€…ã€Œéµç›¤è¼¸å…¥ã€çš„æŒ‡ä»¤ã€‚

   - **ç‚ºä»€éº¼éŒ¯ï¼š** Databricks Job æ˜¯åœ¨ä¼ºæœå™¨å¾Œå°è‡ªå‹•åŒ–åŸ·è¡Œçš„ï¼Œå®ƒæ²’æœ‰ä¸€å€‹ã€Œçµ‚ç«¯æ©Ÿã€è®“ä½ è¼¸å…¥ã€‚åŸ·è¡Œåˆ°é€™è¡Œç¨‹å¼ç¢¼æ™‚ï¼ŒJob æœƒå¡ä½ç„¶å¾Œé€¾æ™‚ (Timeout) å¤±æ•—ã€‚

- **C. `import sys; date = sys.argv[1]`**

   - **ç”¨é€”ï¼š** é€™æ˜¯æ¨™æº– Python ç”¨ä¾†è®€å–ã€Œå‘½ä»¤åˆ—åƒæ•¸ã€çš„æ–¹å¼ï¼ˆä¾‹å¦‚ä½ åŸ·è¡Œ `python my_script.py "hello"`ï¼‰ã€‚

   - **ç‚ºä»€éº¼éŒ¯ï¼š** Databricks Job åŸ·è¡Œçš„æ˜¯ä¸€å€‹ã€ŒNotebookã€æª”æ¡ˆ (`.ipynb` æˆ– `.py` æ ¼å¼çš„ Notebook)ï¼Œè€Œä¸æ˜¯ç›´æ¥åœ¨ä½œæ¥­ç³»çµ±ä¸ŠåŸ·è¡Œä¸€å€‹å‚³çµ±çš„ Python è…³æœ¬ã€‚å®ƒä¸ä½¿ç”¨ `sys.argv` ä¾†å‚³éåƒæ•¸ï¼Œè€Œæ˜¯ä½¿ç”¨ Widgets æ©Ÿåˆ¶ã€‚

- **D. `date = dbutils.notebooks.getParam("date")`**

   - **ç”¨é€”ï¼š** é™·é˜±é¡Œï¼

   - **ç‚ºä»€éº¼éŒ¯ï¼š** `dbutils.notebooks` é€™å€‹æ¨¡çµ„ä¸‹**æ ¹æœ¬æ²’æœ‰ `getParam` é€™å€‹å‡½å¼**ã€‚é€™æ˜¯ä¸€å€‹è™›æ§‹çš„å‡½å¼ï¼Œç”¨ä¾†æ··æ·†ä½ çš„ã€‚

   - (*è£œå……ï¼š`dbutils.notebook.run()` æ˜¯ç”¨ä¾†*å‚³é*åƒæ•¸çµ¦ã€Œå­ Notebookã€ï¼Œä½† `dbutils.notebook.exit()` æ˜¯ç”¨ä¾†*å›å‚³*å€¼çš„ï¼Œæ²’æœ‰ `getParam`ã€‚*)

#### Level 3: çµ¦è³‡æ·±å·¥ç¨‹å¸« (é‡æ¸…è¨è«–å€çš„è¿·æ€)

ä½ æœ‰çœ‹åˆ°æä¾›çš„è¨è«–ä¸²ä¸­ï¼Œ`Sosicha` æå‡ºäº†åå°æ„è¦‹ã€‚é€™æ˜¯ä¸€å€‹éå¸¸å¥½çš„ã€Œé€²éšè¿·æ€ã€é‡æ¸…é»ã€‚

- **è¿·æ€ï¼š** `Sosicha` èªªï¼šã€ŒWidgets æ˜¯çµ¦ã€äººã€ç”¨çš„ã€‚ä¸Šæ¸¸ç³»çµ±ï¼ˆAPIï¼‰é€šå¸¸ä¸ç”¨ Widgetsã€‚ã€

- **äº‹å¯¦ï¼š** **é€™å€‹è§€å¿µæ˜¯éŒ¯èª¤çš„ã€‚** é€™æ˜¯ Databricks æ–°æ‰‹æœ€å¸¸èª¤è§£çš„åœ°æ–¹ã€‚

- **çœŸç›¸ï¼š** Widgets è¢«è¨­è¨ˆç‚º\*\*ã€Œé›™é‡ç”¨é€”ã€\*\*ï¼š

   1. **å°äºº (äº’å‹•æ¨¡å¼)ï¼š** å®ƒæœƒè‡ªå‹•åœ¨ Notebook é ‚ç«¯ç”¢ç”Ÿä¸€å€‹ UI ä»‹é¢ï¼Œæ–¹ä¾¿ä½ æ‰‹å‹•æ¸¬è©¦ã€‚

   2. **å° API (Job æ¨¡å¼)ï¼š** å®ƒæœƒè‡ªå‹•ã€Œè¨»å†Šã€é€™å€‹åƒæ•¸åç¨±ã€‚ç•¶ Jobs API å¸¶è‘—åƒæ•¸ä¾†åŸ·è¡Œé€™å€‹ Notebook æ™‚ï¼ŒDatabricks å¹³å°æœƒè‡ªå‹•å°‡ API å‚³ä¾†çš„å€¼ã€Œæ³¨å…¥ã€åˆ°å°æ‡‰åç¨±çš„ Widget ä¸­ã€‚

- **çµè«–ï¼š** `Sosicha` çš„é‚è¼¯å‰ææ˜¯éŒ¯çš„ã€‚Widgets **æ­£æ˜¯** Databricks Job API ç”¨ä¾†èˆ‡ Notebook æºé€šåƒæ•¸çš„æ¨™æº–æ©‹æ¨‘ã€‚å› æ­¤ï¼ŒE ä»ç„¶æ˜¯æœ€ä½³ä¸”æœ€æ­£ç¢ºçš„ç­”æ¡ˆã€‚

---

### ç¸½çµ

| ä»»å‹™ | æ­£ç¢ºæ–¹æ³• (æœ¬é¡Œç­”æ¡ˆ) | éŒ¯èª¤/ä¸é©ç”¨æ–¹æ³• | 
|---|---|---|
| **è®“ Notebook æ¥æ”¶ Job åƒæ•¸** | `dbutils.widgets.text("param_name", ...)`<br>`dbutils.widgets.get("param_name")` | `sys.argv` (ç”¨æ–¼æœ¬æ©Ÿè…³æœ¬)<br>`input()` (ç”¨æ–¼çµ‚ç«¯æ©Ÿäº’å‹•)<br>`spark.conf.get()` (ç”¨æ–¼ Spark è¨­å®š) | 



---


