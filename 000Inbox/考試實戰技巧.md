# **考試實戰技巧**

---

## **關鍵數字索引**

- **59** (題數)

- **120** (分鐘)

- **70%** (約略通過分數)

- **7 天** (Delta VACUUM 預設保留期)

- **2 年** (證書有效期)

- **銅/銀/金 3 層** (獎牌架構)

- **1 年以上** (建議考前經驗)

---

## **通用選擇題策略** 

1. **識別關鍵字**: 掃描題幹中的特定術語——例如「**增量檔案攝取**」暗示 **Auto Loader**，「**結構演變**」暗示 Delta 的演變功能或 DLT，「**串流延遲**」指向**浮水印**或觸發間隔。這些關鍵字通常對應到某個 Databracks 功能。

2. **情境與服務匹配**: 判斷哪個 Databricks 元件適合該情境。如果是關於排程和依賴性，考慮 **Jobs 工作流程**；如果是部署程式碼，考慮 **Repos/CI-CD**；如果是跨組織的資料共享，考慮 **Delta Sharing**；如果是即時處理，則是**結構化串流**。當你知道哪些工具是相關的，排除錯誤答案就變得更容易。

3. **排除干擾選項**: 通常一兩個選項明顯不對（例如，在有自動化工具存在的情況下建議手動步驟）。先排除它們。對於剩下的選項，根據最佳實踐來權衡——Databracks 的答案通常與推薦的方法一致（例如，使用一個眾所周知的功能而不是一個取巧的辦法）。如果兩個答案看起來都合理，問自己：**哪一個更直接或更安全地解決了核心問題？**——那通常就是正確答案。



## **D.通用選擇題判斷樹 (MCQ Decision Tree)**

看到關鍵詞 → 套流程 → 排除干擾項

- **IF** 題目提到「增量 (incrementally)」、「新檔案 (new files)」、「自動 (automatically)」攝取資料...

   - → **答案 99% 是 Auto Loader。**

   - → *干擾項：* read.parquet() (批次), COPY INTO (需要手動觸發)。

- **IF** 題目提到「管理 ETL 依賴 (dependencies)」、「自動化重試」、「資料品質監控」、「聲明式 (declarative)」...

   - → **答案 99% 是 Delta Live Tables (DLT) 或 Lakeflow。**

   - → *干擾項：* Databricks Jobs (手動編排), Airflow (外部工具)。

- **IF** S題目提到「效能」 *且* 「高基數 (high-cardinality)」欄位 (如 user_id, timestamp)...

   - → **答案 99% 是 Liquid Clustering。**

   - → *干擾項：* Partitioning (傳統分區，高基數會造成小檔案地獄), Z-Order (很好，但 LC 更好且自動)。

- **IF** 題目提到「安全」、「治理」、「權限」、「跨部門」...

   - → **答案 100% 跟 Unity Catalog (UC) 有關。**

   - → *干擾項：* Table ACLs (Legacy) (舊的權限模型，已被 UC 取代), IAM Roles (雲端權限，UC 在其上抽象化)。

- **IF** S題目提到「CI/CD」、「部署 (Deployment)」、「可重複性 (Reproducibility)」...

   - → **答案是 Databricks Asset Bundles (DABs)。**

      → *干擾項：* Databricks Repos (很好，但只是 Git 同步，DABs 才是「部署」標準), 手動匯出 Notebook (絕對錯誤)。



## **常用語法參考 (可直接複製)**

- **Spark DataFrame (PySpark) – 從 S3 讀取 JSON:**

   ```python
   # 從 S3 路徑讀取 JSON 檔案
   df = spark.read.format("json").option("multiline", False).load("s3://path/to/data.json")
   
   # 進行鏈式操作：過濾、分組、計數並顯示結果
   df.filter("age > 30").groupBy("country").count().show()
   ```

- **結構化串流 Auto Loader – 攝取到 Delta:**

   ```python
   # 設定串流讀取器，使用 cloudFiles 格式 (Auto Loader)
   spark.readStream.format("cloudFiles") \
     .option("cloudFiles.format", "json") \
     .load("/mnt/raw/events") \
     .writeStream.format("delta") \
     .option("checkpointLocation", "/mnt/chk/events") \
     .toTable("bronze.events")
   ```

- **Delta Table DML – 合併 (Upsert):**

   ```sql
   -- 當來源(s)與目標(t)的 id 匹配時，進行更新或插入
   MERGE INTO target t
   USING source s
   ON s.id = t.id
   WHEN MATCHED THEN 
     UPDATE SET t.value = s.value
   WHEN NOT MATCHED THEN 
     INSERT (id, value) VALUES(s.id, s.value);
   ```

- **Unity Catalog – 授權範例:**

   ```sql
   -- 授予 `analyst_role` 角色對表格的 SELECT 權限及應用列過濾器的權限
   GRANT SELECT, APPLY ROW FILTER ON TABLE finance.customer_data TO `analyst_role`;
   ```

- **Databricks CLI – 工作區匯入:**

   ```bash
   # 將本地的 notebooks 資料夾上傳到 Databricks 工作區的指定路徑
   databricks workspace import_dir ./project_notebooks /Repos/ProjectA/Notebooks
   ```

---



## **反覆操練 (Drill) 與提取 (Retrieval)**

#### **每日微操練 (每個 10 分鐘):**

- **PySpark API 操練**: 拿一個小資料集（例如一個元組列表），練習**憑記憶**創建一個 DataFrame 並執行一連串的轉換。例如，`filter` 一個條件，用 `withColumn` 或 Spark SQL `CASE` 添加一個**計算欄位**，然後做一個 `groupBy` 匯總。目標是不看筆記就能寫出來。

- **SQL 查詢構建**: 在紙上寫一個複雜的 SQL 查詢：例如，一個包含**窗口函數**（如 `RANK` 或移動平均）和過濾條件的三表連接。不用 IDE——全憑腦袋。然後在一個虛擬表上運行它，看看是否有效。

- **Delta Lake 動手做**: 在 Databricks notebook 中，創建一個小的 Delta 表格，然後**更新**幾行並**時間旅行**到早期版本。接著，嘗試一個 `MERGE` 操作。目標是鞏固 Delta 命令的順序（`CREATE TABLE`, `UPDATE`, `MERGE`, `DESCRIBE HISTORY`）及其效果。

- **資料安全設定模擬**: 創建一個包含一些假敏感資料（電子郵件、薪資）的表格。然後，練習應用一個**欄位遮罩** UDF 或一個簡單的列過濾器。即使你無法在沒有 UC 的環境中完全強制執行，僅僅是寫出語法或概念化它，都有助於記憶安全規則的宣告方式。

- **偵錯 Spark 作業**: 打開過去某次運行的 Spark UI（或公開的 Spark 事件時間軸截圖），花 5 分鐘解讀它。例如，看一個耗時很長的 stage：判斷它是 shuffle、傾斜，還是沒有快取。找出 1-2 件你會檢查的事情（例如，「Stage 3 有 200 個 tasks，但其中一個運行時間是其他的好幾倍——可能是傾斜或 Python UDF 問題」）。



#### **主動回憶 – 快速自我檢測問題:**

(嘗試**不看筆記**回答。這些問題涵蓋所有主要領域，可用於間隔重複。)

1. 在結構化串流作業中使用 Auto Loader 時，你如何確保對新檔案的「僅一次」處理？

2. `OPTIMIZE table ZORDER BY (col)` 和按 `col` 分區表格有什麼區別？在什麼情況下你會選擇其中一種？

3. 在 Delta Live Tables (DLT) 中，如何在不停止管線執行的情況下處理違反品質約束的記錄？

4. 當一個作業運行速度遠低於預期時，在 Spark UI 中通常會檢查哪三件事？

5. Unity Catalog 如何實現對外部 PostgreSQL 資料庫的聯邦查詢 (federated query)？(提示：思考 Lakehouse Federation 的步驟。)

6. Databricks 提供了什麼機制來以可重現的方式將一組 notebooks 和 jobs 從開發環境**部署**到生產環境？

7. 如果你看到一個 join 操作在 executors 上導致記憶體不足錯誤，你會考慮哪兩種可能的解決方案？

8. 一個情境描述：「與第三方供應商共享一份乾淨的生產資料子集進行分析，但不給予他們直接存取你 Databricks 工作區的權限。」——哪個功能或工具最能安全地實現這一點？

9. 你如何在 Delta Lake 表格中實現一個**緩慢變動維度 (SCD) Type 2**？(描述高層次的方法或涉及的 Delta 命令。)

10. 如果一個批次 ETL 作業需要在每天早上 6 點運行，並且每次運行都應使用來自 Git 的最新程式碼，Databricks 的哪些功能組合可以實現這種自動化？

---


