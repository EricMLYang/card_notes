---
tags:
  - my-article
Checkbox 1: false
---
過去50年最重要的統計思想是什麼？

<https://www.facebook.com/share/p/15UhUjSsCo/>

筆記：過去50年最重要的統計思想是什麼？

統計學家Andrew Gelman跟他的合作者Aki Vehtari先前寫了一篇文章叫過去50年最重要的統計思想是什麼？（What are the most important statistical ideas of the past 50 years?》。

本來Andrew Gelman投稿時是待討論的稿件，希望跟編輯群再多聊些，結果陰錯陽差，編輯給他們直接刊出去了（big name lol..)。最近他部落格聊到這篇文章，我就把文章找出來看一下。

在這篇論文中，作者 Andrew Gelman 和 Aki Vehtari 回顧了幾個研究領域，認為他們從根本上塑造了統計學和資料科學（甚至是今日的AI)。

儘管這些思想早有起源，受過去半世紀計算能力進步的推動，它們已經成熟並成為該領域的核心。

這些成果了包括了：

1\.Counterfactual causal inference (反事實因果推斷)：

此框架超越了「相關不等於因果」的傳統觀念。

從Rubin等人以來，提供了一個結構化的方法，透過以潛在結果（potential outcomes---平行世界裡的你在幹麼），來嚴謹地定義因果問題，從而思考並估計來自實驗數據和觀測數據的因果效應。

Andrew Gelman沒提到的是，經濟學家將這概念拿來，用在「自然實驗」上，因而走了非常遠，包含了最近的諾貝爾獎得主Daron Acemoglu 跟James Robinson等人用「工具變數法」來談制度的長期效果，或我老闆Melissa Dell利用制度邊界來做斷點迴歸，都是承接自這一浪潮。

有趣的是，我另一個老闆Ed Glaeser是最早反對Daron Acemoglu 跟James Robinson的研究的人，是為後話。

2\. Bootstrapping and simulation-based inference (自助法與模擬為本的推論)：

從Bradley Efron於1979年在《Annals of Statistics》上發表Bootstrapping後，要計算標準差但硬剛不出來？就Bootstrapping。直接boot不出來？要不要試試更狂野一點的wild bootstrapping?

不過作者們認為這觀念之所以重要，不是因為可以標準差變得好算，而是指在觀念上，人類可是用密集的計算來取代數學分析。自助法是其中的一個關鍵例子，該方法透過重複對數據集進行重抽樣來近似抽樣分佈。此領域還包括其他模擬方法，使得分析複雜模型成為可能。

不過Andrew沒有提到，如果是這觀念的轉移的話，或許更多是馮紐曼在發明蒙地卡羅法的時候？

之前讀到一本書提到，因為要直接得到核反應的的closed-form太困難，於是乾粹用蒙地卡羅來算，馮紐曼因而留下那句有名的“I am thinking about something much more important than bombs; I am thinking about computers.”

3\. Overparameterized models and regularization (過多參數化模型與正規化)：

這算是機器學習的起源之一，這概念是悠關於「讓我們用大量的參數來fit巨量資料吧。但這麼多的參數，哪些重要？我們會不會又過度fitting一部份的資料？

這裡延伸出來的方法，比方說，Lasso regression，就是要透過懲罰項來避免overfitting。

作為一種高度彈性模型的實踐，極端狀況下，有時模型的參數比數據還多，於是我們可以透過正規化技術來控制模型以防止過度擬合。這種方法催生了許多強大的預測模型，如類神經網絡、lasso 等等。

正規化在我哈佛的lab也扮演了重要的角色。

許多深度學習模型的訓練，可以在一樣的資料跟運算，以及幾乎一樣的演算法之下，提升十倍以上的精確程度---可能只是因為我們對loss function做了某些小小的正規化。 算進階通靈。

4\. Bayesian multilevel models (貝氏多層次模型)：

這類也稱為階層式模型（hierarchical models），這些模型用於結構化數據，例如panel data或統合分析（meta-analyses）的數據，應將「經驗（實證）貝氏」(empirical Bayes) 的想法形式化，透過整合資訊和部分共享（partially pooling）估計值，從而得到更穩定和合理的推斷結果。

文章沒提到太多應用，我這邊講一下：經濟學上常見的應用，就是做Meta-Analysis，比方說你要統整20篇不同RCT的結果，每個RCT都有500個受試者，你有RCT之間的差異（m=20)，給定一個RCT，你也有受試者間的差異（n=500)，這種資料就可以做階層式模型，因為你有不同RCT間的統計差異，也有RCT內部受試者的統計差異。

5\. Generic computation algorithms (一般泛用的計算演算法)：

作者認為，功能強大且在各種脈絡下都通用的演算法（如 Gibbs sampler、Hamiltonian Monte Carlo 和變分推斷）的發展，是現代統計學的關鍵推動力。這些演算法使從業人員能夠擬合各種複雜的模型，而無需為每個模型開發客製化的計算方法。

6\. Adaptive decision analysis (適應性決策分析)：

這裡的觀念就是邊收新樣本，調整實驗設計（Adaptive），然後一邊做出決定（比方說，平台商決定該不該調漲價格。）

這裡的觀念是是此領域將統計模型與決策過程連結起來--我這一輪應該多搜集幾個資料、多做幾個實驗？

這概念在商業上被普遍應用到了科技公司（像 A/B 測試），也影響了強化學習和貝氏最適化等方法。

7\. Robust inference (穩健推斷)：

Robust大概是最不robust的術語了，經濟學家跟統計學家對於什麼是Robust的理解可能完全不同。

Andrew這邊所指的Robust inference，是指那些「開發和使用那些對其假設的違反不過度敏感的統計方法。」

其重點是創建即使在面對離群值或模型假設錯誤等問題時，依然能良好運作的模型或統計程序。 

Andrew等人有提到經濟學上最大的應用，就是White's Robust Standard errors （等於STATA的reg y x, robust)。

8 Exploratory data analysis (EDA, 探索性資料分析)：

這觀念由 John Tukey 在1960提倡，也間接催生了S軟體（R軟體的前身）的誕生。在那之前，很多圖形都是手刻的。（事實上，我最近才知道一些哈佛數學系的老教授可能還是傾向手刻................他們版本更新最多到了用Power Point製圖。)

EDA 強調使用圖形化方法來探索數據、發現模式並產生假說。這種方法推動了統計領域朝向一個更開放、更注重新發現的方向發展，圖形化或資料視覺化，已成為理解數據與複雜模型之間關係不可或缺的一部分。

以上這八個觀念：反事實因果推斷、自助法與模擬為本的推論、過參數化模型與正規化、貝氏多層次模型、一般泛用的計算演算法、適應性決策分析、穩健推斷、探索性資料分析（特別是資料視覺化），是Andrew Gelman跟他的合作者Aki Vehtari認為過去半世紀最重要的統計觀念。

我自己是覺得統計研究雖然少，但fat tail的觀念很重要，因為fat tail 可以用來解釋財富不均（皮凱提的二十一世紀資本論）、生產力的分布、甚至是金融危機的黑天鵝事件，至少fat tail不會比Adaptive decision analysis不重要就是了。