---
tags:
  - my-article
Checkbox 1: false
---
考前快覽 - Databricks 效能監控、調整與優化

目標是「看得懂瓶頸、知道該調哪裡、避免常見雷」。

---



一、最常見的效能瓶頸（考試很愛考）

1) Shuffle 太大（Join / GroupBy 最常見）

Join、GroupBy、Distinct、OrderBy 這類「寬依賴」很容易產生 Shuffle。

症狀：

．Spark UI：Shuffle Read/Write 很大、Stage 很慢

．伴隨 Spill（寫磁碟）或網路流量很高

常見根因：分區數不合、Join 策略不佳、資料量被 Join/Explode 放大



2) Data Skew（資料傾斜）

少數 key 資料量爆大，少數 task 跑超久拖慢整個 stage。

症狀：

．Spark UI：同一個 Stage 只有少數 Task 特別慢（長尾很明顯）

．某些 Task 的 Shuffle Read 特別大

常見場景：熱門 ID、低基數欄位做 Join/聚合



3) 記憶體不足 → Spill（效能掉很兇）

中間結果放不下（常見於 Join/聚合/Shuffle）→ 寫磁碟（Disk Spill）

症狀：

．Spark UI：Memory Spill / Disk Spill > 0

．Executor GC 變多、磁碟 I/O 變頻繁



4) I/O 瓶頸與小檔案問題

．小檔案太多：列檔、開檔成本很高，讀取時間被浪費掉。

．冷啟動：第一次讀很慢，第二次快（快取/OS cache 的效果）



5) 叢集資源配置不合

．CPU 長期 100%：算力不夠或平行度不足

．CPU 很低但仍慢：多半卡在 I/O、Shuffle、Skew、或等待資源

---



—

二、什麼時候 Driver 會 OOM（必補重點）

Driver OOM 通常不是「算太多」，而是「把太多東西放到 Driver 記憶體」。

最常見情境：



把大資料拉回 Driver

`．.collect()`、`.toPandas()`、`display()`/`show()` 結果太大

．任何把 DataFrame 變成 Driver 端集合/表格的動作都算



結果集太大（maxResultSize 撐爆）

．就算不是 collect，某些 action 回傳資料量過大也可能打到 `spark.driver.maxResultSize`，嚴重時導致 Driver 不穩或 OOM



Broadcast 太大（小表其實不小）

．Broadcast Join 會把「被廣播那張表」送到每個 Executor，但廣播的準備/序列化與中繼資訊也會讓 Driver 壓力暴增

．典型錯誤：把幾 GB 的表硬當小表廣播



Task/Stage 數量爆炸（排程與 metadata 壓垮 Driver）

．分區數極端大、或小檔案多到產生海量 tasks

．Driver 需要持有與排程大量 task 的資訊，容易記憶體吃滿



Query Plan / Lineage 太複雜

．超長鏈式轉換、非常複雜的 SQL 計畫、或大量 view 疊加

．Driver 在規劃/優化/序列化計畫時就可能吃掉大量記憶體

一句話記法： Driver OOM 多半是「收回來、管太多、計畫太大」。

---



—

三、監控工具：考試與實務都要會看

1) Spark UI（核心必考）

看這些就夠用：

．Jobs / Stages：哪個 Stage 最慢

．Stage 詳細：

    -Shuffle Read/Write（判斷 Shuffle 瓶頸）

    -Task Duration 分佈（判斷 Skew）

    -Spill（判斷記憶體壓力）

．Executors：

    -GC time（高 → 記憶體壓力大）

    -Input/Shuffle/Spill（看誰在扛）



2) 叢集資源監控（CPU/Memory/Network/Disk）

介面可能隨版本調整，但判讀邏輯固定：

．CPU 長期滿載：算力瓶頸或平行度不足

．CPU 低但網路/磁碟高：多半 Shuffle/I/O 瓶頸

．記憶體壓力大 + GC 高：容易 Spill 或 OOM



3) Query History / Query Profile（SQL 常用）

．直接看到 Scan、Filter、Join、Shuffle 的耗時與資料量

．很適合查「全表掃描」、「Join 後資料爆炸」、「哪個節點最慢」

---



—

四、調校重點（考試會問「該用哪招」）

1) Join 優化：先想能不能避免 Shuffle

**．小表 + 大表 → Broadcast Join**（前提：小表真的小、放得下）

．Join 前先 Filter / Select 必要欄位（減少資料量最有效）



2) 分區數（Parallelism）要合理

．太少：單一 task 資料太大 → spill / 慢

．太多：task 太多 → 排程成本高、Driver 壓力大

．考試常見：`spark.sql.shuffle.partitions`、AQE 能自動調整，但不代表永遠不用手動



3) AQE（Adaptive Query Execution）

．常見效果：自動調整 shuffle 分區、動態選 join 策略、處理部分 skew

．觀念題常考：「能自動優化，但不是萬靈丹」



4) Photon（偏 SQL/DataFrame 的加速器）

．常見收益：SQL/DF 的 Join、Aggregate、Scan 加速，也常能降低 JVM 壓力

．不適合：RDD 重度使用、Python UDF 佔比很高（瓶頸不在 Photon）



5) Cache / Persist（只在「會重用」時）

．只用一次就 cache：多半是反效果

．cache 後記得 `unpersist()`，不然容易把記憶體吃滿、GC 變重

---

—

五、反模式（看到就要警覺）

．亂 `.collect()` / `.toPandas()` / 把資料拉回 Driver

．大表 Join 前不過濾、不裁欄位

．以為「加大叢集」能解決一切（Skew / Shuffle / 小檔案通常不是加機器就好）

．cache 一堆中間結果不釋放

．分區數極端大（Driver 被 task metadata 壓垮）

---

—

六、考試速記版：看到症狀要想到什麼

．Stage 慢 + Shuffle 很大 → 想 Join 策略 / 分區數 / AQE / 過濾裁欄位

．少數 Task 超慢 → 想 Data Skew（skew join / salting / 調分區）

．Spill 很多 + GC 高 → 想 記憶體壓力（分區、join、快取、資源配置）

．Driver 掛掉 / OOM → 想 collect/toPandas、task 太多、小檔案、broadcast 太大、plan 太大