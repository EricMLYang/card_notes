# è§£é¡Œ

```markdown
# Role
ä½ æ˜¯ä¸€ä½ç¶“é©—è±å¯Œçš„ Databricks èªè­‰è¬›å¸«èˆ‡è³‡æ·±è³‡æ–™å·¥ç¨‹å¸«ã€‚  
ä½ çš„ç›®æ¨™ä¸åƒ…æ˜¯æä¾›æ­£ç¢ºç­”æ¡ˆï¼Œæ›´æ˜¯è¦æ•™æœƒå­¸ç”Ÿã€Œå¦‚ä½•æ€è€ƒã€ï¼Œå¹«åŠ©ä»–å€‘é€šé Databricks Certified Data Engineer (Associate/Professional) è€ƒè©¦ã€‚

è«‹ç”¨ **ç¹é«”ä¸­æ–‡** è§£èªªï¼Œä½†ä¿ç•™é‡è¦æŠ€è¡“åè©çš„è‹±æ–‡åŸæ–‡ï¼ˆä¾‹å¦‚ï¼š`è‡ªå‹•è¼‰å…¥ï¼ˆAutoloaderï¼‰`ã€`çµæ§‹åŒ–ä¸²æµï¼ˆStructured Streamingï¼‰`ï¼‰ã€‚

---

# Task
æˆ‘å°‡æœƒè¼¸å…¥ä¸€é“ Databricks ç›¸é—œçš„æ¨¡æ“¬è©¦é¡Œï¼ˆé€šå¸¸æ˜¯å–®é¸æˆ–å¤šé¸é¡Œï¼‰ã€‚  
è«‹ä½ ä¾ç…§ä»¥ä¸‹æ­¥é©Ÿé€²è¡Œè§£æï¼Œè¼¸å‡ºè¦ã€Œ**æ¸…æ¥šçµæ§‹åŒ–ï¼Œä½†å¯ä»¥å¯«å¾—å®Œæ•´è©³ç›¡**ã€ã€‚

> è‹¥é¡Œç›®éå¸¸ç°¡å–®ï¼ŒæŸäº›æ­¥é©Ÿå¯ä»¥é©åº¦ç²¾ç°¡ï¼Œä½† **æ­¥é©Ÿ 0 å’Œ 2 ä¸€å®šè¦æœ‰**ã€‚

---

## 0. æœ€çµ‚ç­”æ¡ˆ (Final Answer)

- å…ˆæ˜ç¢ºå¯«å‡ºæ­£ç¢ºé¸é …ï¼ˆä¾‹å¦‚ï¼š`ç­”æ¡ˆï¼šB` æˆ– `ç­”æ¡ˆï¼šAã€D`ï¼‰ã€‚
- å¯ç”¨ä¸€å¥è©±å¿«é€Ÿèªªæ˜ç‚ºä»€éº¼ï¼ˆä¾‹å¦‚ï¼š`å› ç‚ºåªæœ‰é¸é … B åŒæ™‚æ»¿è¶³è‡ªå‹•è¼‰å…¥èˆ‡ Schema æ¼”åŒ–çš„éœ€æ±‚`ï¼‰ã€‚

---

## 1. è€ƒé»åˆ†æ (Question Analysis)

- **æ ¸å¿ƒè€ƒé»**ï¼š  
  ç”¨ä¸€å¥è©±èªªæ˜é€™é¡Œåœ¨è€ƒä»€éº¼  
  - ç¯„ä¾‹ï¼šã€Œé€™é¡Œåœ¨è€ƒ Delta Lake çš„ç‰ˆæœ¬æ§åˆ¶èˆ‡æ™‚é–“æ—…è¡Œï¼ˆTime Travelï¼‰ã€  
  - æˆ–ï¼šã€Œé€™é¡Œä¸»è¦åœ¨è€ƒ Autoloader + çµæ§‹åŒ–ä¸²æµï¼ˆStructured Streamingï¼‰çš„è¡Œç‚ºèˆ‡è¨­å®šã€

- **è§£é¡Œé—œéµå­—**ï¼š  
  åˆ—å‡ºé¡Œç›®ä¸­æœ€é—œéµçš„ **1ï½3 å€‹è©å½™ï¼ˆKeywordsï¼‰**ï¼Œä¸¦èªªæ˜çœ‹åˆ°å®ƒå€‘æ™‚è¦è¯æƒ³åˆ°ä»€éº¼æŠ€è¡“/åŠŸèƒ½ï¼š
  - æ ¼å¼ç¤ºæ„ï¼š
    - `é—œéµå­—ï¼š"manage files for me"` â†’ è¦è¯æƒ³åˆ°ã€ŒManaged Tableï¼ˆå—ç®¡è¡¨ï¼‰ã€  
    - `é—œéµå­—ï¼š"schema inference" + "cloud object storage"` â†’ è¦è¯æƒ³åˆ°ã€ŒAutoloaderã€  
    - `é—œéµå­—ï¼š"time travel" / "VERSION AS OF"` â†’ è¦è¯æƒ³åˆ°ã€ŒDelta Lake ç‰ˆæœ¬æ§åˆ¶ã€

---

## 2. ç°¡æ˜“è§£é¡Œæ€è·¯ (Logic Path)

ç”¨æœ€ç™½è©±ã€ç›´è§€çš„é‚è¼¯èªªæ˜å¦‚ä½•æ¨å°å‡ºç­”æ¡ˆã€‚  
é‡é»æ˜¯è®“å­¸ç”Ÿçœ‹åˆ°ã€Œå¦‚æœè‡ªå·±è€ƒè©¦æ™‚é‡åˆ°ï¼Œè¦æ€éº¼æƒ³ã€ã€‚

- å»ºè­°æ ¼å¼ï¼š
  - ã€Œå› ç‚ºé¡Œç›®è¦æ±‚ **\[éœ€æ±‚ A\]**ï¼Œè€Œåœ¨ Databricks è£¡åªæœ‰ **\[æŠ€è¡“/åŠŸèƒ½ B\]** èƒ½åšåˆ°é€™ä»¶äº‹ï¼Œ  
    å†é…åˆé¡Œç›®æåˆ°çš„ **\[é™åˆ¶/ç’°å¢ƒ C\]**ï¼Œå› æ­¤å¯ä»¥æ’é™¤ \[æŸäº›é¸é …\]ï¼Œæœ€å¾Œç­”æ¡ˆæœƒè½åœ¨ \[æ­£ç¢ºé¸é …\]ã€‚ã€

- å¯ä»¥ç”¨ 2ï½4 å¥è©±ï¼ŒæŠŠã€Œå¾é¡Œç›® â†’ é–å®šæŠ€è¡“ â†’ éæ¿¾é¸é … â†’ å¾—å‡ºç­”æ¡ˆã€çš„æµç¨‹èªªæ¸…æ¥šã€‚

---

## 3. é¸é …è©³è§£ (Option Analysis)

é‡å°æ¯å€‹é¸é …é€ä¸€èªªæ˜ï¼š

- **æ­£ç¢ºç­”æ¡ˆé¸é …**ï¼š
  - æ¸…æ¥šè§£é‡‹ç‚ºä»€éº¼ã€Œå®Œå…¨ç¬¦åˆã€é¡Œç›®éœ€æ±‚ã€‚  
  - èªªæ˜å®ƒå°æ‡‰åˆ°çš„åŠŸèƒ½/è¡Œç‚ºï¼ˆä¾‹å¦‚ï¼š`Auto Loader with cloudFiles`ã€`Delta Live Tables`ã€`GRANT SELECT ON CATALOG` ç­‰ï¼‰ã€‚  
  - è‹¥æœ‰ç‰ˆæœ¬æˆ–è¡Œç‚ºç´°ç¯€ï¼ˆä¾‹å¦‚ Runtime ç‰ˆæœ¬ã€é›²ç«¯å„²å­˜æ”¯æ´å·®ç•°ï¼‰ï¼Œå¯ç°¡çŸ­æä¸€ä¸‹ã€‚

- **éŒ¯èª¤ç­”æ¡ˆé¸é …**ï¼ˆé€ä¸€èªªæ˜ï¼‰ï¼š
  - ç›´æ¥æŒ‡å‡ºå®ƒã€ŒéŒ¯åœ¨ä»€éº¼åœ°æ–¹ã€ï¼š
    - æ˜¯ **èªæ³•éŒ¯èª¤** å—ï¼Ÿ  
    - æ˜¯ **è§€å¿µéæ™‚**ï¼ˆèˆŠç‰ˆè¡Œç‚ºã€Legacy åŠŸèƒ½ï¼‰å—ï¼Ÿ  
    - é‚„æ˜¯ **ä¸ç¬¦åˆé¡Œç›®å ´æ™¯**ï¼ˆä¾‹å¦‚ï¼šè€ƒçš„æ˜¯ä¸²æµï¼Œä½†é¸é …æ˜¯æ‰¹æ¬¡è™•ç†ï¼‰ï¼Ÿ  
  - å„ªå…ˆç”¨ä¸€å¥è©±é»ç ´éŒ¯èª¤é‡é»ï¼Œä¾‹å¦‚ï¼š
    - ã€Œé€™å€‹é¸é …éŒ¯åœ¨å®ƒåªè™•ç†æ‰¹æ¬¡ï¼ˆbatchï¼‰ï¼Œä¸æ”¯æ´é•·æ™‚é–“åŸ·è¡Œçš„çµæ§‹åŒ–ä¸²æµã€‚ã€  
    - ã€Œé€™å€‹é¸é …ä½¿ç”¨çš„èªæ³•å±¬æ–¼èˆŠç‰ˆ APIï¼Œè€ƒè©¦åå¥½ä½¿ç”¨æ–°ç‰ˆèªæ³•ã€‚ã€  
    - ã€Œé€™è£¡çš„æ¬Šé™è¨­å®šåªåšåˆ°è¡¨å±¤ç´šï¼ˆtableï¼‰ï¼Œä½†é¡Œç›®éœ€æ±‚æ˜¯ç›®éŒ„å±¤ç´šï¼ˆcatalogï¼‰ã€‚ã€

---

## 4. é—œéµçŸ¥è­˜æ¸…å–® (Key Concepts Checklist)

åˆ—å‡ºç‚ºäº†å®Œå…¨ç†è§£æ­¤é¡Œï¼Œå¿…é ˆæŒæ¡çš„ **3ï½5 å€‹å°ˆæœ‰åè©æˆ–æ¦‚å¿µ**ï¼Œæ¯å€‹æ¦‚å¿µé™„ä¸Šã€Œä¸€å¥è©±çŸ­è§£é‡‹ã€ã€‚

- å»ºè­°æ ¼å¼ï¼š
  - `Autoloaderï¼ˆè‡ªå‹•è¼‰å…¥ï¼‰`ï¼šç”¨æ–¼å¾é›²ç«¯ç‰©ä»¶å„²å­˜ä»¥å¢é‡æ–¹å¼è®€å–æª”æ¡ˆçš„æ©Ÿåˆ¶ï¼Œæ”¯æ´ schema inference èˆ‡è‡ªå‹•è¿½è¹¤æ–°æª”æ¡ˆã€‚  
  - `Structured Streamingï¼ˆçµæ§‹åŒ–ä¸²æµï¼‰`ï¼šä»¥ã€Œç„¡é™è¡¨ï¼ˆunbounded tableï¼‰ã€æ¦‚å¿µå»ºæ¨¡ï¼Œå¯¦éš›ä»¥ micro-batch æˆ– continuous æ¨¡å¼åŸ·è¡Œçš„ä¸²æµå¼•æ“ã€‚  
  - `Delta Lake Time Travel`ï¼šé€éç‰ˆæœ¬æˆ–æ™‚é–“é»æŸ¥è©¢ Delta è¡¨çš„æ­·å²è³‡æ–™ã€‚  
  - `Managed Tableï¼ˆå—ç®¡è¡¨ï¼‰`ï¼šç”± Databricks ç®¡ç†è³‡æ–™èˆ‡ metadata çš„è¡¨ï¼Œåˆªè¡¨æ™‚æœƒé€£è³‡æ–™ä¸€èµ·åˆªé™¤ã€‚  
  - `Schema Evolutionï¼ˆSchema æ¼”åŒ–ï¼‰`ï¼šåœ¨å¯«å…¥æˆ–è®€å–æ™‚è‡ªå‹•èª¿æ•´è¡¨çš„çµæ§‹ï¼Œä»¥æ”¯æ´æ–°å¢æ¬„ä½ç­‰è®ŠåŒ–ã€‚

---

## 5. å°ˆå®¶è£œå…… (Pro Tips)

åœ¨é€™ä¸€ç¯€æä¾›ä¸€å€‹ã€Œè€ƒè©¦å¯¦æˆ° + å¯¦å‹™ã€è¦–è§’çš„è£œå……ï¼Œå¯ä»¥æ˜¯ï¼š

- **è€ƒè©¦æŠ€å·§**ï¼š
  - ä¾‹å¦‚ï¼šã€Œåœ¨è€ƒé¡Œçœ‹åˆ° `cloudFiles` + `schemaLocation` å¹¾ä¹å¯ä»¥ 90% ç¢ºå®šæ˜¯åœ¨è€ƒ Autoloaderã€‚ã€  
  - æˆ–ï¼šã€ŒDatabricks è€ƒè©¦åå¥½å®˜æ–¹æ¨è–¦åšæ³•ï¼Œæ‰€ä»¥é‡åˆ° Legacy åŠŸèƒ½ï¼ˆä¾‹å¦‚èˆŠç‰ˆ streaming APIï¼‰é€šå¸¸ä¸æ˜¯ç­”æ¡ˆã€‚ã€

- **å¸¸è¦‹é™·é˜±æé†’**ï¼š
  - ä¾‹å¦‚ï¼šã€Œå¾ˆå¤šäººæœƒæŠŠ `Autoloader` è·Ÿä¸€èˆ¬çš„ `spark.readStream` from directory æ··åœ¨ä¸€èµ·ï¼Œä½† Autoloader æ‰æœƒå¹«ä½ ç®¡ç†æª”æ¡ˆç‹€æ…‹èˆ‡é«˜æ•ˆ listingã€‚ã€  
  - æˆ–ï¼šã€Œæœ‰äº›è€ƒé¡Œæœƒæ•…æ„ç”¨å¯¦å‹™ä¸Šã€ä¹Ÿèƒ½å·¥ä½œä½†éæœ€ä½³å¯¦å‹™ã€çš„æ–¹å¼ç•¶éŒ¯èª¤é¸é …ã€‚ã€

- **å¯¦å‹™ vs è€ƒè©¦**ï¼š
  - è‹¥é€™é¡Œçš„ã€Œè€ƒè©¦æ¨™æº–ç­”æ¡ˆã€è·Ÿã€Œä½ åœ¨å¯¦å‹™ä¸Šå¯èƒ½æœƒåšçš„åšæ³•ã€ä¸åŒï¼Œè«‹æ˜ç¢ºèªªï¼š
    - ã€Œ**åœ¨è€ƒè©¦ä¸­æ‡‰è©²é¸ \[X\]ï¼Œå› ç‚ºé€™æ˜¯å®˜æ–¹æ¨è–¦èˆ‡æ–‡ä»¶ä¸­çš„æ¨™æº–åšæ³•ï¼›  
      ä½†åœ¨å¯¦å‹™ä¸Šï¼Œå¦‚æœè€ƒé‡æˆæœ¬ / ç›¸å®¹æ€§ï¼Œæœ‰æ™‚æœƒæ¡ç”¨ \[Y\] çš„æŠ˜è¡·æ–¹å¼ã€‚**ã€

---

# å…¶ä»–è¦æ±‚

- è‹¥é¡Œç›®æè¿°æœ¬èº«æœ‰æ¨¡ç³Šæˆ–ä¸åš´è¬¹ä¹‹è™•ï¼Œå¯ä»¥åœ¨è§£æä¸­ä»¥ä¸€å¥è©±ç°¡çŸ­æŒ‡å‡ºï¼Œä½†ä»è¦ä»¥ã€Œ**æœ€ç¬¦åˆ Databricks å®˜æ–¹æ–‡ä»¶èˆ‡è€ƒè©¦é æœŸçš„ç­”æ¡ˆ**ã€ç‚ºä¸»ã€‚
- æ•´é«”é¢¨æ ¼ï¼š  
  - é‚è¼¯æ¸…æ¥šã€åˆ†æ®µæ˜ç¢ºã€é¿å…ç©ºè©±ã€‚  
  - å¯ä»¥é•·ç¯‡ä¸€é»ï¼Œä½†è¦è®“äººã€Œä¸€çœ¼çœ‹å¾—å‡ºçµæ§‹ã€ï¼Œæ–¹ä¾¿å¾ŒçºŒæ•´ç†æˆç­†è¨˜æˆ–ç°¡å ±ã€‚

---

**ç¾åœ¨ï¼Œè«‹åœ¨æˆ‘è²¼å‡ºé¡Œç›®ä¹‹å¾Œï¼Œä¾ç…§ä¸Šè¿°æ ¼å¼é–‹å§‹è§£æã€‚**

```





## **Question #1**

<https://adb-987663559009628.8.azuredatabricks.net/browse/folders/497278748789585?o=987663559009628>

æ­£ç¢ºç­”æ¡ˆæ˜¯ E

**æ ¸å¿ƒè€ƒé»**

Databricks Jobs Notebook åƒæ•¸çš„å‚³éèˆ‡åœ¨ Notebook ä¸­ä»¥ dbutils.widgets è®€å–ã€‚



**é—œéµçŸ¥è­˜æ¸…å–®**

- Databricks Jobs Notebook åƒæ•¸ï¼ˆnotebook_paramsï¼‰ï¼šJobs é€éåƒæ•¸å°‡å€¼å‚³å…¥ Notebookï¼Œéœ€ä»¥ widgets è®€å–ã€‚

- dbutils.widgets.text èˆ‡ dbutils.widgets.getï¼šåœ¨ Notebook ä¸­å®£å‘Šèˆ‡è®€å–åƒæ•¸çš„æ¨™æº–æ–¹æ³•ï¼›get å›å‚³å­—ä¸²ã€‚

- Notebook ä»»å‹™ vs Python Script ä»»å‹™ï¼šNotebook ç”¨ widgetsï¼›Python Script ç”¨ sys.argvï¼ˆpython_paramsï¼‰ã€‚

- [dbutils.notebook.run](dbutils.notebook.run)ï¼šå‘¼å«å­ Notebook æ™‚ï¼Œåƒæ•¸ä»éœ€åœ¨å­ Notebook ç”¨ dbutils.widgets.get å–å¾—ã€‚

- Spark confï¼šç”¨æ–¼ Spark é…ç½®ï¼Œä¸æœƒè‡ªå‹•æ‰¿è¼‰ Jobs Notebook åƒæ•¸ã€‚

## **Question #2**

æ­£ç¢ºç­”æ¡ˆæ˜¯ D

**æƒ…å¢ƒèªªæ˜**

Databricks å·¥ä½œå€ç®¡ç†å“¡å·²ç‚ºå„å€‹è³‡æ–™å·¥ç¨‹åœ˜éšŠé…ç½®äº†**äº’å‹•å¼å¢é›†** (interactive clusters)ã€‚ç‚ºäº†æ§åˆ¶æˆæœ¬ï¼Œå¢é›†è¨­å®šç‚ºé–’ç½® 30 åˆ†é˜å¾Œçµ‚æ­¢ (terminate)ã€‚æ¯å€‹ä½¿ç”¨è€…éƒ½æ‡‰è©²èƒ½å¤ åœ¨ä¸€å¤©ä¸­çš„ä»»ä½•æ™‚é–“å°åˆ†é…çµ¦ä»–å€‘çš„å¢é›†åŸ·è¡Œå·¥ä½œè² è¼‰ã€‚

**å•é¡Œ**

å‡è¨­ä½¿ç”¨è€…å·²è¢«æ–°å¢åˆ°å·¥ä½œå€ä½†æœªè¢«æˆäºˆä»»ä½•æ¬Šé™ï¼Œä¸‹åˆ—å“ªä¸€é …æè¿°äº†ä½¿ç”¨è€…å•Ÿå‹• (start) å’Œé€£æ¥ (attach to) åˆ°ä¸€å€‹**å·²é…ç½®çš„**å¢é›†æ‰€éœ€çš„**æœ€å°æ¬Šé™**ï¼Ÿ

**é¸é …**

A. å°æ‰€éœ€å¢é›†æ“æœ‰ã€Œå¯ç®¡ç†ã€(Can Manage) æ¬Šé™ 

B. å·¥ä½œå€ç®¡ç†å“¡æ¬Šé™ (Workspace Admin privileges)ã€å…è¨±å»ºç«‹å¢é›† (cluster creation allowed)ã€å°æ‰€éœ€å¢é›†æ“æœ‰ã€Œå¯é€£æ¥ã€(Can Attach To) æ¬Šé™ 

C. å…è¨±å»ºç«‹å¢é›† (cluster creation allowed)ã€å°æ‰€éœ€å¢é›†æ“æœ‰ã€Œå¯é€£æ¥ã€(Can Attach To) æ¬Šé™ 

D. å°æ‰€éœ€å¢é›†æ“æœ‰ã€Œå¯é‡æ–°å•Ÿå‹•ã€(Can Restart) æ¬Šé™ 

E. å…è¨±å»ºç«‹å¢é›† (cluster creation allowed)ã€å°æ‰€éœ€å¢é›†æ“æœ‰ã€Œå¯é‡æ–°å•Ÿå‹•ã€(Can Restart) æ¬Šé™

**é—œéµçŸ¥è­˜æ¸…å–® (Key Concepts Checklist)**

[Access control lists | Databricks on AWS](https://docs.databricks.com/aws/en/security/auth/access-control/#clusters)

| æ¦‚å¿µ | ç°¡çŸ­è§£é‡‹ | 
|---|---|
| **å¢é›†å­˜å–æ§åˆ¶ (Cluster ACLs)** | Databricks ç”¨æ–¼å®šç¾©ä½¿ç”¨è€…æˆ–ç¾¤çµ„å°ç‰¹å®šå¢é›†æ“ä½œæ¬Šé™çš„æ©Ÿåˆ¶ã€‚ | 
| **Can Attach To** | åƒ…å…è¨±é€£æ¥åˆ°**æ­£åœ¨é‹è¡Œä¸­**çš„å¢é›†ï¼Œç„¡æ³•å¾çµ‚æ­¢ç‹€æ…‹å•Ÿå‹•ã€‚ | 
| **Can Restart** | å…è¨±ä½¿ç”¨è€…é‡æ–°å•Ÿå‹•å·²é…ç½®çš„å¢é›†ï¼Œä¸¦è‡ªå‹•ç²å¾—é€£æ¥æ¬Šé™ã€‚é€™æ˜¯å¾çµ‚æ­¢ç‹€æ…‹æ¢å¾©æ‰€éœ€çš„æœ€ä½æ¬Šé™ã€‚ | 
| **Can Manage** | æœ€é«˜çš„é Admin æ¬Šé™ï¼Œå…è¨±å•Ÿå‹•ã€é€£æ¥ï¼Œ**é‚„å…è¨±ç·¨è¼¯å¢é›†é…ç½®**ã€‚ | 
| **æœ€å°æ¬Šé™åŸå‰‡** | åœ¨å®‰å…¨å¯¦å‹™ä¸­ï¼Œé€šå¸¸æ‡‰åªæˆäºˆåŸ·è¡Œä»»å‹™æ‰€éœ€çš„æœ€ä½æ¬Šé™ï¼Œä»¥é™ä½é¢¨éšªã€‚ | 

## **Question # 3 kuo**

When scheduling **Structured Streaming** jobs for production, which configuration **automatically recovers** from query failures and **keeps costs low?**

A. Cluster: New Job Cluster; Retries: Unlimited; Maximum Concurrent Runs: Unlimited

B. Cluster: New Job Cluster; Retries: None; Maximum Concurrent Runs: 1

C. Cluster: Existing All-Purpose Cluster; Retries: Unlimited; Maximum Concurrent Runs: 1

D. Cluster: New Job Cluster; Retries: Unlimited; Maximum Concurrent Runs: 1

E. Cluster: Existing All-Purpose Cluster; Retries: None; Maximum Concurrent Runs: 1



æ ¸å¿ƒè€ƒé»ï¼š

é€™é¡Œåœ¨è€ƒ Structured Streaming åœ¨ç”Ÿç”¢ç’°å¢ƒï¼ˆproductionï¼‰ä¸‹ï¼ŒDatabricks Job çš„æœ€ä½³åŸ·è¡Œçµ„æ…‹ï¼Œç‰¹åˆ¥æ˜¯å¦‚ä½•é”åˆ°è‡ªå‹•å¾©åŸèˆ‡æˆæœ¬æ§åˆ¶ã€‚

è§£é¡Œé—œéµå­—ï¼š

Structured StreamingÂ â†’ è¦è¯æƒ³åˆ°é•·æ™‚é–“é‹è¡Œã€éœ€è¦é«˜å¯ç”¨æ€§èˆ‡è‡ªå‹•å¾©åŸçš„ä¸²æµä»»å‹™ã€‚

automatically recovers from query failuresÂ â†’ è¦è¯æƒ³åˆ° Job Retryï¼ˆè‡ªå‹•é‡è©¦ï¼‰æ©Ÿåˆ¶ã€‚

keeps costs lowÂ â†’ è¦è¯æƒ³åˆ°è³‡æºåˆ†é…æœ€å°åŒ–ï¼ˆä¾‹å¦‚ä½¿ç”¨ New Job Cluster + æ§åˆ¶ä¸¦è¡Œæ•¸ï¼‰ã€‚

é¡Œç›®è¦æ±‚ã€Œèƒ½è‡ªå‹•å¾å¤±æ•—å¾©åŸã€ï¼Œæ‰€ä»¥ä¸€å®šè¦è¨­å®š Retries ç‚º Unlimitedï¼ˆç„¡é™æ¬¡é‡è©¦ï¼‰ï¼Œé€™æ¨£ç•¶ä¸²æµä»»å‹™å¤±æ•—æ™‚ï¼ŒDatabricks Job Scheduler æœƒè‡ªå‹•é‡æ–°å•Ÿå‹•ä»»å‹™ã€‚

åŒæ™‚ï¼Œç‚ºäº†ã€Œæ§åˆ¶æˆæœ¬ã€ï¼Œæ‡‰è©²è¦ä½¿ç”¨ New Job Clusterï¼ˆå°ˆå±¬æ–¼é€™å€‹ä»»å‹™ã€ä»»å‹™çµæŸå°±æœƒè‡ªå‹•é‡‹æ”¾è³‡æºï¼‰ï¼Œè€Œä¸æ˜¯ Existing All-Purpose Clusterï¼ˆé•·æ™‚é–“æ›è‘—ï¼Œå®¹æ˜“ç”¢ç”Ÿå¤šé¤˜è²»ç”¨ï¼‰ã€‚

æœ€å¾Œï¼Œã€ŒMaximum Concurrent Runsã€è¨­ç‚º 1ï¼ˆé Unlimitedï¼‰ï¼Œå¯ä»¥ç¢ºä¿åŒä¸€æ™‚é–“ä¸æœƒå•Ÿå‹•å¤šå€‹ç›¸åŒçš„ä¸²æµ Jobï¼Œé¿å…è³‡æºæµªè²»èˆ‡é‡è¤‡è™•ç†ã€‚

ç¶œåˆä¸Šè¿°æ¢ä»¶ï¼Œåªæœ‰é¸é … D å®Œå…¨ç¬¦åˆï¼Œå› æ­¤é¸ Dã€‚



## **Question #4 jing**

The data engineering team has configured a Databricks SQL query and alert to monitor the values in a Delta Lake table. The `recent_sensor_recordings` table contains an identifying sensor_id alongside the timestamp and temperature for the most recent 5 minutes of recordings.

The below query is used to create the alert:

```sql
SELECT MEAN (temperature), MAX (temperature), MIN (temperature)
FROM recent_sensor_recordings
GROUP BY sensor_id
```

The query is set to refresh each minute and always completes in less than 10 seconds. The alert is set to trigger when mean (temperature) > 120. Notifications are triggered to be sent at most every 1 minute.

If this alert raises notifications for 3 consecutive minutes and then stops, which statement must be true?

- A. The total average temperature across all sensors exceeded 120 on three consecutive executions of the query

- B. The recent_sensor_recordings table was unresponsive for three consecutive runs of the query

- C. The source query failed to update properly for three consecutive minutes and then restarted

- D. The maximum temperature recording for at least one sensor exceeded 120 on three consecutive executions of the query

- E. The average temperature recordings for at least one sensor exceeded 120 on three consecutive executions of the query

**ç­”æ¡ˆï¼šE**

è§£é¡Œé‡é»

â€¢å› ç‚ºæœ‰ GROUP BY sensor_idï¼ŒæŸ¥è©¢æœƒå›å‚³ã€Œæ¯å€‹æ„Ÿæ¸¬å™¨ä¸€åˆ—ã€çš„å¹³å‡ã€æœ€é«˜ã€æœ€ä½æº«ã€‚

â€¢è­¦ç¤ºæ¢ä»¶æ˜¯ã€Œå¹³å‡æº«åº¦ > 120ã€ã€‚

â€¢é¡Œç›®èªªã€Œé€£çºŒ 3 åˆ†é˜éƒ½æœ‰é€šçŸ¥ï¼Œä¹‹å¾Œåœæ­¢ã€ä»£è¡¨ï¼šåœ¨é‚£ 3 æ¬¡æ¯åˆ†é˜çš„åŸ·è¡Œä¸­ï¼Œè‡³å°‘æœ‰ä¸€å€‹æ„Ÿæ¸¬å™¨çš„ã€Œå¹³å‡æº«åº¦ã€éƒ½è¶…é 120ï¼›ä¹‹å¾ŒæŸæ¬¡åŸ·è¡Œæ¢ä»¶ä¸å†æˆç«‹ï¼ˆä¾‹å¦‚å¹³å‡é™åˆ° 120 ä»¥ä¸‹ï¼‰ï¼Œæ‰€ä»¥é€šçŸ¥åœæ­¢ã€‚

é¸é …åˆ†æ

A.æ‰€æœ‰æ„Ÿæ¸¬å™¨çš„ç¸½å¹³å‡æº«åº¦åœ¨é€£çºŒä¸‰æ¬¡æŸ¥è©¢åŸ·è¡Œä¸­è¶…éäº† 120

éŒ¯èª¤ï¼šæŸ¥è©¢ä¸­ä½¿ç”¨äº† GROUP BY sensor_idï¼Œå› æ­¤çµæœä¸æ˜¯é‡å°æ‰€æœ‰æ„Ÿæ¸¬å™¨çš„ç¸½å¹³å‡å€¼

B. recent_sensor_recordings è¡¨åœ¨é€£çºŒä¸‰æ¬¡åŸ·è¡Œè©²æŸ¥è©¢æ™‚éƒ½æ²’æœ‰å›æ‡‰

éŒ¯èª¤ï¼šå¦‚æœè³‡æ–™è¡¨æ²’æœ‰å›æ‡‰ï¼ŒæŸ¥è©¢æœƒå¤±æ•—ï¼Œç„¡æ³•æˆåŠŸè¨ˆç®—çµæœä¸¦è§¸ç™¼è­¦å ±é€šçŸ¥

C. æŸ¥è©¢é€£çºŒä¸‰åˆ†é˜æœªèƒ½æ›´æ–°ï¼Œä¹‹å¾Œé‡æ–°å•Ÿå‹•

éŒ¯èª¤ï¼šè­¦å ±ä»£è¡¨æŸ¥è©¢æˆåŠŸé‹è¡Œä¸¦è¿”å›äº†è§¸ç™¼æ¢ä»¶çš„çµæœã€‚å¤±æ•—çš„æŸ¥è©¢ä¸æœƒè§¸ç™¼æˆåŠŸé€šçŸ¥

D. è‡³å°‘ä¸€å€‹æ„Ÿæ¸¬å™¨çš„æœ€é«˜æº«åº¦åœ¨é€£çºŒä¸‰æ¬¡åŸ·è¡Œè©²æŸ¥è©¢æ™‚è¶…é 120

éŒ¯èª¤ï¼šè­¦å ±æ¢ä»¶æ˜ç¢ºæ˜¯é‡å° MEAN(temperature) (å¹³å‡å€¼)ï¼Œè€Œé MAX(temperature) (æœ€å¤§å€¼)



å®˜æ–¹Alertè¨­å®šèˆ‡ç‹€æ…‹:<https://docs.databricks.com/aws/en/sql/user/alerts/>

## **Question #5 - Aaron**

A junior developer complains that the code in their notebook isn't producing the correct results in the development environment. A shared screenshot reveals that while they're using a notebook versioned with Databricks Repos, they're using a personal branch that contains old logic. The desired branch named dev-2.3.9 is not available from the branch selection dropdown.

Which approach will allow this developer to review the current logic for this notebook?

\
A. Use Repos to make a pull request use the Databricks REST API to update the current branch to dev-2.3.9

B. Use Repos to pull changes from the remote Git repository and select the dev-2.3.9 branch

C. Use Repos to checkout the dev-2.3.9 branch and auto-resolve conflicts with the current branch

D. Merge all changes back to the main branch in the remote Git repository and clone the repo again

E. Use Repos to merge the current branch and the dev-2.3.9 branch, then make a pull request to sync with the remote repository



æ ¸å¿ƒè€ƒé»ï¼š

Git åŸºæœ¬æ¦‚å¿µã€Databricks ä¸Šçš„ç‰ˆæœ¬æ§åˆ¶æ“ä½œ



è§£é¡Œé—œéµå­—ï¼š

**Checkout -** åˆ‡æ›åˆ†æ”¯

**Fetch -** æŠŠé ç«¯ç‰ˆæœ¬çš„ç‹€æ…‹æ›´æ–°åˆ° local

**Pull -** æŠŠé ç«¯çš„ç‰ˆæœ¬æœ€æ–°ç¨‹å¼ç¢¼æ›´æ–°åˆ° local (Fetch + Merge)

**Pull Request -** æŠŠæŸåˆ†æ”¯åˆä½µåˆ°æŸåˆ†æ”¯çš„å¯©æŸ¥è«‹æ±‚

**Conflict -** åŸ·è¡Œåˆä½µæ™‚å…©ç‰ˆæœ¬ç›¸åŒè¡Œæ•¸å…§å®¹ä¸ä¸€æœƒç™¼ç”Ÿçš„äº‹æƒ…

**Merge -** åˆä½µæŸåˆ†æ”¯åˆ°æŸåˆ†æ”¯ä¸­

**Clone -** æŠŠé ç«¯æ•´å€‹ç¨‹å¼ç¢¼åº«è¤‡è£½åˆ° local çš„è¡Œç‚º



ç”±é¡Œç›®å¯ä»¥å¾—çŸ¥ä»¥ä¸‹é‡é»

- Notebook ç¨‹å¼ç¢¼æ˜¯èˆŠçš„

- é–‹ç™¼è€…æ­£åœ¨ä½¿ç”¨å€‹äººåˆ†æ”¯é–‹ç™¼

- dev-2.3.9 ä¸åœ¨åˆ†æ”¯ä¸‹æ‹‰é¸é …ä¸­

â†’ ä¹Ÿå°±æ˜¯èªª â€œ local (Databricks) å°šæœªæ›´æ–°åˆ°é ç«¯ç‰ˆæœ¬ â€œ



æ‰€ä»¥éœ€è¦é¸æ“‡æœ‰å°‡é ç«¯ç‰ˆæœ¬æ›´æ–°å› Databricks çš„æ“ä½œ â‡’ B



## **Question #6 Eric**

The security team is exploring whether or not the **Databricks secrets** module can be leveraged for connecting to an external database. After testing the code with all Python variables being defined with strings, they upload the password to the secrets module and **configure the correct permissions** for the currently active user.

They then modify their code to the following (leaving all other variables unchanged).

```python
password = dbutils.secrets.get(scope = "db_creds", key = "jdbc_password")
print(password)
df = (spark.read
.format("jdbc")
.option("url", connection)
.option("dbtable", tablename)
.option("user", username)
.option("password", password)
.load()
)
```

Which statement describes what will happen when the above code is executed?

- A. The connection to the external table will fail; the string "REDACTED" will be printed.

- B. An interactive input box will appear in the notebook; if the right password is provided, the connection will succeed and the encoded password will be saved to DBFS.

- C. An interactive input box will appear in the notebook; if the right password is provided, the connection will succeed and the password will be printed in plain text.

- D. The connection to the external table will succeed; the string value of password will be printed in plain text.

- E. The connection to the external table will succeed; the string "REDACTED" will be printed.

é—œæ–¼ **Databricks Secrets å®‰å…¨æ©Ÿåˆ¶**çš„è€ƒé¡Œã€‚

**ç­”æ¡ˆï¼šE**ï¼ˆé€£ç·šæˆåŠŸï¼›å°å‡º `[REDACTED]`ï¼‰

**æ ¸å¿ƒè§£æï¼š**

1. **é€£ç·šæˆåŠŸ**ï¼š`dbutils.secrets.get` æœƒåœ¨ç¨‹å¼åŸ·è¡Œæ™‚å–å›æ­£ç¢ºçš„å¯†ç¢¼ï¼Œå› æ­¤ JDBC é€£ç·šèƒ½é †åˆ©å»ºç«‹ã€‚

2. **è¼¸å‡ºé®è”½**ï¼šç‚ºäº†å®‰å…¨æ€§ï¼ŒDatabricks æœƒè‡ªå‹•æ””æˆªä¾†è‡ª Secrets çš„è®Šæ•¸ï¼Œè‹¥å˜—è©¦å°‡å…¶åˆ—å°ï¼ˆ`print`ï¼‰è‡³æ¨™æº–è¼¸å‡ºï¼Œç³»çµ±æœƒå¼·åˆ¶é¡¯ç¤ºç‚º **`[REDACTED]`** ä»¥é˜²å¤–æ´©ã€‚

**æŠ€å·§**ï¼šçœ‹åˆ° `dbutils.secrets` è¢« `print`ï¼Œè¼¸å‡ºçµæœä¸€å®šæ˜¯ `[REDACTED]`ï¼Œçµ•ä¸æœƒæ˜¯æ˜æ–‡ã€‚

é€™æ˜¯ä¸€é“é—œæ–¼ Databricks å®‰å…¨æ€§ï¼ˆSecurityï¼‰èˆ‡ Secrets Management çš„ç¶“å…¸è€ƒé¡Œã€‚

é€™é¡Œçœ‹ä¼¼åœ¨è€ƒç¨‹å¼ç¢¼èªæ³•ï¼Œå¯¦éš›ä¸Šæ˜¯åœ¨è€ƒä½ å° ã€Œæ©Ÿæ•è³‡æ–™ä¿è­·æ©Ÿåˆ¶ï¼ˆRedactionï¼‰ã€ çš„ç†è§£ã€‚

ä»¥ä¸‹æ˜¯é‡å°é€™é“é¡Œç›®çš„è©³ç´°è§£æï¼š

---

## 0\. æœ€çµ‚ç­”æ¡ˆ (Final Answer)

- **ç­”æ¡ˆï¼šE**

- **åŸå› **ï¼š`dbutils.secrets.get` æœƒæ­£ç¢ºå–å›å¯†ç¢¼ä¾›ç¨‹å¼å…§éƒ¨ä½¿ç”¨ï¼ˆå› æ­¤é€£ç·šæˆåŠŸï¼‰ï¼Œä½† Databricks ç‚ºäº†å®‰å…¨æ€§ï¼Œæœƒå¼·åˆ¶å°‡ä¾†è‡ª Secrets çš„å€¼åœ¨æ¨™æº–è¼¸å‡ºï¼ˆStandard Outputï¼‰ä¸­é¡¯ç¤ºç‚º `[REDACTED]`ã€‚

---

## 1\. è€ƒé»åˆ†æ (Question Analysis)

- æ ¸å¿ƒè€ƒé»ï¼š

    é€™é¡Œåœ¨è€ƒ Databricks Secrets Utility (dbutils.secrets) çš„è¡Œç‚ºç‰¹å¾µï¼Œç‰¹åˆ¥æ˜¯ç•¶ä½¿ç”¨è€…è©¦åœ–å°‡æ©Ÿæ•è³‡è¨Šåˆ—å°ï¼ˆprintï¼‰å‡ºä¾†æ™‚çš„è‡ªå‹•é®è”½æ©Ÿåˆ¶ã€‚

- **è§£é¡Œé—œéµå­—**ï¼š

   - `dbutils.secrets.get`ï¼šçœ‹åˆ°é€™å€‹ï¼Œå°±è¦æƒ³åˆ°ã€Œå¾ Secret Scope è®€å–æ©Ÿæ•è³‡è¨Šã€ã€‚

   - `print(password)`ï¼šçœ‹åˆ°é€™å€‹ï¼Œè¦é¦¬ä¸Šè¯æƒ³åˆ° Databricks çš„ **Redactionï¼ˆé®è”½/ç·¨ä¿®ï¼‰** æ©Ÿåˆ¶ã€‚

   - `configure the correct permissions`ï¼šé€™å¥è©±æ˜¯ç‚ºäº†æ’é™¤ã€Œæ¬Šé™ä¸è¶³å°è‡´å¤±æ•—ã€çš„å¯èƒ½æ€§ï¼Œç¢ºä¿ç¨‹å¼èƒ½è®€åˆ°å€¼ã€‚



## **Question #09**

 A junior member of the data engineering team is exploring the language interoperability of Databricks notebooks. The intended outcome of the below code is to register a view of all sales that occurred in countries on the continent of Africa that appear in the `geo_lookup` table. Before executing the code, running `SHOW TABLES` on the current database indicates the database contains only two tables: `geo_lookup` and `sales`.

```
Cmd 1
%python
countries_af = [x for x in 
    spark.table ("geo_lookup").filter("continent = 'AF'").select("country").collect()]

Cmd 2
%sql
CREATE VIEW sales_af AS
SELECT *
FROM sales
WHERE city IN countries_af
AND CONTINENT = "AF"

```

**å•é¡Œï¼š** Which statement correctly describes the outcome of executing these command cells in order in an interactive notebook? 

(åŸ·è¡Œé€™äº›å‘½ä»¤å–®å…ƒæ ¼çš„é †åºåœ¨äº’å‹•å¼ Notebook ä¸­æœƒç”¢ç”Ÿä»€éº¼çµæœï¼Ÿ)

A. Both commands will succeed. Executing SHOW TABLES will show that countries_af and sales_af have been registered as views. 

B. Cmd 1 will succeed. Cmd 2 will search all accessible databases for a table or view named countries_af: if this entity exists, Cmd 2 will succeed. 

C. Cmd 1 will succeed and Cmd 2 will fail. countries_af will be a Python variable representing a PySpark DataFrame. 

D. Both commands will fail. No new variables, tables, or views will be created. 

E. Cmd 1 will succeed and Cmd 2 will fail. countries_af will be a Python variable containing a list of strings.

**è§£é¡Œé—œéµå­—**ï¼š

Â Â Â Â â—¦ `%python` vs. `%sql`ï¼šæ¨™ç¤ºäº†å…©å€‹ä¸åŒçš„åŸ·è¡Œä¸Šä¸‹æ–‡ã€‚

Â Â Â Â â—¦ `spark.collect()`ï¼šå¼·åˆ¶åŸ·è¡Œ Spark æŸ¥è©¢ï¼Œå°‡çµæœè½‰æ›ç‚º Python ç‰©ä»¶ï¼ˆæ­¤è™•ç‚º List of stringsï¼‰ï¼Œä¸¦å¸¶å› Driver è¨˜æ†¶é«”ã€‚

Â Â Â Â â—¦ `WHERE city IN countries_af`ï¼šå˜—è©¦åœ¨ SQL ä¸­ç›´æ¥ä½¿ç”¨ Python è®Šæ•¸ã€‚

**é¸é …è©³è§£ (Option Analysis)**

â€¢ A. `Both commands will succeed...`

Â Â Â Â â—¦ **éŒ¯èª¤ã€‚** `Cmd 2` æœƒå¤±æ•—ï¼Œå› ç‚º SQL ç„¡æ³•è­˜åˆ¥ Python è®Šæ•¸ `countries_af`ã€‚

â€¢ B. `Cmd 1 will succeed. Cmd 2 will search all accessible databases for a table or view named countries_af...`

Â Â Â Â â—¦ **éŒ¯èª¤ã€‚** é›–ç„¶ `Cmd 1` æˆåŠŸï¼Œä½† `Cmd 2` å¤±æ•—çš„æ ¹æœ¬åŸå› ä¸æ˜¯æ‰¾ä¸åˆ°åç‚º `countries_af` çš„è¡¨æˆ–è¦–åœ–ï¼Œè€Œæ˜¯å› ç‚ºåœ¨ SQL èªæ³•ä¸­ç›´æ¥å¼•ç”¨äº†**éæ³•çš„ Python è®Šæ•¸**ã€‚

â€¢ C. `Cmd 1 will succeed and Cmd 2 will fail. countries_af will be a Python variable representing a PySpark DataFrame.`

Â Â Â Â â—¦ **éŒ¯èª¤ã€‚** é›–ç„¶åŸ·è¡Œçµæœå’Œå¤±æ•—åŸå› å¤§è‡´æ­£ç¢ºï¼Œä½† `countries_af` ä¸æ˜¯ä¸€å€‹ PySpark DataFrameï¼›`.collect()` æ“ä½œå¼·åˆ¶å°‡æ•¸æ“šæ‹‰åˆ° Driver ä¸¦è½‰æ›ç‚º Python åˆ—è¡¨ã€‚

â€¢ **E. Cmd 1 will succeed and Cmd 2 will fail. countries_af will be a Python variable containing a list of strings.**

Â Â Â Â â—¦ **æ­£ç¢ºã€‚** `Cmd 1` æˆåŠŸï¼Œä¸¦ç”¢ç”Ÿäº†ä¸€å€‹ Python åˆ—è¡¨ (list of strings)ã€‚`Cmd 2` ç”±æ–¼è·¨èªè¨€è®Šæ•¸å­˜å–é™åˆ¶è€Œå¤±æ•—ã€‚

## **Question #10 Bob**

è€ƒ Data Skipping çš„è§€å¿µ

A Delta table of weather records is partitioned by date and has the below schema: `date DATE, device_id INT, temp FLOAT, latitude FLOAT, longitude FLOAT`ã€‚

To find all the records from within the Arctic Circle, you execute a query with the below filter: `latitude > 66.3`ã€‚

Which statement describes how the Delta engine identifies which files to load? 

(å“ªä¸€å€‹æ•˜è¿°æè¿°äº† Delta å¼•æ“å¦‚ä½•è­˜åˆ¥è¦è¼‰å…¥å“ªäº›æ–‡ä»¶ï¼Ÿ)

A. All records are cached to an operational database and then the filter is applied 

B. The Parquet file footers are scanned for min and max statistics for the latitude column 

C. All records are cached to attached storage and then the filter is applied 

D. The Delta log is scanned for min and max statistics for the latitude column 

E. The Hive metastore is scanned for min and max statistics for the latitude column

**æ­£ç¢ºç­”æ¡ˆæ˜¯ D**

**D. The Delta log is scanned for min and max statistics for the latitude column. (æƒæ Delta äº‹å‹™æ—¥èªŒä»¥ç²å– latitude æ¬„ä½çš„æœ€å°å’Œæœ€å¤§çµ±è¨ˆä¿¡æ¯ã€‚)**



**è€ƒé»åˆ†æ (Question Analysis)**
â€¢ æ ¸å¿ƒè€ƒé»ï¼š Delta Lake çš„**æ•¸æ“šè·³éï¼ˆData Skippingï¼‰**æ©Ÿåˆ¶ï¼Œä»¥åŠå„²å­˜é€™äº›æ–‡ä»¶çµ±è¨ˆè³‡è¨Šçš„ä½ç½®ã€‚
â€¢ è§£é¡Œé—œéµå­—ï¼š
â—¦ Delta table + filter: latitude > 66.3ï¼šè¡¨ç¤ºæŸ¥è©¢å°‡è§¸ç™¼æ•¸æ“šè·³éå„ªåŒ–ã€‚
â—¦ identifies which files to loadï¼šè©¢å•æª”æ¡ˆé¸æ“‡ï¼ˆFile Pruningï¼‰çš„ä¾æ“šã€‚
â—¦ min and max statisticsï¼šé€™æ˜¯æ•¸æ“šè·³éçš„æ ¸å¿ƒè³‡è¨Šã€‚

**é¸é …è©³è§£ (Option Analysis)**

**æ­£ç¢ºç­”æ¡ˆé¸é …**

**D. The Delta log is scanned for min and max statistics for the latitude column. (æƒæ Delta äº‹å‹™æ—¥èªŒä»¥ç²å– latitude æ¬„ä½çš„æœ€å°å’Œæœ€å¤§çµ±è¨ˆä¿¡æ¯ã€‚)**

â€¢ **å®Œå…¨ç¬¦åˆè¦æ±‚ï¼š** é€™æ˜¯ Delta Lake æ•¸æ“šè·³éï¼ˆData Skippingï¼‰åŠŸèƒ½çš„æ¨™æº–æè¿°ã€‚äº‹å‹™æ—¥èªŒ (`_delta_log`) ä¸­å„²å­˜äº†æ¯å€‹æ•¸æ“šæ–‡ä»¶çš„çµ±è¨ˆä¿¡æ¯ï¼ˆå¦‚ Min/Max å€¼ï¼‰ï¼ŒæŸ¥è©¢å„ªåŒ–å™¨åˆ©ç”¨é€™äº›çµ±è¨ˆä¿¡æ¯ï¼Œåœ¨è®€å–å¯¦éš›æ•¸æ“šä¹‹å‰å°±èƒ½æ’é™¤å¤§é‡ä¸ç›¸é—œçš„æ•¸æ“šæ–‡ä»¶ï¼Œå¤§å¹…æé«˜æŸ¥è©¢æ•ˆç‡ã€‚

**éŒ¯èª¤ç­”æ¡ˆé¸é …**

**A. All records are cached to an operational database and then the filter is applied.**

â€¢ **éŒ¯èª¤é»ï¼š** æŸ¥è©¢å„ªåŒ–çš„ç›®çš„å°±æ˜¯ç‚ºäº†é¿å…è®€å–æ‰€æœ‰è¨˜éŒ„ã€‚å°‡æ‰€æœ‰è¨˜éŒ„ç·©å­˜ï¼ˆCachingï¼‰ç„¶å¾Œå†ç¯©é¸ï¼Œæ˜¯ä½æ•ˆçš„æ‰¹æ¬¡è™•ç†è¡Œç‚ºï¼Œè€Œä¸æ˜¯å„ªåŒ–éç¨‹ã€‚

**B. The Parquet file footers are scanned for min and max statistics for the latitude column.**

â€¢ **éŒ¯èª¤é»ï¼š** é€™æ˜¯å¸¸è¦‹çš„é™·é˜±ã€‚é›–ç„¶åº•å±¤ Parquet æ–‡ä»¶ä¸­åŒ…å«é€™äº›çµ±è¨ˆä¿¡æ¯ï¼Œä½† Delta Lake çš„é—œéµå‰µæ–°åœ¨æ–¼å®ƒå°‡é€™äº›çµ±è¨ˆä¿¡æ¯æå–ä¸¦é›†ä¸­å„²å­˜åœ¨ **Delta Log** ä¸­ã€‚å¦‚æœæ¯æ¬¡æŸ¥è©¢éƒ½å¿…é ˆæƒææ•¸åƒå€‹ Parquet æ–‡ä»¶çš„è…³è¨»ä¾†ç²å–çµ±è¨ˆä¿¡æ¯ï¼Œæ•ˆç‡æœƒéå¸¸ä½ã€‚Delta å¼•æ“æœƒåˆ©ç”¨ Log ä¾†é¿å…é€™ç¨®ä½æ•ˆæ“ä½œã€‚

**C. All records are cached to attached storage and then the filter is applied.**

â€¢ **éŒ¯èª¤é»ï¼š** èˆ‡é¸é … A é¡ä¼¼ï¼Œé€™ä¸æ˜¯ Delta Lake æŸ¥è©¢å„ªåŒ–çš„æ–¹æ³•ã€‚

**E. The Hive metastore is scanned for min and max statistics for the latitude column.**

â€¢ **éŒ¯èª¤é»ï¼š** å‚³çµ±çš„ Hive Metastore é€šå¸¸åªå­˜å„²è¡¨çš„ Schema å’Œåˆ†å€ä¿¡æ¯ã€‚Delta Lake çš„æ–‡ä»¶ç´šåˆ¥ Min/Max çµ±è¨ˆä¿¡æ¯æ˜¯å…¶ç¨æœ‰çš„åŠŸèƒ½ï¼Œå­˜å„²åœ¨ Delta Log ä¸­ï¼Œè€Œé Hive Metastoreã€‚

## **Question #11 Kuo**

The data engineering team has configured a job to process customer requests to be forgotten (have their data deleted). All user data that needs to be deleted is stored in Delta Lake tables using default table settings. The team has decided to process all deletions from the previous week as a batch job at 1am each Sunday. The total duration of this job is less than one hour.

Every Monday at 3am, a batch job executes a series of VACUUM commands on all Delta Lake tables throughout the organization.

The compliance officer has recently learned about Delta Lake's time travel functionality. They are concerned that this might allow continued access to deleted data.

Assuming all delete logic is correctly implemented, which statement correctly addresses this concern?

- A. Because the VACUUM command permanently deletes all files containing deleted records, deleted records may be accessible with time travel for around 24 hours.

- B. Because the default data retention threshold is 24 hours, data files containing deleted records will be retained until the VACUUM job is run the following day.

- C. Because Delta Lake time travel provides full access to the entire history of a table, deleted records can always be recreated by users with full admin privileges.

- D. Because Delta Lake's delete statements have ACID guarantees, deleted records will be permanently purged from all storage systems as soon as a delete job completes.

- E. Because the default data retention threshold is 7 days, data files containing deleted records will be retained until the VACUUM job is run 8 days later.

   

ç­”æ¡ˆï¼šB

å› ç‚º Delta Lake çš„é è¨­è³‡æ–™ä¿ç•™ï¼ˆdata retentionï¼‰æ™‚é–“æ˜¯ 24 å°æ™‚ï¼Œåªæœ‰åœ¨è¶…éé€™å€‹æ™‚é–“ä¸¦åŸ·è¡Œ VACUUM ä¹‹å¾Œï¼ŒåŒ…å«å·²åˆªé™¤ç´€éŒ„çš„æª”æ¡ˆæ‰æœƒè¢«æ°¸ä¹…åˆªé™¤ï¼›æ‰€ä»¥åœ¨ä¸‹æ¬¡åŸ·è¡Œ VACUUMï¼ˆå³éš”å¤©ï¼‰å‰ï¼Œé€™äº›æª”æ¡ˆä»å¯é€é time travel å­˜å–ã€‚

### æ ¸å¿ƒè€ƒé»ï¼š

é€™é¡Œåœ¨è€ƒ Delta Lake çš„è³‡æ–™åˆªé™¤æµç¨‹ã€è³‡æ–™ä¿ç•™ï¼ˆdata retentionï¼‰ã€VACUUM æŒ‡ä»¤ï¼Œä»¥åŠ time travel çš„å­˜å–è¡Œç‚ºã€‚

### è§£é¡Œé—œéµå­—ï¼š

- VACUUM command â†’ è¦è¯æƒ³åˆ° Delta Lake æ¸…ç†èˆŠæª”æ¡ˆçš„è¡Œç‚ºï¼Œåªæœƒåˆªé™¤è¶…é retention threshold çš„æª”æ¡ˆã€‚

- time travel functionality â†’ è¯æƒ³åˆ°å¯ä»¥æŸ¥è©¢æ­·å²ç‰ˆæœ¬çš„ç´€éŒ„ï¼ŒåŒ…æ‹¬å°šæœªè¢«ç‰©ç†åˆªé™¤çš„æª”æ¡ˆã€‚

- default data retention threshold â†’ é è¨­æ˜¯ 24 å°æ™‚ï¼Œå½±éŸ¿ VACUUM èƒ½å¦åˆªæª”æ¡ˆã€‚

## **Question #12 Kuo**

A junior data engineer has configured a workload that posts the following JSON to the Databricks REST API endpoint 2.0/jobs/create.

```plain
{
  "name": "Ingest new data",

  "existing_cluster_id": "6015-954420-peace720",

  "notebook_task": {

    "notebook_path": "/Prod/ingest.py"

  }

}
```

Assuming that all configurations and referenced resources are available, which statement describes the result of executing this workload three times?

- A. Three new jobs named "Ingest new data" will be defined in the workspace, and they will each run once daily.

- B. The logic defined in the referenced notebook will be executed three times on new clusters with the configurations of the provided cluster ID.

- C. Three new jobs named "Ingest new data" will be defined in the workspace, but no jobs will be executed.

- D. One new job named "Ingest new data" will be defined in the workspace, but it will not be executed.

- E. The logic defined in the referenced notebook will be executed three times on the referenced existing all purpose cluster.

ç­”æ¡ˆï¼šE

å› ç‚ºæ¯æ¬¡å‘¼å« Databricks REST API /jobs/create ä¸¦æŒ‡å®š existing_cluster_idï¼Œéƒ½æœƒåœ¨è©²å·²å­˜åœ¨çš„ cluster ä¸ŠåŸ·è¡Œ notebookï¼ˆ[ingest.py](ingest.py)ï¼‰ï¼Œè€Œä¸”æ˜¯ä¸‰æ¬¡éƒ½åœ¨åŒä¸€å€‹ cluster ä¸ŠåŸ·è¡Œé€™å€‹é‚è¼¯ã€‚

### æ ¸å¿ƒè€ƒé»ï¼š

é€™é¡Œåœ¨è€ƒ Databricks Jobs API çš„ /jobs/create è¡Œç‚ºã€existing_cluster_id çš„æ„ç¾©ï¼Œä»¥åŠã€ŒJob èˆ‡ Cluster çš„é—œä¿‚ã€ã€‚

### è§£é¡Œé—œéµå­—ï¼š

- existing_cluster_id â†’ è¦è¯æƒ³åˆ°ã€Œä½¿ç”¨ç¾æœ‰çš„ All Purpose Cluster åŸ·è¡Œä»»å‹™ï¼Œè€Œéæ¯æ¬¡å»ºç«‹æ–° Clusterã€

- /jobs/create â†’ ä»£è¡¨é€™æ˜¯ã€å»ºç«‹ Jobã€çš„ APIï¼Œä¸æ˜¯åŸ·è¡Œï¼ˆrunï¼‰Job

- notebook_task â†’ ä»£è¡¨åŸ·è¡Œçš„æ˜¯ä¸€å€‹ notebookï¼Œè€Œä¸æ˜¯ script æˆ– JAR



## **Question #13 jing**

An upstream system is emitting change data capture (CDC) logs that are being written to a cloud object storage directory.

Each record in the log indicates the change type (insert, update, or delete) and the values for each field after the change.

The source table has a primary key identified by the field pk_id.

For auditing purposes, the data governance team wishes to maintain a full record of all values that have ever been valid in the source system.

For analytical purposes, only the most recent value for each record needs to be recorded.

The Databricks job to ingest these records occurs once per hour, but each individual record may have changed multiple times over the course of an hour.

Which solution meets these requirements?

A. Create a separate history table for each pk_id resolve the current state of the table by running a union all filtering the history tables for the most recent state.

B. Use MERGE INTO to insert, update, or delete the most recent entry for each pk_id into a bronze table, then propagate all changes throughout the system.

C. Iterate through an ordered set of changes to the table, applying each in turn; rely on Delta Lake's versioning ability to create an audit log.

D. Use Delta Lake's change data feed to automatically process CDC data from an external system, propagating all changes to all dependent tables in the Lakehouse.

E. Ingest all log information into a bronze table; use MERGE INTO to insert, update, or delete the most recent entry for each pk_id into a silver table to recreate the current table state.

ç­”æ¡ˆ:E

A.ç‚ºæ¯å€‹ pk_id å»ºç«‹ä¸€å€‹ç¨ç«‹çš„æ­·å²è¡¨ï¼Œä¸¦è—‰ç”± union all å’Œéæ¿¾æ­·å²è¡¨ä»¥æ‰¾å‡ºæœ€æ–°ç‹€æ…‹ä¾†é‚„åŸè³‡æ–™è¡¨ã€‚

éŒ¯èª¤: ç‚ºæ¯å€‹ pk_id å»ºç«‹ç¨ç«‹æ­·å²è¡¨æ¥µåº¦ä¸å¯¦éš›ï¼ˆå¯èƒ½æœ‰æ•¸è¬ç”šè‡³ç™¾è¬ pk_idï¼‰ï¼Œä¸æ˜“ç®¡ç†ä¸”æŸ¥è©¢æ•ˆç‡æ¥µå·®ã€‚union all æ‰€æœ‰è¡¨ä¾†æŸ¥è©¢æœ€æ–°ç‹€æ…‹éå¸¸ä½æ•ˆï¼Œèˆ‡ Databricks çš„è¨­è¨ˆç†å¿µä¸ç¬¦ã€‚

B. ä½¿ç”¨ MERGE INTO å°‡æ¯ç­† pk_id çš„æœ€æ–°ç´€éŒ„ insertã€update æˆ– delete åˆ° bronze tableï¼Œç„¶å¾Œå°‡æ‰€æœ‰è®Šæ›´å‚³éåˆ°ç³»çµ±å„è™•ã€‚

éŒ¯èª¤: Bronze å±¤æœ¬èº«æ‡‰è©²å„²å­˜åŸå§‹è³‡æ–™ï¼Œä¸æ‡‰åªç•™æœ€æ–°ç´€éŒ„ï¼Œé€™æ¨£æœƒéºå¤±å¯©è¨ˆè³‡æ–™ã€‚

C.ä¾ç…§é †åºé€ä¸€å¥—ç”¨æ¯é …è®Šæ›´åˆ°è³‡æ–™è¡¨ï¼Œä¸¦åˆ©ç”¨ Delta Lake çš„ç‰ˆæœ¬ç®¡ç†åŠŸèƒ½å»ºç«‹å®Œæ•´çš„ç¨½æ ¸ç´€éŒ„ã€‚

éŒ¯èª¤:åªä¾è³´ Delta Lake ç‰ˆæœ¬æ§åˆ¶ï¼ˆTime Travelï¼‰åš audit logï¼Œé›–ç„¶æŠ€è¡“ä¸Šå¯æŸ¥æ­·å²ï¼Œä½†æŸ¥è©¢ã€ç®¡ç†ä¸Šä¸å¦‚åˆ†å±¤å„²å­˜ç›´è¦ºä¸”é«˜æ•ˆã€‚Delta çš„ç‰ˆæœ¬ç®¡ç†æ˜¯ã€Œè¡¨ç´šå¿«ç…§ã€ï¼Œä¸æ˜¯ã€Œåˆ—ç´šæ­·å²ã€ã€‚è¦é‚„åŸæ¯å€‹ pk_id æ›¾ç¶“æœ‰æ•ˆçš„æ‰€æœ‰å€¼ï¼Œéœ€è¦è·¨å¤šå€‹ç‰ˆæœ¬åšå·®ç•°æ¯”å°ï¼Œæ˜‚è²´ä¸”è¤‡é›œã€‚

D.ä½¿ç”¨ Delta Lake çš„ change data feed è‡ªå‹•è™•ç†å¤–éƒ¨ç³»çµ±çš„ CDC è³‡æ–™ï¼Œä¸¦å°‡æ‰€æœ‰è®Šæ›´å‚³éåˆ° Lakehouse æ‰€æœ‰ç›¸ä¾çš„è³‡æ–™è¡¨ã€‚

éŒ¯èª¤:Change Data Feedï¼ˆCDFï¼‰æ˜¯ç”¨ä¾†è¿½è¹¤ Delta è¡¨çš„è®Šæ›´ï¼Œä¸¦éç›´æ¥ç”¨æ–¼ ingest å¤–éƒ¨ CDC logã€‚é¡Œç›®å¼·èª¿çš„æ˜¯ã€Œå¦‚ä½•å„²å­˜ã€è€Œéã€Œå¦‚ä½• propagate/change feedã€ï¼Œæ­¤é¸é …åé›¢é¡Œç›®ä¸»è»¸ï¼Œä¹Ÿæ²’æ˜ç¢ºä¿è­‰å¯©è¨ˆå’Œåˆ†æéœ€æ±‚éƒ½æ»¿è¶³ã€‚

E. Â å°‡æ‰€æœ‰æ—¥èªŒè³‡æ–™åŒ¯å…¥ bronze tableï¼›åœ¨ silver table åˆ©ç”¨ MERGE INTO ä»¥ pk_id é€²è¡Œ insertã€update æˆ– deleteï¼Œé‡å»ºç•¶å‰è³‡æ–™ç‹€æ…‹ã€‚

æ­£ç¢º: Bronze Tableï¼šåŸå§‹è³‡æ–™é€²ä¾†ï¼Œå®Œæ•´ä¿ç•™æ‰€æœ‰ CDC logï¼Œç¬¦åˆå¯©è¨ˆéœ€æ±‚ã€‚

Silver Tableï¼šç”¨ MERGE INTOï¼ˆDelta Lake åŸç”Ÿæ“ä½œï¼‰åªä¿ç•™æ¯å€‹ pk_id æœ€æ–°è³‡æ–™ï¼Œä¾›åˆ†ææŸ¥è©¢ã€‚

é€™æ˜¯ Databricks å®˜æ–¹ Lakehouse åˆ†å±¤ï¼ˆBronze/Silver/Goldï¼‰æœ€æ¨è–¦çš„åšæ³•ï¼Œå®Œå…¨ç¬¦åˆé¡Œç›®é›™é‡éœ€æ±‚ã€‚



**æ ¸å¿ƒè€ƒé»:**

 é€™é¡Œåœ¨è€ƒã€ŒCDCï¼ˆChange Data Captureï¼‰è³‡æ–™çš„åˆ†å±¤å„²å­˜è¨­è¨ˆï¼Œå¦‚ä½•åŒæ™‚æ»¿è¶³å¯©è¨ˆï¼ˆaudit logï¼‰èˆ‡åˆ†æï¼ˆcurrent stateï¼‰éœ€æ±‚ã€ã€‚\
**è§£é¡Œé—œéµå­— :**

- CDC (Change Data Capture)ï¼šè¨˜éŒ„è³‡æ–™è¡¨æ¯ç­†è³‡æ–™çš„è®Šæ›´äº‹ä»¶ï¼ˆinsert/update/deleteï¼‰ï¼Œå¸¸ç”¨æ–¼åŒæ­¥ç³»çµ±æˆ–å¯©è¨ˆã€‚  

- Bronze Tableï¼ˆåŸå§‹å±¤ï¼‰ï¼šLakehouse æ¶æ§‹ä¸­ç”¨ä¾†å„²å­˜åŸå§‹ã€æœªåŠ å·¥è³‡æ–™ï¼Œé©åˆå®Œæ•´ä¿ç•™æ‰€æœ‰æ­·å²ç´€éŒ„ã€‚  

- Silver Tableï¼ˆè™•ç†å±¤ï¼‰ï¼šç”¨æ–¼æ¸…ç†ã€è½‰æ›å¾Œçš„è³‡æ–™ï¼Œé€šå¸¸åªä¿ç•™æœ‰ç”¨çš„æœ€æ–°ç‹€æ…‹ï¼Œä¾¿æ–¼åˆ†æèˆ‡æŸ¥è©¢ã€‚  

- MERGE INTOï¼šDelta Lake æ”¯æ´çš„ upsert æ“ä½œï¼Œå¯æ ¹æ“šä¸»éµæ¢ä»¶æ›´æ–°ã€æ’å…¥æˆ–åˆªé™¤è³‡æ–™ï¼Œå¸¸ç”¨æ–¼è™•ç† CDCã€‚  

- Audit Logï¼ˆå¯©è¨ˆæ—¥èªŒï¼šç”¨æ–¼è¿½è¹¤è³‡æ–™çš„æ‰€æœ‰æ­·å²è®Šæ›´ï¼Œç¬¦åˆæ³•è¦ã€æ²»ç†è¦æ±‚ã€‚  

**è£œå……èªªæ˜:**

â€¢Databricks CDCï¼ˆChange Data Captureï¼Œç•°å‹•è³‡æ–™æ“·å–ï¼‰æ˜¯ä¸€ç¨®è³‡æ–™æ•´åˆæ¨¡å¼ï¼Œç”¨æ–¼è­˜åˆ¥ã€æ“·å–ä¾†æºç³»çµ±ä¸­è³‡æ–™çš„è®Šæ›´ï¼ˆä¾‹å¦‚æ’å…¥ã€æ›´æ–°å’Œåˆªé™¤ï¼‰ï¼Œä¸¦å°‡é€™äº›è®Šæ›´è¿‘ä¹å³æ™‚åœ°å‚³éåˆ°ä¸‹æ¸¸ç³»çµ±ï¼Œè€Œç„¡éœ€é‡æ–°è¼‰å…¥æ•´å€‹è³‡æ–™é›†ã€‚é€™æœ‰åŠ©æ–¼ä¿æŒè³‡æ–™åœ¨ä¸åŒç³»çµ±é–“çš„ä¸€è‡´æ€§ã€æé«˜è™•ç†æ•ˆç‡ä¸¦æ¸›å°‘å»¶é²ã€‚Â å®˜æ–¹: <https://docs.databricks.com/aws/en/ldp/what-is-change-data-capture#drawbacks-of-using-merge-into-and-foreachbatch-for-change-data-capture>

â€¢CDFï¼ˆChange Data Feedï¼‰æ˜¯ Delta Lake on Databricks çš„å…§å»ºåŠŸèƒ½ï¼šç•¶å°æŸå€‹ Delta è¡¨å•Ÿç”¨å¾Œï¼Œç³»çµ±æœƒä¿ç•™è©²è¡¨åœ¨å„ç‰ˆæœ¬é–“çš„ã€Œåˆ—ç´šè®Šæ›´è³‡æ–™ã€ï¼Œä½ å¯ä»¥ç›´æ¥æŸ¥è©¢æŸå…©å€‹ç‰ˆæœ¬æˆ–æ™‚é–“å€é–“ä¹‹é–“çš„ã€Œæ”¹äº†å“ªäº›åˆ—ã€æ”¹æˆä»€éº¼å€¼ã€ã€‚

â€¢ç•¶ä½ æ¥æ”¶åˆ°ä¸€å€‹ CDCï¼ˆä¹Ÿå°±æ˜¯ä¸€é€£ä¸²è³‡æ–™æ–°å¢ã€ä¿®æ”¹ã€åˆªé™¤çš„ç´€éŒ„ï¼‰ï¼Œä½ å¿…é ˆæ±ºå®šå¦‚ä½•æŠŠé€™äº›è®Šå‹•è½åœ°åˆ°ä½ çš„è³‡æ–™å€‰å„²æˆ–åˆ†æè³‡æ–™è¡¨ã€‚è€Œé€™ç¨®ã€Œè½åœ°ã€çš„ç­–ç•¥ï¼Œå°±æ˜¯æ‰€è¬‚çš„ **SCD** (slowly changing dimensions)è™•ç†æ–¹å¼ï¼š

![image 184.png](./è§£é¡Œ-assets/image%20184.png)

![image 185.png](./è§£é¡Œ-assets/image%20185.png)

## **Question #14 jing**

An hourly batch job is configured to ingest data files from a cloud object storage container where each batch represent all records produced by the source system in a given hour.

The batch job to process these records into the Lakehouse is sufficiently delayed to ensure no late-arriving data is missed.

The user_id field represents a unique key for the data, which has the following schema: user_id BIGINT, username STRING, user_utc STRING, user_region STRING, last_login BIGINT, auto_pay BOOLEAN, last_updated BIGINT

New records are all ingested into a table named account_history which maintains a full record of all data in the same schema as the source.

The next table in the system is named account_current and is implemented as a Type 1 table representing the most recent value for each unique user_id.

Assuming there are millions of user accounts and tens of thousands of records processed hourly, which implementation can be used to efficiently update the described account_current table as part of each hourly batch job?

A. Use Auto Loader to subscribe to new files in the account_history directory; configure a Structured Streaming trigger once job to batch update newly detected files into the account_current table.

B. Overwrite the account_current table with each batch using the results of a query against the account_history table grouping by user_id and filtering for the max value of last_updated.

C. Filter records in account_history using the last_updated field and the most recent hour processed, as well as the max last_login by user_id write a merge statement to update or insert the most recent value for each user_id.

D. Use Delta Lake version history to get the difference between the latest version of account_history and one version prior, then write these records to account_current.

E. Filter records in account_history using the last_updated field and the most recent hour processed, making sure to deduplicate on username; write a merge statement to update or insert the most recent value for each username.

ç­”æ¡ˆ:C

A.ä½¿ç”¨ Auto Loader è¨‚é–±Â account_historyÂ ç›®éŒ„ä¸­çš„æ–°æª”æ¡ˆï¼›è¨­å®š Structured Streaming çš„ trigger once ä½œæ¥­ï¼Œå°‡æ–°åµæ¸¬åˆ°çš„æª”æ¡ˆä»¥æ‰¹æ¬¡æ–¹å¼æ›´æ–°åˆ°Â account_currentÂ è¡¨ã€‚

éŒ¯èª¤:é›–ç„¶ Autoloader + Structured Streaming trigger once å¯ä»¥åšæ‰¹æ¬¡è™•ç†ï¼Œä½†é¡Œç›®æ˜ç¢ºèªªã€Œhourly batch jobã€ï¼Œè€Œä¸”é€™è£¡æ²’æœ‰æåˆ°å¦‚ä½•å»é‡èˆ‡ upsertï¼ˆMERGEï¼‰åˆ° account_currentã€‚åªç”¨ streaming è§¸ç™¼é€²è¡Œæ‰¹æ¬¡ ingestionï¼Œç„¡æ³•ä¿è­‰ account_current åªä¿ç•™æ¯å€‹ user_id æœ€æ–°è³‡æ–™ã€‚\
B. æ¯å€‹æ‰¹æ¬¡éƒ½ç”¨æŸ¥è©¢Â account_historyÂ è¡¨çš„çµæœï¼ˆä¾ user_id åˆ†çµ„ä¸¦ç¯©é¸æœ€å¤§ last_updated å€¼ï¼‰ï¼Œå»è¦†è“‹ï¼ˆoverwriteï¼‰æ•´å€‹Â account_currentÂ è¡¨ã€‚

éŒ¯èª¤:é›–ç„¶é€™æ¨£å¯ä»¥æ­£ç¢ºç”¢ç”Ÿæ¯å€‹ user_id æœ€æ–°ç´€éŒ„ï¼Œä½†æ¯å€‹å°æ™‚éƒ½å°‡æ•´å€‹è¡¨ overwriteï¼ˆè¦†è“‹ï¼‰ï¼Œå°æ–¼ã€Œå¹¾ç™¾è¬ä½¿ç”¨è€…ã€æ¯å°æ™‚å¹¾è¬ç­†ã€çš„è¦æ¨¡æœƒé€ æˆä¸å¿…è¦çš„å¯«å…¥æˆæœ¬ï¼Œä¸æ˜¯æ•ˆç‡æœ€ä½³åšæ³•ï¼ˆå°¤å…¶ç•¶å¤šå°æ™‚å¾Œåƒ…å°‘é‡è³‡æ–™æœ‰ç•°å‹•æ™‚ï¼‰ã€‚Delta Lake æ¨è–¦ç”¨ MERGE/upsertã€‚\
C. ç”¨ last_updated æ¬„ä½å’Œæœ€è¿‘ä¸€å°æ™‚è™•ç†çš„è³‡æ–™å»éæ¿¾Â account_historyï¼ŒåŒæ™‚ä¾ user_id å–æœ€å¤§ last_loginï¼Œç„¶å¾Œç”¨ merge èªå¥å°‡æ¯å€‹ user_id çš„æœ€æ–°å€¼æ›´æ–°æˆ–æ’å…¥åˆ°Â account_currentã€‚

æ­£ç¢º:é€™å€‹é¸é …å…ˆæ ¹æ“š last_updated ç¯©é¸å‡ºé€™å°æ™‚æœ‰ç•°å‹•çš„è³‡æ–™ï¼Œå†æ ¹æ“š user_id å»é‡ã€é¸æœ€å¤§ last_loginï¼ˆæˆ– last_updatedï¼‰ï¼Œæœ€å¾Œç”¨ merge åš upsertï¼Œé€™æ­£æ˜¯é«˜æ•ˆåŒæ­¥ Type 1 tableï¼ˆaccount_currentï¼‰çš„æ¨™æº–æ–¹æ³•ï¼Œå®Œå…¨ç¬¦åˆé¡Œæ„èˆ‡æ•ˆèƒ½éœ€æ±‚ã€‚\
D. åˆ©ç”¨ Delta Lake ç‰ˆæœ¬æ­·å²ï¼Œå–å¾—Â account_historyÂ çš„æœ€æ–°ç‰ˆæœ¬å’Œå‰ä¸€å€‹ç‰ˆæœ¬çš„å·®ç•°ï¼Œç„¶å¾Œå°‡é€™äº›ç´€éŒ„å¯«å…¥Â account_currentã€‚

éŒ¯èª¤:é›–ç„¶ Delta Lake æ”¯æ´ version historyï¼Œä½†ç›´æ¥ diff ç‰ˆæœ¬é–“æ‰€æœ‰è®ŠåŒ–ï¼Œå¯¦éš›é‹ä½œä¸Šé›£ä»¥æ­£ç¢ºå–å¾—æ¯å€‹ user_id çš„ã€Œæœ€çµ‚æœ€æ–°å€¼ã€ï¼ˆå› ç‚ºä¸€å°æ™‚å…§å¯èƒ½åŒå€‹ user_id å¤šæ¬¡ç•°å‹•ï¼‰ã€‚è€Œä¸”é€™æ–¹æ³•è¤‡é›œä¸”ä¸å…·æ“´å±•æ€§ï¼Œä¸æ˜¯å®˜æ–¹æ¨è–¦åšæ³•ã€‚\
E. ç”¨ last_updated æ¬„ä½å’Œæœ€è¿‘ä¸€å°æ™‚è™•ç†çš„è³‡æ–™ï¼Œéæ¿¾Â account_historyï¼Œä¸¦ç¢ºä¿ä»¥ username åšå»é‡ï¼Œç„¶å¾Œç”¨ merge èªå¥å°‡æ¯å€‹ username çš„æœ€æ–°å€¼æ›´æ–°æˆ–æ’å…¥åˆ°Â account_currentã€‚

éŒ¯èª¤:é¡Œç›®èªª user_id æ‰æ˜¯å”¯ä¸€ keyï¼Œusername å¯èƒ½æœƒé‡è¤‡æˆ–è®Šå‹•ã€‚ä»¥ username ç‚º key æœƒå°è‡´è³‡æ–™éŒ¯èª¤ï¼ˆä¾‹å¦‚åŒä¸€ user_id ä¸åŒ username æœƒå‡ºç¾å¤šç­†ï¼‰ï¼Œä¸ç¬¦ Type 1 SCD éœ€æ±‚ã€‚

**æ ¸å¿ƒè€ƒé»:**

é€™é¡Œåœ¨è€ƒã€Œå¦‚ä½•ç”¨ Delta Lake é€²è¡Œæ‰¹æ¬¡å‹ Type 1 SCDï¼ˆSlowly Changing Dimensionï¼‰è¡¨çš„é«˜æ•ˆè³‡æ–™åŒæ­¥èˆ‡æ›´æ–°ã€ã€‚

**è§£é¡Œé—œéµå­—:**

\-Type 1 SCDï¼ˆç·©æ…¢è®Šæ›´ç¶­åº¦ï¼Œé¡å‹ä¸€ï¼‰ï¼šåªä¿ç•™æ¯å€‹ keyï¼ˆå¦‚ user_idï¼‰æœ€æ–°ä¸€ç­†ç´€éŒ„ï¼Œæ­·å²è³‡æ–™ç›´æ¥è¦†è“‹ï¼Œä¸è¿½è¹¤éå»è®ŠåŒ–ã€‚

\-MERGE INTOï¼ˆåˆä½µ/Upsertï¼‰ï¼šDelta Lake æ”¯æ´æ ¹æ“š key æ¢ä»¶ï¼ŒåŒæ­¥æ’å…¥æˆ–æ›´æ–°ç›®æ¨™è¡¨çš„æ“ä½œï¼Œé©åˆ SCD èˆ‡è³‡æ–™å€‰å„²åŒæ­¥å ´æ™¯ã€‚

\-last_updatedï¼ˆæœ€å¾Œæ›´æ–°æ™‚é–“ï¼‰ï¼šå¸¸ç”¨æ–¼åˆ¤æ–·è³‡æ–™çš„ã€Œæœ€æ–°ç‹€æ…‹ã€ï¼Œåœ¨å»é‡èˆ‡åŒæ­¥æ™‚ç‚ºä¾æ“šã€‚

\-Overwriteï¼ˆè¦†è“‹å¯«å…¥ï¼‰ï¼šæ¯æ¬¡å®Œæ•´è¦†è“‹æ•´å€‹è¡¨ï¼Œç°¡å–®ä½†æ•ˆèƒ½ä¸ä½³ï¼Œåƒ…é©åˆå°å‹è¡¨æˆ–å…¨é‡åˆ·æ–°å ´æ™¯ã€‚

\-Auto Loaderï¼ˆè‡ªå‹•è¼‰å…¥ï¼‰ï¼šå°ˆç‚ºé›²ç«¯ç›®éŒ„å¢é‡æ‰¹æ¬¡æˆ–ä¸²æµ ingest è¨­è¨ˆï¼Œèƒ½è¿½è¹¤æ–°æª”æ¡ˆï¼Œä½†ä¸è² è²¬ Â downstreamï¼ˆä¸‹æ¸¸ï¼‰è³‡æ–™æµç¨‹ä¸­çš„ upsert æ“ä½œã€‚

## **Question #15 Aaron**

A table in the Lakehouse named customer_churn_params is used in churn prediction by the machine learning team.

The table contains information about customers derived from a number of upstream sources.

Currently, the data engineering team populates this table nightly by overwriting the table with the current valid values derived from upstream data sources.

The churn prediction model used by the ML team is fairly stable in production.

The team is only interested in making predictions on records that have changed in the past 24 hours.

Which approach would simplify the identification of these changed records?



A. Apply the churn model to all rows in the customer_churn_params table, but implement logic to perform an upsert into the predictions table that ignores rows where predictions have not changed.



B. Convert the batch job to a Structured Streaming job using the complete output mode; configure a Structured Streaming job to read from the customer_churn_params table and incrementally predict against the churn model.



C. Calculate the difference between the previous model predictions and the current customer_churn_params on a key identifying unique customers before making new predictions; only make predictions on those customers not in the previous predictions.



D. Modify the overwrite logic to include a field populated by calling spark.sql.functions.current_timestamp() as data are being written; use this field to identify records written on a particular date.



E. Replace the current overwrite logic with a merge statement to modify only those records that have changed; write logic to make predictions on the changed records identified by the change data feed.

## ğŸ’¡ è€ƒé»ï¼šDelta Lake Change Data Feed (CDF)

1. **æ•ˆç‡è½‰æ›ï¼š** å¾æ¯æ™š**å…¨é‡è¦†è“‹**ï¼ˆæµªè²»è³‡æºï¼‰è½‰æ›ç‚º**åƒ…è™•ç†è®Šå‹•æ•¸æ“š**ï¼ˆé«˜æ•ˆï¼‰ã€‚

2. **è®Šå‹•è­˜åˆ¥ï¼š** å¦‚ä½•åœ¨ä¸æ‰‹å‹•è¨ˆç®—å·®ç•°çš„æƒ…æ³ä¸‹ï¼Œ**è‡ªå‹•ã€æº–ç¢º**åœ°è­˜åˆ¥å‡ºå“ªäº›è¨˜éŒ„åœ¨éå» 24 å°æ™‚å…§æœ‰**å¯¦éš›è®Šå‹•**ã€‚

3. **ç­”æ¡ˆï¼š** å¯¦ç¾é€™ä¸€é»çš„æœ€ä½³å·¥å…·æ˜¯ **Delta Lake çš„ Change Data Feed (CDF)**ã€‚

## âœ… æ­£ç¢ºç­”æ¡ˆ

**E. Replace the current overwrite logic with a merge statement to modify only those records that have changed; write logic to make predictions on the changed records identified by the change data feed.**

é€™å€‹é¸é …æ˜¯ Databricks Lakehouse çš„æ¨™æº–ã€é«˜æ•ˆæ¶æ§‹æ¨¡å¼ï¼š**ä½¿ç”¨ MERGE èªå¥æ›´æ–°ï¼Œä¸¦åˆ©ç”¨ CDF è¼¸å‡ºå¢é‡è®Šå‹•ã€‚**

1. **å„ªåŒ–æ•¸æ“šå·¥ç¨‹ (DE) å¯«å…¥ï¼š**

   - å°‡ä½æ•ˆçš„ `OVERWRITE`ï¼ˆå…¨é‡å¯«å…¥ï¼‰æ›¿æ›ç‚ºé«˜æ•ˆçš„ **`MERGE INTO`**ã€‚

   - `MERGE INTO` ç¢ºä¿äº† `customer_churn_params` è¡¨**åªæ›´æ–°**é‚£äº›å¯¦éš›åœ¨æºé ­æœ‰è®Šå‹•çš„è¨˜éŒ„ã€‚é€™æœ¬èº«å°±å¤§å¹…æ¸›å°‘äº†å¯«å…¥ I/O å’Œè¨ˆç®—æˆæœ¬ã€‚

2. **å„ªåŒ– ML åœ˜éšŠè®€å– (é—œéµ)ï¼š**

   - åœ¨ `customer_churn_params` è¡¨ä¸Šå•Ÿç”¨ **Change Data Feed (CDF)** åŠŸèƒ½ã€‚

   - CDF æœƒè‡ªå‹•è¿½è¹¤ä¸¦è¨˜éŒ„æ‰€æœ‰ `MERGE`ã€`UPDATE`ã€`DELETE` æ“ä½œæ‰€ç”¢ç”Ÿçš„**è¡Œç´šè®Šå‹•**ã€‚

   - ML åœ˜éšŠå¯ä»¥ç›´æ¥è®€å– **CDF çš„æ•¸æ“šæµ**ã€‚é€™å€‹æ•¸æ“šæµå°±æ˜¯ä¸€å€‹**ä¹¾æ·¨ã€ç²¾ç¢ºã€å¢é‡**çš„é›†åˆï¼Œå…¶ä¸­**åªåŒ…å«**åœ¨éå» 24 å°æ™‚å…§å…§å®¹æœ‰è®Šå‹•çš„è¨˜éŒ„ã€‚

   - ML æ¨¡å‹åªéœ€è¦å°é€™å€‹å°å¾—å¤šçš„æ•¸æ“šé›†é‹è¡Œï¼Œæ•ˆç‡æ¯”å°æ•´å€‹è¡¨é‹è¡Œé«˜å‡ºæ•¸åç”šè‡³æ•¸ç™¾å€ã€‚

## âŒ éŒ¯èª¤é¸é …

| **é¸é …** | **ä¸­æ–‡è§£é‡‹** | **ç‚ºä»€éº¼ä¸æ˜¯æœ€ä½³è§£ï¼Ÿ** | 
|---|---|---|
| **A** | å°**æ‰€æœ‰è¡Œ**é‹è¡Œæ¨¡å‹ï¼Œä½†åªæ›´æ–°é æ¸¬çµæœæœ‰è®Šå‹•çš„è¡Œã€‚ | **æµªè²»è¨ˆç®—è³‡æºã€‚** æ¨¡å‹é‹è¡Œæ˜¯æœ€å¤§çš„æˆæœ¬ã€‚å³ä½¿æœ€çµ‚åªå¯«å…¥å°‘é‡æ•¸æ“šï¼Œæ¨¡å‹ä»éœ€å°æ•´å€‹è¡¨ï¼ˆæ•¸ç™¾è¬è¡Œï¼‰åŸ·è¡Œï¼Œé€™æ•ˆç‡æ¥µä½ã€‚ | 
| **B** | è½‰ç‚º Structured Streamingï¼Œä½¿ç”¨ **Complete** è¼¸å‡ºæ¨¡å¼ã€‚ | **æ•ˆç‡ä½ä¸”ä¸é©ç”¨ã€‚** `complete` æ¨¡å¼æœƒå°‡**æ•´å€‹è¡¨**ä½œç‚ºæœ€çµ‚çµæœè¼¸å‡ºï¼Œå¯¦è³ªä¸Šä»ç„¶æ˜¯å…¨é‡è™•ç†å’Œå…¨é‡å¯«å…¥ã€‚ | 
| **C** | **æ‰‹å‹•**è¨ˆç®—ç•¶å‰æ•¸æ“šèˆ‡ä¸Šæ¬¡é æ¸¬ä¹‹é–“çš„å·®ç•°ã€‚ | **è¤‡é›œä¸”ä½æ•ˆã€‚** éœ€è¦æ˜‚è²´çš„ Join æ“ä½œä¾†å°æ¯”æ–°èˆŠæ•¸æ“šé›†ï¼Œé€™æ¯”ä½¿ç”¨ Delta Lake å…§å»ºçš„ CDF è¿½è¹¤æ©Ÿåˆ¶è¦æ…¢å¾—å¤šã€æ›´è¤‡é›œã€‚ | 
| **D** | ä¿®æ”¹è¦†è“‹é‚è¼¯ï¼Œå¢åŠ å¯«å…¥æ™‚é–“æˆ³æ¬„ä½ã€‚ | **ç„¡æ³•è­˜åˆ¥çœŸå¯¦è®Šå‹•ã€‚** ç”±æ–¼èˆŠé‚è¼¯æ˜¯ `OVERWRITE`ï¼Œæ¯æ¬¡å¯«å…¥éƒ½æœƒæ›´æ–°**æ•´å€‹è¡¨æ‰€æœ‰è¡Œ**çš„æ™‚é–“æˆ³ã€‚å› æ­¤ï¼Œæ‚¨ç„¡æ³•å¾æ™‚é–“æˆ³ä¸Šå€åˆ†å“ªäº›è¡Œæ˜¯**å…§å®¹**ç™¼ç”Ÿäº†å¯¦éš›è®Šå‹•ã€‚ | 

## **Question #16 Aaron**

A table is registered with the following code:

```sql
CREATE TABLE recent_orders AS (
    SELECT a.user_id, a.email, b.order_id, b.order_date
    FROM
    (SELECT user_id, email 
    FROM users) a
    INNER JOIN
    (SELECT user_id, order_id, order_date
    FROM orders
    WHERE order_date > current_date() - 7) b
    ON a.user_id = b.user_id
)
```

Both users and orders are Delta Lake tables. Which statement describes the results of querying recent_orders?

\
A. All logic will execute at query time and return the result of joining the valid versions of the source tables at the time the query finishes.



B. All logic will execute when the table is defined and store the result of joining tables to the DBFS; this stored data will be returned when the table is queried.

\
C. Results will be computed and cached when the table is defined; these cached results will incrementally update as new records are inserted into source tables.

\
D. All logic will execute at query time and return the result of joining the valid versions of the source tables at the time the query began.

\
E. The versions of each source table will be stored in the table transaction log; query results will be saved to DBFS with each query.

## ğŸ’¡ è€ƒé»ï¼šCTAS vs. VIEW

é€™é¡Œçš„è€ƒé»æ˜¯å€åˆ† **`CREATE TABLE AS SELECT` (CTAS)** å’Œ **`CREATE VIEW AS SELECT`** çš„æ ¹æœ¬å·®ç•°ã€‚

- **CTAS (å¯¦é«”è¡¨)ï¼š** åœ¨**å®šç¾©æ™‚**åŸ·è¡ŒæŸ¥è©¢ï¼Œä¸¦å°‡çµæœå…·é«”åŒ–ï¼ˆå„²å­˜ï¼‰ç‚ºä¸€å€‹éœæ…‹çš„ Delta Lake è¡¨ã€‚

- **VIEW (é‚è¼¯è¡¨)ï¼š** åœ¨**æŸ¥è©¢æ™‚**åŸ·è¡Œåº•å±¤çš„ SELECT æŸ¥è©¢é‚è¼¯ã€‚

## âœ… æ­£ç¢ºç­”æ¡ˆ

**B. All logic will execute when the table is defined and store the result of joining tables to the DBFS; this stored data will be returned when the table is queried.**

- **è§£é‡‹ï¼š** `CREATE TABLE AS` æ˜¯ä¸€å€‹æ•¸æ“šå®šç¾©èªè¨€ (DDL) æ“ä½œï¼Œå®ƒæœƒåœ¨åŸ·è¡Œæ™‚**ç«‹å³**é‹è¡Œ SELECT èªå¥ï¼Œä¸¦å°‡ Join çš„çµæœæ•¸æ“šæ–‡ä»¶**å¯«å…¥**åˆ°é›²ç«¯å„²å­˜ï¼ˆDBFS/S3/ADLS/GCSï¼‰ï¼Œå½¢æˆä¸€å€‹**éœæ…‹çš„** `recent_orders` è¡¨ã€‚ä¹‹å¾Œå°è©²è¡¨çš„æŸ¥è©¢ï¼Œåªéœ€è¦è®€å–é€™äº›å·²å„²å­˜çš„éœæ…‹æ•¸æ“šï¼Œè€Œä¸æœƒé‡æ–°è¨ˆç®— Join é‚è¼¯ã€‚

## âŒ éŒ¯èª¤é¸é …

| **é¸é …** | **ä¸­æ–‡ç¿»è­¯** | **éŒ¯èª¤é»åˆ†æ** | 
|---|---|---|
| **A** | æ‰€æœ‰é‚è¼¯å°‡åœ¨**æŸ¥è©¢æ™‚**åŸ·è¡Œï¼Œä¸¦è¿”å›**æŸ¥è©¢å®Œæˆæ™‚**æºè¡¨çš„æœ‰æ•ˆç‰ˆæœ¬é€£æ¥çš„çµæœã€‚ | **éŒ¯èª¤é»ï¼šåŸ·è¡Œæ™‚æ©Ÿã€‚** é€™æ˜¯ **View (æª¢è¦–è¡¨)** çš„è¡Œç‚ºã€‚CTAS å‰µå»ºçš„æ˜¯å¯¦é«”è¡¨ï¼Œå®ƒçš„ SELECT é‚è¼¯åªåœ¨**å‰µå»ºæ™‚**åŸ·è¡Œä¸€æ¬¡ã€‚ | 
| **C** | çµæœå°‡åœ¨å®šç¾©è¡¨æ ¼æ™‚è¨ˆç®—å’Œå¿«å–ï¼›ç•¶æ–°è¨˜éŒ„æ’å…¥åˆ°æºè¡¨æ™‚ï¼Œé€™äº›å¿«å–çš„çµæœæœƒ**å¢é‡æ›´æ–°**ã€‚ | **éŒ¯èª¤é»ï¼šæ›´æ–°è¡Œç‚ºã€‚** CTAS å‰µå»ºçš„è¡¨æ˜¯**éœæ…‹çš„**ï¼Œä¸æœƒè‡ªå‹•æˆ–å¢é‡åœ°å¾æºè¡¨ (`users` æˆ– `orders`) æ›´æ–°æ•¸æ“šã€‚å®ƒåªæ˜¯ä¸€å€‹æ•¸æ“šå¿«ç…§ã€‚ | 
| **D** | æ‰€æœ‰é‚è¼¯å°‡åœ¨**æŸ¥è©¢æ™‚**åŸ·è¡Œï¼Œä¸¦è¿”å›**æŸ¥è©¢é–‹å§‹æ™‚**æºè¡¨çš„æœ‰æ•ˆç‰ˆæœ¬é€£æ¥çš„çµæœã€‚ | **éŒ¯èª¤é»ï¼šåŸ·è¡Œæ™‚æ©Ÿã€‚** èˆ‡é¸é … A ç›¸åŒï¼Œé€™æ˜¯ **View (æª¢è¦–è¡¨)** çš„è¡Œç‚ºã€‚ç‰ˆæœ¬æ§åˆ¶çš„æ™‚é–“é» (é–‹å§‹æ™‚ vs. å®Œæˆæ™‚) é›–ç„¶æœ‰ç´°å¾®å·®åˆ¥ï¼Œä½†é—œéµéŒ¯èª¤ä»æ˜¯**æŸ¥è©¢æ™‚åŸ·è¡Œé‚è¼¯**ã€‚ | 
| **E** | æ¯å€‹æºè¡¨çš„ç‰ˆæœ¬å°‡å„²å­˜åœ¨è¡¨æ ¼äº‹å‹™æ—¥èªŒä¸­ï¼›æ¯æ¬¡æŸ¥è©¢éƒ½æœƒå°‡æŸ¥è©¢çµæœå„²å­˜åˆ° DBFSã€‚ | **éŒ¯èª¤é»ï¼šæµç¨‹éŒ¯èª¤ã€‚** **æºè¡¨çš„ç‰ˆæœ¬**åªåœ¨æºè¡¨çš„æ—¥èªŒä¸­è¿½è¹¤ã€‚**æŸ¥è©¢çµæœ**ä¸æœƒåœ¨æ¯æ¬¡æŸ¥è©¢æ™‚éƒ½å„²å­˜åˆ° DBFSï¼ˆé™¤éæ‚¨ä½¿ç”¨ `CREATE TABLE` æˆ– `CACHE` ç­‰èªå¥ï¼‰ï¼Œå¦å‰‡å°±æ˜¯æ¨™æº–è®€å–æ“ä½œã€‚ | 
|  |  |  | 

## **Question #22**

é¡Œç›®ï¼š Which statement describes Delta Lake **Auto Compaction**?

ç¿»è­¯ï¼š å“ªä¸€å€‹æ•˜è¿°æè¿°äº† Delta Lake çš„è‡ªå‹•å£“ç¸®ï¼ˆAuto Compactionï¼‰ï¼Ÿ

A. An asynchronous job runs after the write completes to detect if files could be further compacted; if yes, an OPTIMIZE job is executed toward a default of 1 GB.

(åœ¨å¯«å…¥å®Œæˆå¾Œé‹è¡Œä¸€å€‹éåŒæ­¥ä½œæ¥­ä¾†æª¢æ¸¬æ˜¯å¦å¯ä»¥é€²ä¸€æ­¥å£“ç¸®æ–‡ä»¶ï¼›å¦‚æœå¯ä»¥ï¼Œå‰‡åŸ·è¡Œ OPTIMIZE ä½œæ¥­ï¼Œç›®æ¨™ç‚ºé è¨­çš„ 1 GBã€‚)

B. Before a Jobs cluster terminates, OPTIMIZE is executed on all tables modified during the most recent job.

(åœ¨ Job Cluster çµ‚æ­¢å‰ï¼Œå°æœ€è¿‘ä½œæ¥­ä¸­ä¿®æ”¹çš„æ‰€æœ‰è¡¨åŸ·è¡Œ OPTIMIZEã€‚)

C. Optimized writes use logical partitions instead of directory partitions; because partition boundaries are only represented in metadata, fewer small files are written.

(å„ªåŒ–å¯«å…¥ä½¿ç”¨é‚è¼¯åˆ†å€è€Œä¸æ˜¯ç›®éŒ„åˆ†å€ï¼›å› ç‚ºåˆ†å€é‚Šç•Œåªåœ¨ metadata ä¸­è¡¨ç¤ºï¼Œæ‰€ä»¥å¯«å…¥è¼ƒå°‘çš„å°æ–‡ä»¶ã€‚)

D. Data is queued in a messaging bus instead of committing data directly to memory; all data is committed from the messaging bus in one batch once the job is complete.

(æ•¸æ“šåœ¨æäº¤åˆ°è¨˜æ†¶é«”å‰æœƒæ’å…¥æ¶ˆæ¯ä½‡åˆ—ä¸­ï¼›æ‰€æœ‰æ•¸æ“šåœ¨ä½œæ¥­å®Œæˆå¾Œï¼Œæœƒå¾æ¶ˆæ¯ä½‡åˆ—ä¸­ä»¥æ‰¹æ¬¡æ–¹å¼æäº¤ã€‚)

E. An asynchronous job runs after the write completes to detect if files could be further compacted; if yes, an OPTIMIZE job is executed toward a default of 128 MB.

(åœ¨å¯«å…¥å®Œæˆå¾Œé‹è¡Œä¸€å€‹éåŒæ­¥ä½œæ¥­ä¾†æª¢æ¸¬æ˜¯å¦å¯ä»¥é€²ä¸€æ­¥å£“ç¸®æ–‡ä»¶ï¼›å¦‚æœå¯ä»¥ï¼Œå‰‡åŸ·è¡Œ OPTIMIZE ä½œæ¥­ï¼Œç›®æ¨™ç‚ºé è¨­çš„ 128 MBã€‚)

> ç­”æ¡ˆ:

E. An asynchronous job runs after the write completes to detect if files could be further compacted; if yes, an OPTIMIZE job is executed toward a default of 128 MB.

> è§£èªª:

â€¢ æ ¸å¿ƒè€ƒé»ï¼š

Delta Lake çš„æª”æ¡ˆç®¡ç†å’Œå„ªåŒ–æ©Ÿåˆ¶ï¼Œç‰¹åˆ¥æ˜¯ Auto Compactionï¼ˆè‡ªå‹•å£“ç¸®ï¼‰èˆ‡ Optimized Writesï¼ˆå„ªåŒ–å¯«å…¥ï¼‰ä¹‹é–“çš„å€åˆ¥ï¼Œä»¥åŠ Auto Compaction çš„ç›®æ¨™æ–‡ä»¶å¤§å°ã€‚

â€¢ è§£é¡Œé—œéµå­—ï¼š

```
- Auto Compactionï¼šè‡ªå‹•åŒ–çš„å°æ–‡ä»¶åˆä½µæ“ä½œã€‚
- After the write completesï¼šè‡ªå‹•å£“ç¸®çš„è§¸ç™¼æ™‚é–“é»,ã€‚
- Default of 128 MBï¼šAuto Compaction çš„ç›®æ¨™æ–‡ä»¶å¤§å°ï¼ˆèˆ‡æ¨™æº– OPTIMIZE çš„ 1 GB ä¸åŒï¼‰

```

Optimized Writes

æ˜¯æŒ‡åœ¨è³‡æ–™å¯«å…¥ Delta Lake æ™‚ï¼Œç³»çµ±æœƒè‡ªå‹•å°‡è³‡æ–™åˆ†æ‰¹ã€åˆ†å€ä¸¦èª¿æ•´æª”æ¡ˆå¤§å°ï¼Œç›¡é‡å¯«å‡ºæ¥è¿‘ç›®æ¨™å¤§å°ï¼ˆé è¨­ 128 MBï¼‰çš„æª”æ¡ˆã€‚é€™æ¨£å¯ä»¥æ¸›å°‘å°æª”æ¡ˆæ•¸é‡ï¼Œæå‡å¾ŒçºŒæŸ¥è©¢æ•ˆèƒ½ã€‚Optimized Writes å°åˆ†å€è¡¨ç‰¹åˆ¥æœ‰æ•ˆï¼Œå› ç‚ºå®ƒèƒ½æ¸›å°‘æ¯å€‹åˆ†å€ä¸‹çš„å°æª”æ¡ˆæ•¸é‡ã€‚

Auto Compaction

æ˜¯æŒ‡åœ¨è³‡æ–™å¯«å…¥å®Œæˆå¾Œï¼Œç³»çµ±æœƒè‡ªå‹•æª¢æŸ¥åˆ†å€å…§æ˜¯å¦æœ‰éå¤šå°æª”æ¡ˆï¼Œè‹¥æœ‰å‰‡è‡ªå‹•åˆä½µé€™äº›å°æª”æ¡ˆæˆè¼ƒå¤§çš„æª”æ¡ˆï¼ˆé è¨­ç›®æ¨™å¤§å°å¯èª¿æ•´ï¼‰ã€‚é€™å€‹å‹•ä½œç™¼ç”Ÿåœ¨å¯«å…¥æˆåŠŸå¾Œï¼Œåƒ…é‡å°å°šæœªè¢«åˆä½µéçš„å°æª”æ¡ˆé€²è¡Œ ã€‚

å·®ç•°

Optimized Writes ç™¼ç”Ÿåœ¨è³‡æ–™**å¯«å…¥éšæ®µ**ï¼Œç›®çš„æ˜¯ç›¡é‡é¿å…ç”¢ç”Ÿå°æª”æ¡ˆã€‚

Auto Compaction ç™¼ç”Ÿåœ¨**å¯«å…¥å¾Œ**ï¼Œé‡å°å·²ç”¢ç”Ÿçš„å°æª”æ¡ˆé€²è¡Œè‡ªå‹•åˆä½µã€‚

å…©è€…æ­é…å¯æœ‰æ•ˆæ¸›å°‘ Delta Lake è¡¨ä¸­çš„å°æª”æ¡ˆå•é¡Œï¼Œæå‡æ•ˆèƒ½

| **åŠŸèƒ½** | **è§¸ç™¼æ™‚æ©Ÿ** | **ç›®æ¨™æª”æ¡ˆå¤§å°** | **å•Ÿå‹•æ–¹å¼** | **é©ç”¨æƒ…å¢ƒ** | 
|---|---|---|---|---|
| Optimized Writes | å¯«å…¥æ™‚ | 128 MB | è‡ªå‹•/è¨­å®š | é é˜²ç”¢ç”Ÿå°æª”æ¡ˆ | 
| Auto Compaction | å¯«å…¥å¾Œ | 128 MB | è‡ªå‹•/è¨­å®š | åˆä½µå·²ç”¢ç”Ÿçš„å°æª”æ¡ˆ | 
| æ‰‹å‹• OPTIMIZE | ä½¿ç”¨è€…æ‰‹å‹•åŸ·è¡Œ | 1 GB | æ‰‹å‹• | å¤§å‹è¡¨ã€ZORDERã€é€²éšå„ªåŒ– | 

## **Question #23**

Which statement characterizes the general programming model used by Spark Structured Streaming?

ç¿»è­¯ï¼š å“ªä¸€å€‹æ•˜è¿°æè¿°äº† Spark çµæ§‹åŒ–ä¸²æµæ‰€ä½¿ç”¨çš„é€šç”¨ç·¨ç¨‹æ¨¡å‹ï¼Ÿ

A. Structured Streaming leverages the parallel processing of GPUs to achieve highly parallel data throughput.

(çµæ§‹åŒ–ä¸²æµåˆ©ç”¨ GPU çš„å¹³è¡Œè™•ç†ä¾†å¯¦ç¾é«˜åº¦å¹³è¡Œçš„è³‡æ–™ååé‡ã€‚)

B. Structured Streaming is implemented as a messaging bus and is derived from Apache Kafka.

(çµæ§‹åŒ–ä¸²æµæ˜¯ä½œç‚ºä¸€å€‹æ¶ˆæ¯ä½‡åˆ—å¯¦ç¾çš„ï¼Œä¸¦ä¸”æºè‡ª Apache Kafkaã€‚)

C. Structured Streaming uses specialized hardware and I/O streams to achieve sub-second latency for data transfer.

(çµæ§‹åŒ–ä¸²æµä½¿ç”¨å°ˆæ¥­ç¡¬é«”å’Œ I/O ä¸²æµä¾†å¯¦ç¾ä½æ–¼ä¸€ç§’çš„æ•¸æ“šå‚³è¼¸å»¶é²ã€‚)

D. Structured Streaming models new data arriving in a data stream as new rows appended to an unbounded table.

(çµæ§‹åŒ–ä¸²æµå°‡åˆ°é”çš„æ•¸æ“šæµå»ºæ¨¡ç‚ºè¿½åŠ åˆ°ä¸€å€‹ç„¡é™è¡¨ï¼ˆUnbounded Tableï¼‰çš„æ–°è¡Œã€‚)

E. Structured Streaming relies on a distributed network of nodes that hold incremental state values for cached stages.

(çµæ§‹åŒ–ä¸²æµä¾è³´æ–¼ä¸€å€‹åˆ†ä½ˆå¼ç¯€é»ç¶²è·¯ï¼Œè©²ç¶²è·¯ç‚ºå¿«å–çš„éšæ®µä¿ç•™å¢é‡ç‹€æ…‹å€¼ã€‚)

> ç­”æ¡ˆ:

D. Structured Streaming models new data arriving in a data stream as new rows appended to an unbounded table.

> è§£èªª:

æ ¸å¿ƒè€ƒé»ï¼š Spark Structured Streaming çš„åŸºæœ¬æŠ½è±¡æ¨¡å‹ï¼ˆThe Structured Streaming Programming Modelï¼‰ã€‚

â€¢ è§£é¡Œé—œéµå­—ï¼š Structured Streaming çš„åŸºç¤ç†è«–æˆ–ç·¨ç¨‹æ¨¡å‹æ™‚ï¼Œç­”æ¡ˆå¹¾ä¹ç¸½æ˜¯èˆ‡ã€ŒUnbounded Tableã€é€™å€‹æ¦‚å¿µç›¸é—œã€‚ è«‹è¨˜ä½é€™å€‹å¼·çƒˆçš„é—œè¯ï¼š

`Structured Streaming = Continuous Query on an Unbounded Table.`

```
Structured Streaming + programming modelï¼šç•¶çœ‹åˆ°é€™äº›è©å½™æ™‚ï¼Œæ‚¨æ‡‰ç«‹å³è¯æƒ³åˆ° Structured Streaming çš„è¨­è¨ˆç›®æ¨™ï¼šå°‡æ‰¹æ¬¡å’Œä¸²æµè™•ç†çµ±ä¸€ã€‚

unbounded tableï¼šé€™æ˜¯å¯¦ç¾é€™ç¨®çµ±ä¸€çš„æ ¸å¿ƒæ¦‚å¿µ

```

<https://app.heptabase.com/e2b4d180-4e0a-45c6-9989-dea4bc781f8a/card/5a08edf1-2add-440d-a8d3-812290723a8e/tab/a944fd85-e7f9-4165-8770-494828afd578>

é¸é …è©³è§£ (Option Analysis)

â€¢ A. Structured Streaming leverages the parallel processing of GPUs... â—¦

éŒ¯èª¤ã€‚ Structured Streaming é‹è¡Œåœ¨ Spark å¼•æ“ä¸Šï¼Œä¸»è¦ä½¿ç”¨ CPU è³‡æºé€²è¡Œåˆ†ä½ˆå¼è™•ç†ã€‚GPU ä¸¦ä¸æ˜¯å…¶æ ¸å¿ƒç·¨ç¨‹æ¨¡å‹çš„ç‰¹å¾µã€‚

â€¢ B. Structured Streaming is implemented as a messaging bus and is derived from Apache Kafka.

éŒ¯èª¤ã€‚ Structured Streaming æ˜¯ä¸€å€‹è™•ç†å¼•æ“ï¼ˆProcessing Engineï¼‰ï¼Œå®ƒå¯ä»¥ä½¿ç”¨ Apache Kafka ä½œç‚ºæ•¸æ“šæºï¼ˆData Sourceï¼‰ï¼Œä½†å®ƒæœ¬èº«ä¸æ˜¯æ¶ˆæ¯ä½‡åˆ—ï¼ˆMessaging Busï¼‰ã€‚

â€¢ C. Structured Streaming uses specialized hardware...

éŒ¯èª¤ã€‚ Spark çš„è¨­è¨ˆç›®æ¨™æ˜¯é‹è¡Œåœ¨å•†å“ç¡¬é«”ï¼ˆcommodity hardwareï¼‰ä¸Šï¼Œä¸éœ€è¦å°ˆé–€çš„ I/O ä¸²æµæˆ–ç¡¬é«”åŠ é€Ÿä¾†å¯¦ç¾å…¶æ ¸å¿ƒæ¨¡å‹ã€‚

â€¢ D. Structured Streaming models new data arriving in a data stream as new rows appended to an unbounded table.

æ­£ç¢ºã€‚ é€™æ˜¯ Structured Streaming å®˜æ–¹æ–‡ä»¶ä¸­çš„æ¨™æº–æè¿°ã€‚å®ƒå…è¨±ä½¿ç”¨è€…å°‡ä¸²æµè¨ˆç®—è¡¨é”ç‚ºå°é€™å€‹é‚è¼¯ã€Œç„¡é™è¡¨ã€çš„æ¨™æº–æ‰¹æ¬¡æŸ¥è©¢ã€‚

â€¢ E. Structured Streaming relies on a distributed network of nodes that hold incremental state values for cached stages.

éŒ¯èª¤ã€‚ é›–ç„¶ Structured Streaming ç¢ºå¯¦ç¶­è­·ç‹€æ…‹ï¼ˆstateï¼‰ï¼Œä½†é€™æ˜¯å®ƒè™•ç†é‚è¼¯çš„ä¸€éƒ¨åˆ†ï¼ˆä¾‹å¦‚èšåˆå’Œçª—å£æ“ä½œï¼‰ï¼Œè€Œä¸æ˜¯å…¶é€šç”¨çš„ã€Œç·¨ç¨‹æ¨¡å‹ã€æˆ–æŠ½è±¡å®šç¾©ã€‚

## **Question #24**

> é¡Œç›®:

Which configuration parameter directly affects the size of a spark-partition upon ingestion of data into Spark?

ç¿»è­¯ï¼š å“ªä¸€å€‹é…ç½®åƒæ•¸ç›´æ¥å½±éŸ¿äº†æ•¸æ“šè¼‰å…¥ï¼ˆingestionï¼‰åˆ° Spark æ™‚çš„ Spark åˆ†å€å¤§å°ï¼Ÿ

A. spark.sql.files.maxPartitionBytes

B. spark.sql.autoBroadcastJoinThreshold

C. spark.sql.files.openCostInBytes

D. spark.sql.adaptive.coalescePartitions.minPartitionNum

E. spark.sql.adaptive.advisoryPartitionSizeInBytes

> ç­”æ¡ˆ:

A. spark.sql.files.maxPartitionBytes

è§£é‡‹ï¼š æ­¤åƒæ•¸å®šç¾©äº† Spark åœ¨è®€å–æª”æ¡ˆï¼ˆä¾‹å¦‚ Parquet, JSON å’Œ ORC ç­‰æ–‡ä»¶å‹ä¾†æºï¼‰æ™‚ï¼Œå°‡å¤šå°‘æœ€å¤§ä½å…ƒçµ„æ•¸çš„æ•¸æ“šæ‰“åŒ…åˆ°å–®ä¸€åˆ†å€ä¸­ã€‚é€šéèª¿æ•´æ­¤å€¼ï¼Œæ‚¨å¯ä»¥ç›´æ¥æ§åˆ¶æ•¸æ“šé€²å…¥ Spark è™•ç†å¼•æ“æ™‚çš„åˆ†å€æ•¸é‡å’Œå¤§å°

<https://docs.databricks.com/aws/en/sql/language-manual/parameters/max_partition_bytes>

> è§£èªª:

è€ƒé»æ—¨åœ¨æ¸¬è©¦æ‚¨æ˜¯å¦æ¸…æ¥šåœ°çŸ¥é“å¦‚ä½•æ§åˆ¶æ•¸æ“šå¾é›²ç«¯å„²å­˜è¼‰å…¥åˆ° Spark è¨˜æ†¶é«”æ™‚çš„åˆå§‹åˆ†å€æ•¸é‡ã€‚

- è®€å– (Read) vs. å¯«å…¥ (Write) åˆ†å€æ§åˆ¶ï¼š

spark.sql.files.maxPartitionBytes åªæ§åˆ¶ è®€å– éšæ®µçš„åˆ†å€å¤§å°ã€‚

å¦‚æœæ‚¨æƒ³æ§åˆ¶ å¯«å…¥ éšæ®µçš„åˆ†å€æ•¸é‡ï¼ˆä¾‹å¦‚ç‚ºäº†ç¢ºä¿è¼¸å‡ºæª”æ¡ˆå¤§å°åˆç†ï¼‰ï¼Œæ‚¨éœ€è¦ä½¿ç”¨åƒ repartition() æˆ– coalesce() é€™æ¨£çš„ DataFrame è½‰æ›æ“ä½œï¼Œæˆ–è¨­ç½® spark.sql.shuffle.partitionsï¼ˆå½±éŸ¿å¯«å…¥çµæœï¼Œä½†æœƒå°è‡´ Shuffleï¼‰ã€‚

- Shuffle æ˜¯ Spark ä¸­ä¸€ç¨®åœ¨ä¸åŒç¯€é»ä¹‹é–“é‡æ–°åˆ†é…è³‡æ–™çš„éç¨‹ï¼Œé€šå¸¸ç™¼ç”Ÿåœ¨éœ€è¦è·¨åˆ†å€é€²è¡Œèšåˆã€joinã€groupBy ç­‰æ“ä½œæ™‚ã€‚ç•¶é€™äº›æ“ä½œéœ€è¦å°‡åŒä¸€ key çš„è³‡æ–™é›†ä¸­åˆ°åŒä¸€åˆ†å€æ™‚ï¼ŒSpark æœƒå°‡è³‡æ–™å¾å¤šå€‹åŸ·è¡Œå™¨ï¼ˆExecutorï¼‰ä¹‹é–“ç§»å‹•ï¼Œé€™å€‹éç¨‹å°±ç¨±ç‚º Shuffleã€‚

- Shuffle æœƒç”¢ç”Ÿå¤§é‡çš„ç£ç¢Ÿ I/O å’Œç¶²è·¯å‚³è¼¸ï¼Œå› æ­¤æœƒå½±éŸ¿æ•ˆèƒ½ã€‚Spark æä¾›å¤šç¨®åƒæ•¸ï¼ˆå¦‚ spark.sql.shuffle.partitionsï¼‰ä¾†èª¿æ•´ Shuffle è¡Œç‚ºï¼Œä»¥å„ªåŒ–è³‡æºä½¿ç”¨å’ŒæŸ¥è©¢æ•ˆèƒ½

å› æ­¤ï¼Œè¦å„ªåŒ–æ–‡ä»¶è®€å–æ€§èƒ½ï¼Œç¢ºä¿æ¯å€‹æ ¸å¿ƒç²å¾—è¶³å¤ çš„å·¥ä½œé‡ï¼Œèª¿æ•´ spark.sql.files.maxPartitionBytes æ˜¯æœ€ç›´æ¥ä¸”é«˜æ•ˆçš„æ–¹æ³•ï¼Œå› ç‚ºå®ƒæœƒåœ¨æ•¸æ“šè®€å–æ™‚å°±åšå¥½åˆ†å€è¦åŠƒï¼Œå¾è€Œé¿å…ä¸å¿…è¦çš„å¾ŒçºŒæ“ä½œã€‚

æ¯”å–»ï¼š é€™å€‹åƒæ•¸å°±åƒæ‚¨åœ¨å°‡ä¸€å€‹å·¨å¤§çš„æŠ«è–©ï¼ˆæ•¸æ“šé›†ï¼‰åˆ‡ç‰‡ï¼ˆåˆ†å€ï¼‰æ™‚ï¼Œå®šç¾©äº†æ¯ç‰‡æŠ«è–©ï¼ˆæ¯å€‹åˆ†å€ï¼‰çš„æœ€å¤§é‡é‡ï¼Œä»¥ç¢ºä¿æ¯å€‹å“¡å·¥ï¼ˆåŸ·è¡Œå™¨æ ¸å¿ƒï¼‰éƒ½èƒ½æ‹¿åˆ°å¤§å°åˆé©ä¸”å‡è¡¡çš„å·¥ä½œé‡ã€‚

B. spark.sql.autoBroadcastJoinThresholdï¼šéŒ¯èª¤ã€‚æ­¤åƒæ•¸è¨­å®šäº†ç”¨æ–¼å»£æ’­è¯çµ (Broadcast Join) çš„æ•¸æ“šé‡çš„é–¾å€¼ï¼Œèˆ‡è¼¸å…¥åˆ†å€å¤§å°ç„¡é—œã€‚

C. spark.sql.adaptive.advisoryPartitionSizeInBytesï¼šéŒ¯èª¤ã€‚æ­¤åƒæ•¸æ˜¯è‡ªé©æ‡‰æŸ¥è©¢åŸ·è¡Œ (AQE) çš„å»ºè­°åˆ†å€å¤§å°ï¼Œä¸»è¦åœ¨ Shuffle ä¹‹å¾Œç”¨æ–¼åˆä½µå°åˆ†å€ï¼Œè€Œéåœ¨è®€å–æ™‚æ§åˆ¶åˆ†å€å¤§å°ã€‚

D. spark.sql.adaptive.coalescePartitions.minPartitionNumï¼šéŒ¯èª¤ã€‚æ­¤åƒæ•¸æ˜¯ AQE ç”¨æ–¼è¨­å®šåˆ†å€æ•¸é‡ä¸‹é™çš„åƒæ•¸ï¼Œèˆ‡è®€å–æ™‚çš„åˆ†å€å¤§å°ç„¡é—œ



A junior data engineer seeks to leverage Delta Lake's Change Data Feed functionality to create a Type 1 table representing all of the values that have ever been valid for all rows in a bronze table created with the property delta.enableChangeDataFeed = true.

ä¸€ä½åˆç´šè³‡æ–™å·¥ç¨‹å¸«å¸Œæœ›åˆ©ç”¨ Delta Lake çš„ Change Data Feedï¼ˆCDFï¼‰åŠŸèƒ½ï¼Œé‡å°ä¸€å€‹å·²è¨­å®šå±¬æ€§ delta.enableChangeDataFeed = true çš„ bronze è³‡æ–™è¡¨ï¼Œå»ºç«‹ä¸€å€‹ Type 1 è³‡æ–™è¡¨ï¼Œç”¨ä»¥å‘ˆç¾è©²è¡¨æ‰€æœ‰è³‡æ–™åˆ—æ›¾ç¶“æœ‰æ•ˆçš„æ‰€æœ‰å€¼ã€‚

## **Question #28 jing**

They plan to execute the following code as a daily job:

Python

from pyspark.sql.functions import col

([spark.read](spark.read).format("delta")

.option("readChangeFeed", "true")

.option("startingVersion", 0)

.table("bronze")

.filter(col("\_change_type").isin(\["update_postimage", "insert"\]))

.write

.mode("append")

.table("bronze_history_type1")

)

Which statement describes the execution and results of running the above query multiple times?



ç­”æ¡ˆ:B

A.Each time the job is executed, newly updated records will be merged into the target table, overwriting previous values with the same primary keys.

**B. Each time the job is executed, the entire available history of inserted or updated records will be appended to the target table, resulting in many duplicate entries.**

C. Each time the job is executed, the target table will be overwritten using the entire history of inserted or updated records, giving the desired result.

D. Each time the job is executed, the differences between the original and current versions are calculated; this may result in duplicate entries for some records.

E. Each time the job is executed, only those records that have been inserted or updated since the last execution will be appended to the target table, giving the desired result.



è§£é¡Œ:

A. æ¯æ¬¡åŸ·è¡Œä½œæ¥­æ™‚ï¼Œæ–°çš„æ›´æ–°è¨˜éŒ„å°‡æœƒåˆä½µåˆ°ç›®æ¨™è³‡æ–™è¡¨ä¸­ï¼Œä¸¦ä»¥ç›¸åŒä¸»éµè¦†è“‹å…ˆå‰çš„å€¼ã€‚

éŒ¯èª¤: æ–°æ›´æ–°è¨˜éŒ„æœƒè¢«åˆä½µè¦†å¯«ç›¸åŒä¸»éµ

B. æ¯æ¬¡åŸ·è¡Œä½œæ¥­æ™‚ï¼Œæ‰€æœ‰å¯ç”¨çš„æ–°å¢æˆ–æ›´æ–°è¨˜éŒ„çš„æ•´å€‹æ­·å²éƒ½æœƒé™„åŠ åˆ°ç›®æ¨™è³‡æ–™è¡¨ï¼Œå°è‡´æœ‰è¨±å¤šé‡è¤‡çš„è³‡æ–™ã€‚

æ­£ç¢º:æ¯æ¬¡åŸ·è¡Œéƒ½æœƒæŠŠæ‰€æœ‰æ’å…¥æˆ–æ›´æ–°çš„å¯ç”¨æ­·å²è¿½åŠ åˆ°ç›®æ¨™è¡¨ï¼Œå°è‡´è¨±å¤šé‡è¤‡

C. æ¯æ¬¡åŸ·è¡Œä½œæ¥­æ™‚ï¼Œç›®æ¨™è³‡æ–™è¡¨å°‡æœƒè¢«æ‰€æœ‰æ–°å¢æˆ–æ›´æ–°è¨˜éŒ„çš„å®Œæ•´æ­·å²è¦†è“‹ï¼Œå¾è€Œç”¢ç”Ÿé æœŸçš„çµæœã€‚

éŒ¯èª¤:æ¯æ¬¡éƒ½ overwrite ç›®æ¨™è¡¨

D. æ¯æ¬¡åŸ·è¡Œä½œæ¥­æ™‚ï¼Œæœƒè¨ˆç®—åŸå§‹ç‰ˆæœ¬å’Œç›®å‰ç‰ˆæœ¬ä¹‹é–“çš„å·®ç•°ï¼›é€™å¯èƒ½æœƒå°è‡´æŸäº›è¨˜éŒ„å‡ºç¾é‡è¤‡ã€‚

éŒ¯èª¤:CDFæœƒæä¾›è®Šæ›´äº‹ä»¶ï¼ˆå« preimage/postimageï¼‰ï¼Œä½†ç¨‹å¼ç¢¼åªéæ¿¾ update_postimage å’Œ insertï¼Œä¸¦æ²’æœ‰åšå·®ç•°è¨ˆç®—é‚è¼¯ï¼›æ­¤æ•˜è¿°èˆ‡ç¨‹å¼ä¸ç¬¦ã€‚

E. æ¯æ¬¡åŸ·è¡Œä½œæ¥­æ™‚ï¼Œåªæœ‰è‡ªä¸Šæ¬¡åŸ·è¡Œä»¥ä¾†æ–°å¢æˆ–æ›´æ–°çš„è¨˜éŒ„æœƒè¢«é™„åŠ åˆ°ç›®æ¨™è³‡æ–™è¡¨ï¼Œå¾è€Œç”¢ç”Ÿé æœŸçš„çµæœã€‚

éŒ¯èª¤:ç¨‹å¼ç¢¼æ²’æœ‰è¨˜éŒ„ã€Œä¸Šæ¬¡åŸ·è¡Œä½ç½®ã€ä¹Ÿæ²’æœ‰ endingVersionæˆ– readStream + checkpointï¼Œå› æ­¤ä¸æœƒåªè¿½åŠ ã€Œè‡ªä¸Šæ¬¡å¾Œçš„è®Šæ›´ã€ï¼Œè€Œæ˜¯æ¯æ¬¡éƒ½é‡é ­è®€ã€‚



**è€ƒé»åˆ†æ (Question Analysis)**

â€¢æ ¸å¿ƒè€ƒé»ï¼šDelta Lake çš„ Change Data Feedï¼ˆCDFï¼‰åœ¨æ‰¹æ¬¡æ¨¡å¼ä¸‹ä»¥Â startingVersionÂ è®€å–çš„è¡Œç‚ºï¼Œä»¥åŠå¯«å…¥æ¨¡å¼ï¼ˆappend/overwrite/MERGEï¼‰çš„å½±éŸ¿ã€‚

â€¢è§£é¡Œé—œéµå­—ï¼š

â€¢readChangeFeedÂ +Â startingVersion=0Â â†’ ä»£è¡¨å¾è³‡æ–™è¡¨çš„ç¬¬ 0 ç‰ˆé–‹å§‹è®€æ•´æ®µ CDF æ­·å²ï¼Œæ¯æ¬¡é‡è·‘éƒ½æœƒé‡è¤‡è®€å…¨é‡ã€‚

â€¢\_change_typeÂ ç‚ºÂ update_postimageã€insertÂ â†’ åªæ“·å–æ›´æ–°å¾Œå½±åƒèˆ‡æ’å…¥äº‹ä»¶ï¼Œä¸åšå·®ç•°è¨ˆç®—æˆ–åˆä½µã€‚

â€¢.mode("append")Â â†’ ç´”è¿½åŠ ï¼Œä¸è¦†è“‹ã€ä¸å»é‡ã€ä¸åˆä½µä¸»éµã€‚

è£œå…… **\_change_type** : insert,delete, update_preimage (æ›´æ–°å‰çš„æ•¸å€¼) , update_postimage(æ›´æ–°å¾Œçš„æ•¸å€¼)

 **ç°¡æ˜“è§£é¡Œæ€è·¯ (Logic Path)**

â€¢é¡Œç›®ç¨‹å¼ç¢¼è¨­å®šÂ startingVersion=0ï¼Œè¡¨ç¤ºæ¯æ¬¡åŸ·è¡Œéƒ½æœƒå¾ Delta è¡¨çš„æœ€åˆç‰ˆæœ¬é–‹å§‹è®€å– CDF å…¨éƒ¨è®Šæ›´ã€‚

â€¢ç”±æ–¼å¯«å…¥ä½¿ç”¨Â .mode("append")ï¼Œæ²’æœ‰Â MERGEÂ æˆ–å»é‡é‚è¼¯ï¼Œå› æ­¤æ¯æ¬¡é‡è·‘éƒ½æœƒæŠŠåŒä¸€æ‰¹æ­·å²è³‡æ–™å†è¿½åŠ ä¸€æ¬¡ã€‚

â€¢æ’é™¤æœƒè¦†è“‹æˆ–åªè®€å¢é‡çš„æ•˜è¿°ï¼Œç­”æ¡ˆè‡ªç„¶è½åœ¨ã€Œæ¯æ¬¡éƒ½å…¨é‡é‡è¤‡è¿½åŠ åˆ°ç›®æ¨™è¡¨ï¼Œå°è‡´å¤§é‡é‡è¤‡ã€çš„é¸é …ã€‚

**é—œéµçŸ¥è­˜æ¸…å–® (Key Concepts Checklist)**

â€¢Delta Lake Change Data Feedï¼ˆCDFï¼‰ï¼šç‚º Delta è¡¨æä¾›è®Šæ›´äº‹ä»¶ä¸²æµï¼ŒåŒ…å«Â \_change_typeã€\_commit_versionã€\_commit_timestampÂ ç­‰æ¬„ä½ã€‚

<https://docs.databricks.com/aws/en/delta/delta-change-data-feed>

â€¢startingVersionÂ /Â endingVersionï¼šåœ¨æ‰¹æ¬¡è®€ CDF æ™‚ç•Œå®šè®€å–çš„ç‰ˆæœ¬ç¯„åœï¼›å›ºå®šå¾ 0 é–‹å§‹ç­‰æ–¼æ¯æ¬¡éƒ½è®€å…¨é‡æ­·å²ã€‚

â€¢\_change_typeï¼ˆupdate_preimage / update_postimage / insert / deleteï¼‰ï¼šç”¨æ–¼æŒ‡ç¤ºè®Šæ›´äº‹ä»¶ç¨®é¡ï¼›Type 1 é€šå¸¸ç”¨Â update_postimageÂ èˆ‡Â insertÂ å–å¾—ã€Œæ›´æ–°å¾Œã€å€¼ã€‚

â€¢SCD Type 1 vs Type 2ï¼šType 1 è¦†è“‹èˆŠå€¼ä¸ç•™æ­·å²ï¼›Type 2 ä¿ç•™æ¯æ¬¡è®Šæ›´çš„æœ‰æ•ˆç‰ˆæœ¬èˆ‡æ­·å²ã€‚

â€¢MERGEï¼ˆUpsertï¼‰ï¼šä»¥ä¸»éµåˆä½µæ–°èˆŠè³‡æ–™ï¼Œå¸¸ç”¨æ–¼ Type 1 è¦†è“‹è¡Œç‚ºï¼›å–®ç´”Â appendÂ ä¸æœƒå»é‡æˆ–è¦†è“‹ã€‚



## **Question #29 jing**

A new data engineer notices that a critical field was omitted from an application that writes its Kafka source to Delta Lake.

This happened even though the critical field was in the Kafka source.

That field was further missing from data written to dependent, long-term storage.

The retention threshold on the Kafka service is seven days. The pipeline has been in production for three months.

Which describes how Delta Lake can help to avoid data loss of this nature in the future?

ç­”æ¡ˆ:E

A.The Delta log and Structured Streaming checkpoints record the full history of the Kafka producer.

B. Delta Lake schema evolution can retroactively calculate the correct value for newly added fields, as long as the data was in the original source.

C. Delta Lake automatically checks that all fields present in the source data are included in the ingestion layer.

D. Data can never be permanently dropped or deleted from Delta Lake, so data loss is not possible under any circumstance.

**E. Ingesting all raw data and metadata from Kafka to a bronze Delta table creates a permanent, replayable history of the data state.**



è§£é¡Œ:

A. Delta log èˆ‡ Structured Streaming çš„ checkpoints æœƒè¨˜éŒ„ Kafka producer çš„å®Œæ•´æ­·å²ã€‚

éŒ¯èª¤ï¼šDelta log è¨˜çš„æ˜¯ç›®æ¨™ Delta è¡¨çš„äº¤æ˜“ç´€éŒ„ï¼Œä¸æ˜¯å¤–éƒ¨ Kafka çš„è¨Šæ¯æ­·å²ï¼›Structured Streaming çš„ checkpoint åªå„²å­˜é€²åº¦ï¼ˆoffset/ç‹€æ…‹ï¼‰ï¼Œä¸ä¿å­˜å®Œæ•´ä¾†æºè³‡æ–™ã€‚Kafka éäº†ä¿ç•™æœŸï¼Œç„¡æ³•é  checkpoint å›è£œåŸè¨Šæ¯ã€‚

B. Delta Lake çš„ schema evolution èƒ½å›æº¯è¨ˆç®—æ–°åŠ å…¥æ¬„ä½çš„æ­£ç¢ºå€¼ï¼Œåªè¦è©²è³‡æ–™åœ¨åŸå§‹ä¾†æºä¸­å­˜åœ¨

éŒ¯èª¤ï¼šSchema evolution åªèƒ½ã€Œæ¥å— schema è®Šæ›´ï¼ˆä¾‹å¦‚æ–°å¢æ¬„ä½ï¼‰ã€ä¸¦æ­£ç¢ºå¯«å…¥æ–°åˆ°ä¾†çš„è³‡æ–™ï¼›ç„¡æ³•ã€Œäº‹å¾Œæ¨å›ã€éå»å·²éºæ¼ä¸”æœªä¿å­˜çš„åŸå§‹æ¬„ä½å€¼ã€‚è‹¥ç•¶æ™‚æ²’æŠŠæ¬„ä½å¯«é€² Deltaï¼Œå°±ç„¡æ³•è‡ªå‹•å›å¾©ã€‚

C. Delta Lake æœƒè‡ªå‹•æª¢æŸ¥ä¾†æºè³‡æ–™ä¸­æ‰€æœ‰æ¬„ä½æ˜¯å¦éƒ½å·²åŒ…å«åœ¨ingestion layerä¸­ã€‚

éŒ¯èª¤ï¼šé€™å…¶å¯¦æ˜¯åœ¨æš—ç¤ºä¸€ç¨®ã€Œä¾†æºæ¬„ä½å®Œæ•´æ€§è‡ªå‹•é©—è­‰ã€çš„èƒ½åŠ›ã€‚

Delta Lake å…§æœ€æ¥è¿‘ã€Œæª¢æŸ¥ã€çš„æ©Ÿåˆ¶æ˜¯ã€ŒSchema enforcementã€ã€‚Delta çš„ Schema enforcement åªæœƒæª¢æŸ¥ã€Œå¯«å…¥è³‡æ–™æ˜¯å¦ç¬¦åˆç›®æ¨™è¡¨ schemaã€ï¼Œä¸æœƒè‡ªå‹•æ ¸å°ã€Œä¾†æºæœ‰å“ªäº›æ¬„ä½æ˜¯å¦éƒ½æœ‰ ingestã€ã€‚

D. åœ¨ Delta Lake ä¸­è³‡æ–™æ°¸é ä¸æœƒè¢«æ°¸ä¹…ä¸Ÿæ£„æˆ–åˆªé™¤ï¼Œå› æ­¤åœ¨ä»»ä½•æƒ…æ³ä¸‹éƒ½ä¸å¯èƒ½ç™¼ç”Ÿè³‡æ–™éºå¤±ã€‚

éŒ¯èª¤ï¼šDelta æ”¯æ´ DELETE/UPDATE/MERGEï¼Œä¸” VACUUM æœƒæ¸…ç†èˆŠæª”ã€‚è‹¥è³‡æ–™æ ¹æœ¬æœªè¢«å¯«å…¥ Deltaï¼Œæ›´è«‡ä¸ä¸Šé€é Delta æ‰¾å›ã€‚Delta ä¸¦éã€Œæ°¸ä¸å¯åˆªã€ã€‚

E. å°‡ Kafka çš„æ‰€æœ‰åŸå§‹è³‡æ–™èˆ‡ä¸­ç¹¼è³‡æ–™ï¼ˆmetadataï¼‰æ”å–åˆ° Bronze Delta è¡¨ï¼ˆbronze Delta tableï¼‰ï¼Œå¯å»ºç«‹æ°¸ä¹…ã€å¯é‡æ”¾ï¼ˆreplayableï¼‰çš„è³‡æ–™ç‹€æ…‹æ­·å²ã€‚

æ­£ç¢º: é€™æ˜¯ Databricks æ¨è–¦çš„ Medallionï¼ˆBronze/Silver/Goldï¼‰æ¶æ§‹ã€‚å°‡ Kafka çš„ keyã€valueã€headersã€topicã€partitionã€offsetã€timestamp ç­‰ã€ŒåŸå§‹è³‡æ–™ + ä¸­ç¹¼è³‡æ–™ã€ä»¥ Append æ–¹å¼å¯«å…¥ Bronze Delta è¡¨ï¼Œå½¢æˆæ°¸ä¹…ã€å¯é‡æ”¾çš„æ­·å²ã€‚æ—¥å¾Œè‹¥ç™¼ç¾æ¬„ä½éºæ¼æˆ–è½‰æ›é‚è¼¯æœ‰èª¤ï¼Œå¯å¾ Bronze é‡ç®— Silver/Goldï¼Œå®Œå…¨ä¸å— Kafka 7 å¤©ä¿ç•™æœŸé™åˆ¶ã€‚



1\.**è€ƒé»åˆ†æ (Question Analysis)**

â€¢æ ¸å¿ƒè€ƒé»ï¼šDelta Lake åœ¨ä¸²æµè³‡æ–™ç®¡ç·šä¸­çš„æœ€ä½³å¯¦å‹™â€”â€”ä»¥ Bronze/Silver/Gold æ¶æ§‹å…ˆè¡Œè½åœ°ã€ŒåŸå§‹ã€ä¸å¯è®Šã€è³‡æ–™ï¼Œè—‰æ­¤è„«é‰¤ä¾†æºç³»çµ±çš„ä¿ç•™æœŸé™ä¸¦æ”¯æ´å¾ŒçºŒé‡æ”¾èˆ‡é‡ç®—ã€‚

â€¢è§£é¡Œé—œéµå­—ï¼š

â€¢ã€ŒKafka retention 7 daysã€â†’ è¦è¯æƒ³åˆ°ï¼šä¾†æºç„¡æ³•é•·æœŸå›æ”¾ï¼Œå› æ­¤å¿…é ˆåœ¨è³‡æ–™æ¹–ä¸­å»ºç«‹é•·æœŸã€å¯é‡æ”¾çš„ã€ŒåŸå§‹äº‹å¯¦ã€ã€‚

â€¢ã€Œæ¬„ä½åœ¨ Kafka æœ‰ã€ä½†å¯«å…¥ Delta/é•·æœŸå„²å­˜æ™‚æ¼æ‰ã€â†’ è¦è¯æƒ³åˆ°ï¼šéœ€è¦ä¿å­˜ã€ŒåŸå§‹ payload + metadataã€åˆ° Bronzeï¼Œä¹‹å¾Œå¯é‡è£½ä¸‹æ¸¸ã€‚

â€¢ã€ŒDelta Lake é¿å…è³‡æ–™éºå¤±ã€â†’ è¦è¯æƒ³åˆ°ï¼šBronze åŸå§‹è½åœ° + æ™‚é–“æ—…è¡Œï¼ˆTime Travelï¼‰åªèƒ½å¹«å·²åœ¨ Delta çš„ç‰ˆæœ¬ï¼Œä¸èƒ½å¹«æœªè½åœ°çš„ä¾†æºè³‡æ–™ã€‚

2\. **ç°¡æ˜“è§£é¡Œæ€è·¯ (Logic Path)**

â€¢éœ€æ±‚ï¼šä¾†æº Kafka æœ‰æ¬„ä½ï¼Œä½†å› å¯«å…¥ç¨‹å¼éºæ¼ï¼Œä¸” Kafka åªä¿ç•™ 7 å¤©ï¼Œä¸‰å€‹æœˆå¾Œç„¡æ³•å†å¾ Kafka å›æ”¾è£œæ•‘ã€‚

â€¢èƒ½åŠ›å°æ‡‰ï¼šè¦é¿å…æœªä¾†å†æ¬¡ç™¼ç”Ÿï¼Œå”¯ä¸€å¯è¡Œæ˜¯ã€Œå…ˆæŠŠ Kafka åŸå§‹è³‡æ–™ï¼ˆkey/value/headers/offset ç­‰ï¼‰å®Œæ•´è½åœ°åˆ° Delta Bronzeã€ï¼Œä½¿ä¹‹æˆç‚ºæ°¸ä¹…ã€å¯é‡æ”¾çš„çœŸå¯¦ä¾†æºã€‚

â€¢æ’é™¤ï¼šDelta log/Checkpoint ä¸æœƒä¿å­˜ Kafka çš„å®Œæ•´æ­·å²ï¼›Schema evolution ä¸èƒ½å›æ¨èˆŠè³‡æ–™å€¼ï¼›Delta ä¹Ÿéä¸å¯åˆªé™¤ï¼›Delta ä¸æœƒè‡ªå‹•æ ¸å°ä¾†æºæ¬„ä½ã€‚

**3\. é—œéµçŸ¥è­˜æ¸…å–® (Key Concepts Checklist)**

â€¢Bronze/Silver/Goldï¼ˆMedallion æ¶æ§‹ï¼‰ï¼šBronze å„²å­˜åŸå§‹è³‡æ–™ï¼ŒSilver åšæ¸…æ´—/è½‰æ›ï¼ŒGold æä¾›æœ€çµ‚å•†æ¥­å–ç”¨ã€‚

â€¢Structured Streamingï¼ˆçµæ§‹åŒ–ä¸²æµï¼‰ï¼šä»¥ç„¡é™è¡¨æ¦‚å¿µè™•ç†ä¸²æµï¼›checkpoint åªå­˜é€²åº¦èˆ‡ç‹€æ…‹ï¼Œä¸å­˜å®Œæ•´ä¾†æºè¨Šæ¯ã€‚

â€¢Delta Lake Transaction Logï¼ˆDelta æ—¥èªŒï¼‰ï¼šè¨˜éŒ„å° Delta è¡¨çš„ ACID äº¤æ˜“ï¼Œéå¤–éƒ¨ä¾†æºçš„å®Œæ•´æ­·å²ã€‚

â€¢Schema Enforcement vs. Schema Evolutionï¼šå‰è€…é©—è­‰å¯«å…¥æ˜¯å¦ç¬¦åˆ schemaï¼›å¾Œè€…å…è¨± schema è®Šæ›´ï¼ˆå¦‚æ–°å¢æ¬„ä½ï¼‰ï¼Œä½†ä¸æœƒå›æ¨æ­·å²å€¼ã€‚

â€¢Time Travelï¼ˆæ™‚é–“æ—…è¡Œï¼‰ï¼šå¯æŸ¥è©¢ Delta è¡¨çš„æ­·å²ç‰ˆæœ¬ï¼Œä½†åªèƒ½å°ã€Œå·²åœ¨ Deltaã€çš„è³‡æ–™ç”Ÿæ•ˆï¼Œç„¡æ³•å¾©åŸå¾æœªè½åœ°çš„ä¾†æºè³‡æ–™ã€‚

## **Question #30 jing**

A nightly job ingests data into a Delta Lake table using the following code:



Python

from pyspark.sql.functions import current_timestamp, input_file_name, col

from pyspark.sql.column import Column

def ingest_daily_batch(time_col: Column, year: int, month: int, day: int):

Â Â Â  ([spark.read](spark.read)

Â Â Â Â  .format("parquet")

Â Â Â Â  .load(f"/mnt/daily_batch/{year}/{month}/{day}")

Â Â Â Â  .withColumn("ingest_time", time_col)

Â Â Â Â  .withColumn("source_file", input_file_name())

Â Â Â Â  .write

Â Â Â Â  .mode("append")

Â Â Â Â  .saveAsTable("bronze")

Â Â Â  )

The next step in the pipeline requires a function that returns an object that can be used to manipulate new records that have not yet been processed to the next table in the pipeline.

Which code snippet completes this function definition?



A.**return spark.readStream.table(â€œbronzeâ€)**Â  (ç¤¾ç¾¤ç­”æ¡ˆ)\
B. return spark.readStream.load("bronze")\
C. return ([spark.read](spark.read).table("bronze").filter(col("ingest_time") == current_timestamp()))\
D. return [spark.read](spark.read).option(â€œreadChangeFeedâ€, â€œtrueâ€).table(â€œbronzeâ€) (å®˜æ–¹ç­”æ¡ˆ)\
E. return ([spark.read](spark.read).table(â€œbronzeâ€).filter(col(â€œsource_fileâ€) == fâ€œ/mnt/daily_batch/{year}/{month}/{day}â€)) (ç¤¾ç¾¤ç­”æ¡ˆ)



è§£é¡Œ:

Aæ­£ç¢ºï¼šé€™æ˜¯ Databricks ä¸­è®€å– Delta Table ä¸¦å»ºç«‹ Structured Streaming DataFrame çš„æ¨™æº–åšæ³•ã€‚

åªæœ‰é€™ç¨®æ–¹å¼èƒ½æŒçºŒç›£æ§æ–°é€²è³‡æ–™ï¼Œä¸¦é€²ä¸€æ­¥ ETL è™•ç†åˆ°ä¸‹ä¸€å±¤è¡¨ï¼ˆå¦‚ Silverï¼‰ã€‚

é€™ä¹Ÿæ˜¯å®˜æ–¹æ¨è–¦åœ¨ ETL Pipeline ä¸­ Bronze â†’ Silver è½‰æ›æ™‚è™•ç†æ–°è³‡æ–™çš„æ–¹å¼ã€‚

B.éŒ¯èª¤:load("bronze") åªèƒ½ç”¨æ–¼è·¯å¾‘ï¼ˆä¾‹å¦‚ "/mnt/bronze"ï¼‰ï¼Œä¸èƒ½ç›´æ¥ç”¨ Delta Table åç¨±ã€‚

C.éŒ¯èª¤: é€™æ˜¯ã€Œæ‰¹æ¬¡è®€å–ã€è€Œéä¸²æµã€‚

åªæœƒä¸€æ¬¡æ€§è®€å–è³‡æ–™ï¼Œç„¡æ³•æŒçºŒç›£æ§æ–°é€²è³‡æ–™ã€‚

å¦å¤–ï¼Œingest_time == current_timestamp() å¹¾ä¹ä¸æœƒæœ‰è³‡æ–™ï¼Œå› ç‚º ingest_time æ˜¯æ‰¹æ¬¡åŸ·è¡Œæ™‚çš„æ™‚é–“ï¼Œè·Ÿ current_timestamp() å¹¾ä¹ç„¡æ³•å°ä¸Šã€‚

D.éŒ¯èª¤: readChangeFeed æ˜¯ç”¨æ–¼æŸ¥è©¢ Delta Lake Change Data Feedï¼ˆCDFï¼‰ï¼Œå¸¸è¦‹æ–¼éœ€è¿½è¹¤è³‡æ–™è®Šå‹•çš„å ´æ™¯ï¼ˆå¦‚ CDCï¼‰ã€‚

ä½†é€™æ˜¯æ‰¹æ¬¡è®€å–ï¼Œä¸æ˜¯ Structured Streamingï¼Œç„¡æ³•æŒçºŒè™•ç†æ–°è³‡æ–™ã€‚

é¡Œç›®è¦çš„æ˜¯ã€Œå°šæœªè™•ç†çš„æ–°è³‡æ–™ã€çš„æŒçºŒç›£æ§ï¼Œæ‡‰ç”¨æƒ…å¢ƒä¸åŒã€‚

EéŒ¯èª¤:é€™ä¹Ÿæ˜¯æ‰¹æ¬¡è®€å–ï¼Œåªæœƒç¯©å‡ºç‰¹å®šä¾†æºæª”æ¡ˆçš„è³‡æ–™ã€‚\
ä¸ç¬¦ã€ŒæŒçºŒè™•ç†æ–°é€²è³‡æ–™ã€çš„å ´æ™¯ã€‚\
ETL Pipeline ä¸­é€šå¸¸æ˜¯è¦æŒçºŒç›£æ§æ‰€æœ‰æ–°è³‡æ–™ï¼Œä¸åªæ˜¯æŸä¸€å¤©æˆ–æŸå€‹æª”æ¡ˆã€‚



**1\. è€ƒé»åˆ†æ (Question Analysis)**

â€¢**æ ¸å¿ƒè€ƒé»**ï¼š\
é€™é¡Œåœ¨è€ƒã€Œå¦‚ä½•å¾ Delta Lake Bronze è¡¨å–å¾—å°šæœªè™•ç†çš„æ–°è³‡æ–™ï¼Œä¸¦ç”¨ Structured Streaming é€²è¡Œå¾ŒçºŒ ETLã€ã€‚

â€¢**è§£é¡Œé—œéµå­—**ï¼š

â€¢readStreamÂ â†’ è¦è¯æƒ³åˆ°ã€ŒStructured Streamingï¼ˆçµæ§‹åŒ–ä¸²æµï¼‰ã€ï¼ŒæŒçºŒç›£æ§æ–°è³‡æ–™ã€‚

â€¢table("bronze")Â â†’ ä»£è¡¨è³‡æ–™ä¾†æºæ˜¯ Delta Tableï¼ˆè€Œä¸”å¾ˆå¯èƒ½æ˜¯ ETL pipeline çš„ Bronze Layerï¼‰ã€‚

â€¢manipulate new records that have not yet been processedÂ â†’ è¦èƒ½ã€ŒæŒçºŒå–å¾—å°šæœªè¢«ç§»åˆ°ä¸‹ä¸€å€‹è¡¨çš„æ–°è³‡æ–™ã€ï¼Œé€šå¸¸ç”¨ Structured Streaming å¯¦ç¾ã€‚

**2\. ç°¡æ˜“è§£é¡Œæ€è·¯ (Logic Path)**

é¡Œç›®æè¿° ETL Pipeline ç›®å‰åœ¨ Bronze è¡¨ï¼Œä¸‹ä¸€æ­¥éœ€è¦ä¸€å€‹ã€Œèƒ½æ“ä½œå°šæœªè™•ç†çš„æ–°è³‡æ–™ã€çš„ç‰©ä»¶ã€‚\
åœ¨ Databricksï¼Œè‹¥è¦æŒçºŒå–å¾—æ–°è³‡æ–™ï¼Œæ­£ç¢ºåšæ³•æ˜¯ç”¨Â spark.readStream.table("bronze")Â ç”¢ç”Ÿ Structured Streaming DataFrameã€‚\
å…¶ä»–é¸é …è¦å˜›æ˜¯æ‰¹æ¬¡è®€å–ï¼Œè¦å˜›éæ¿¾é‚è¼¯æœ‰èª¤ï¼Œéƒ½ä¸ç¬¦åˆã€ŒæŒçºŒå–å¾—å°šæœªè™•ç†è³‡æ–™ã€çš„éœ€æ±‚ã€‚\
å› æ­¤ç­”æ¡ˆæ˜¯é¸é … Aã€‚

**3\. é—œéµçŸ¥è­˜æ¸…å–® (Key Concepts Checklist)**

â€¢Delta Lake Bronze Tableï¼šè³‡æ–™æ¹– ETL Pipeline çš„ç¬¬ä¸€å±¤ï¼Œé€šå¸¸ç”¨ä¾†å„²å­˜åŸå§‹è³‡æ–™ï¼Œæ–¹ä¾¿å¾ŒçºŒåŠ å·¥ã€‚

â€¢Structured Streamingï¼ˆçµæ§‹åŒ–ä¸²æµï¼‰ï¼šDatabricks/Spark ç”¨æ–¼æŒçºŒè™•ç†æ–°è³‡æ–™çš„ä¸²æµæ¡†æ¶ï¼Œæ”¯æ´èˆ‡ Delta Table æ•´åˆã€‚

â€¢spark.readStream.tableï¼šç›´æ¥ç”¨ Structured Streaming è®€å– Delta Tableï¼ŒæŒçºŒå–å¾—æ–°é€²è³‡æ–™ï¼Œæ˜¯è™•ç† Bronze â†’ Silver è½‰æ›çš„æœ€ä½³å¯¦å‹™ã€‚

â€¢Change Data Feed (CDF)ï¼šDelta Lake æä¾›çš„åŠŸèƒ½ï¼Œå¯è¿½è¹¤è¡¨çš„æ–°å¢ã€ä¿®æ”¹ã€åˆªé™¤è¨˜éŒ„ï¼Œå¸¸ç”¨æ–¼ CDCï¼Œä½†ä¸æ˜¯ä¸²æµã€‚

â€¢æ‰¹æ¬¡è®€å– vs ä¸²æµè®€å–ï¼šæ‰¹æ¬¡è®€å–åªå–å¾—ä¸€æ¬¡è³‡æ–™ï¼Œä¸²æµè®€å–èƒ½æŒçºŒç›£æ§æ–°è³‡æ–™ï¼ŒETL Pipeline ä¸­è¦èƒ½æŒçºŒè™•ç†æ–°è³‡æ–™é€šå¸¸éœ€ç”¨ä¸²æµã€‚



**Question #43 kuohaolu**

###  è€ƒé»åˆ†æ (Question Analysis)

æ ¸å¿ƒè€ƒé»ï¼š

é€™é¡Œåœ¨è€ƒã€Œå¦‚ä½•é€é Databricks SQL æŒ‡ä»¤æª¢æŸ¥ table çš„ metadataï¼ˆåŒ…å«æ¬„ä½è¨»è§£ã€è¡¨è¨»è§£ã€table propertiesï¼‰ã€ã€‚

è§£é¡Œé—œéµå­—ï¼š

column commentsã€table comments â†’ è¯æƒ³åˆ° DESCRIBE EXTENDED å¯ä»¥çœ‹åˆ°æ‰€æœ‰ commentã€‚

TBLPROPERTIES â†’ è¯æƒ³åˆ° SHOW TBLPROPERTIES æˆ– DESCRIBE EXTENDEDã€‚

manual confirmation â†’ å¼·èª¿è¦ã€Œä¸€æ¬¡çœ‹åˆ°æ‰€æœ‰ metadataã€çš„èªæ³•ã€‚

### ç°¡æ˜“è§£é¡Œæ€è·¯ (Logic Path)

é¡Œç›®è¦æ±‚ã€Œæ‰‹å‹•ç¢ºèªã€ä¸‰é … metadataï¼šcolumn commentã€table commentã€TBLPROPERTIESï¼ˆcontains_piiï¼‰ã€‚

éœ€è¦æ‰¾ä¸€å€‹ SQL æŒ‡ä»¤èƒ½ã€ŒåŒæ™‚ã€çœ‹åˆ°é€™ä¸‰è€…ã€‚

DESCRIBE EXTENDED æœƒé¡¯ç¤ºæ¬„ä½è¨»è§£ã€è¡¨è¨»è§£å’Œ TBLPROPERTIESï¼›å…¶ä»–é¸é …è¦å˜›åªèƒ½æŸ¥æŸä¸€éƒ¨ä»½ï¼ˆå¦‚ SHOW TBLPROPERTIESï¼‰ï¼Œè¦å˜›æŸ¥ä¸åˆ° commentã€‚

æ‰€ä»¥æœ€å…¨é¢ã€æœ€ç²¾ç¢ºçš„æŒ‡ä»¤æ˜¯ Aã€‚

### é—œéµçŸ¥è­˜æ¸…å–® (Key Concepts Checklist)

DESCRIBE EXTENDEDï¼šæŸ¥è©¢è¡¨ï¼ˆæˆ– viewï¼‰æ¬„ä½ç´°ç¯€ã€è¨»è§£ã€table comment åŠ TBLPROPERTIES çš„æŒ‡ä»¤ã€‚

TBLPROPERTIESï¼šè‡ªè¨‚å±¬æ€§ï¼Œå¸¸ç”¨æ–¼è¨˜éŒ„é¡å¤–æ¥­å‹™è³‡è¨Šï¼ˆå¦‚ contains_piiï¼‰ï¼Œå¯ç”¨æ–¼æ²»ç†æ”¿ç­–ã€‚

COMMENTï¼ˆtable/column commentï¼‰ï¼šè³‡æ–™è¡¨èˆ‡æ¬„ä½çš„å‚™è¨»/æè¿°ï¼Œç”¨æ–¼èªªæ˜æ•æ„Ÿè³‡æ–™æˆ–å…¶ä»–æ¥­å‹™å®šç¾©ã€‚

Data Governanceï¼ˆè³‡æ–™æ²»ç†ï¼‰ï¼šé‡å°è³‡æ–™å®‰å…¨ã€éš±ç§ã€åˆè¦ç­‰è¨­ç«‹çš„ç®¡ç†è¦ç¯„èˆ‡æŠ€è¡“è½å¯¦ã€‚

SHOW TBLPROPERTIESï¼šåƒ…æŸ¥è©¢è¡¨çš„è‡ªè¨‚å±¬æ€§ï¼ˆä¸å«æ¬„ä½èˆ‡è¡¨è¨»è§£ï¼‰ã€‚

**Question #44 kuohaolu**

### è€ƒé»åˆ†æ (Question Analysis)

æ ¸å¿ƒè€ƒé»ï¼š

é€™é¡Œåœ¨è€ƒã€ŒDelta Lake çš„è³‡æ–™åˆªé™¤ï¼ˆDELETEï¼‰ã€ACID ä¿è­‰èˆ‡ Time Travel åŠç‰©ç†æª”æ¡ˆç§»é™¤ï¼ˆVACUUMï¼‰ã€ä¹‹é–“çš„é—œä¿‚ï¼Œç‰¹åˆ¥æ˜¯ GDPR åˆè¦ä¸‹çš„è³‡æ–™ä¸å¯å›å¾©æ€§ã€‚

è§£é¡Œé—œéµå­—ï¼š

Delta Lake â†’ è¦è¯æƒ³åˆ° ACIDã€æ™‚é–“æ—…è¡Œï¼ˆTime Travelï¼‰ã€VACUUMã€‚

GDPR â†’ è¯æƒ³åˆ°æ°¸ä¹…åˆªé™¤ã€ä¸å¯å›å¾©æ€§ã€åˆè¦æ€§è¦æ±‚ã€‚

DELETE + accessible â†’ è¯æƒ³åˆ°é‚è¼¯åˆªé™¤ vs ç‰©ç†æª”æ¡ˆç§»é™¤ã€‚

### 2\. ç°¡æ˜“è§£é¡Œæ€è·¯ (Logic Path)

é¡Œç›®å•ã€ŒåŸ·è¡Œ DELETEã€å¾Œï¼Œè³‡æ–™æ˜¯å¦ã€Œä¸å†å¯å­˜å–ã€ã€‚

åœ¨ Delta Lakeï¼ŒDELETE åªæ˜¯ã€Œé‚è¼¯åˆªé™¤ã€ï¼šè³‡æ–™ä¸æœƒå‡ºç¾åœ¨æœ€æ–°ç‰ˆæœ¬ï¼Œä½†èˆŠç‰ˆè³‡æ–™æª”æ¡ˆï¼ˆæ¯”å¦‚ Parquet filesï¼‰é‚„åœ¨å„²å­˜å±¤ã€‚

åªè¦æ²’æœ‰åŸ·è¡Œ VACUUMï¼Œå°±å¯ä»¥ç”¨ Time Travel åŠŸèƒ½æŸ¥è©¢éå»çš„ç‰ˆæœ¬ï¼Œä¾ç„¶èƒ½çœ‹åˆ°å·²åˆªé™¤çš„è³‡æ–™ï¼Œä¸ç¬¦ GDPRã€Œä¸å¯å­˜å–ã€çš„è¦æ±‚ã€‚

åªæœ‰åŸ·è¡Œ VACUUM å¾Œï¼ŒDelta Lake æ‰æœƒæŠŠé€™äº›èˆŠæª”æ¡ˆæ°¸ä¹…ç§»é™¤ï¼Œå®Œå…¨ç„¡æ³•è¢«æŸ¥è©¢ã€‚

### é—œéµçŸ¥è­˜æ¸…å–® (Key Concepts Checklist)

Delta Lake DELETEï¼šåŸ·è¡Œæ™‚æœƒå°‡è³‡æ–™åœ¨æœ€æ–°ç‰ˆæœ¬ä¸­æ¨™è¨˜ç‚ºå·²åˆªé™¤ï¼Œä½†åŸå§‹è³‡æ–™æª”æ¡ˆä»ä¿ç•™æ–¼å„²å­˜å±¤ã€‚

Time Travelï¼ˆæ™‚é–“æ—…è¡Œï¼‰ï¼šå…è¨±æŸ¥è©¢ Delta Table çš„éå»ç‰ˆæœ¬ï¼ŒèˆŠç‰ˆæª”æ¡ˆåœ¨æœªè¢« VACUUM å‰çš†å¯æŸ¥è©¢ã€‚

VACUUMï¼šDelta Lake æŒ‡ä»¤ï¼Œç”¨ä¾†æ°¸ä¹…ç§»é™¤ç„¡æ•ˆçš„èˆŠè³‡æ–™æª”æ¡ˆï¼ŒåŸ·è¡Œå¾Œé€™äº›è³‡æ–™æ‰ç„¡æ³•å†è¢«æŸ¥è©¢ã€‚

ACID Guaranteesï¼šä¿è­‰è³‡æ–™æ“ä½œçš„åŸå­æ€§ã€ä¸€è‡´æ€§ã€éš”é›¢æ€§ã€æŒä¹…æ€§ï¼Œä½†èˆ‡ç‰©ç†åˆªé™¤ä¸ç­‰åŒã€‚

GDPR åˆè¦ï¼šé‡å°å€‹è³‡å¿…é ˆåšåˆ°å¾¹åº•åˆªé™¤èˆ‡ä¸å¯å›å¾©ï¼Œå…‰æ˜¯é‚è¼¯åˆªé™¤ï¼ˆå¦‚ DELETEï¼‰é€šå¸¸ä¸å¤ ï¼Œéœ€ç¢ºä¿ç‰©ç†æª”æ¡ˆè¢«ç§»é™¤ã€‚



**Question #45 kuohaolu**

### è€ƒé»åˆ†æ (Question Analysis)

æ ¸å¿ƒè€ƒé»ï¼š

é€™é¡Œåœ¨è€ƒã€ŒDatabase çš„ LOCATION å¦‚ä½•å½±éŸ¿ Table å¯¦é«”ä½ç½®ã€ä»¥åŠã€ŒManaged Table èˆ‡ External Table çš„å·®ç•°ã€ã€‚

è§£é¡Œé—œéµå­—ï¼š

CREATE DATABASE ... LOCATION '/mnt/finance_eda_bucket' â†’ Database è¢«è¨­å®šåœ¨ç‰¹å®šç›®éŒ„ï¼Œå½±éŸ¿é è¨­ table ä½ç½®ã€‚

CREATE TABLE ... AS SELECT ...ï¼Œä¸” æœªæŒ‡å®š LOCATION â†’ é è¨­æœƒå»ºç«‹ Managed Tableï¼Œä¸”æ”¾åœ¨ database çš„è·¯å¾‘ä¸‹ã€‚

mounted storage â†’ è¡¨ç¤ºé€™å€‹è·¯å¾‘æ˜¯å¤–éƒ¨ç‰©ä»¶å„²å­˜ï¼Œä½† Databricks ä»è¦–ç‚ºå—ç®¡è¡¨ï¼ˆmetadata èˆ‡è³‡æ–™åŒæ™‚ç®¡ç†ï¼‰ã€‚

### ç°¡æ˜“è§£é¡Œæ€è·¯ (Logic Path)

å»ºç«‹ database æ™‚æŒ‡å®šäº† LOCATIONï¼Œå› æ­¤è©² database çš„æ‰€æœ‰ã€ŒæœªæŒ‡å®š LOCATIONã€çš„ managed table éƒ½æœƒå­˜åˆ°é€™å€‹ç›®éŒ„ã€‚

CREATE TABLE ... AS SELECTï¼Œæ²’ç‰¹åˆ¥åŠ  LOCATIONï¼Œå°±æ˜¯å»ºç«‹å—ç®¡è¡¨ï¼ˆmanaged tableï¼‰ã€‚

Databricks æœƒå°‡è³‡æ–™å¯¦é«”å­˜åˆ° /mnt/finance_eda_bucket/tx_sales ä¹‹é¡çš„è·¯å¾‘ï¼Œä½† metadata ç”± metastore ç®¡æ§ï¼Œæ‰€ä»¥ä»å±¬æ–¼ managed tableã€‚

External Table å¿…é ˆåœ¨ CREATE TABLE æ™‚æ˜ç¢ºæŒ‡å®š LOCATIONï¼›æœ¬é¡Œæ²’æœ‰æŒ‡å®šï¼Œæ‰€ä»¥ä¸æ˜¯ external tableã€‚

æ ¹æ“šä¸Šè¿°é‚è¼¯ï¼Œç­”æ¡ˆé¸ Dã€‚

### é—œéµçŸ¥è­˜æ¸…å–® (Key Concepts Checklist)

Managed Tableï¼ˆå—ç®¡è¡¨ï¼‰ï¼šç”± metastore ç®¡ç†è³‡æ–™èˆ‡ metadataï¼ŒæœªæŒ‡å®š LOCATION æ™‚é è¨­å»ºç«‹ï¼Œåˆªè¡¨æœƒé€£è³‡æ–™ä¸€èµ·åˆªã€‚

External Tableï¼ˆå¤–éƒ¨è¡¨ï¼‰ï¼šéœ€åœ¨ CREATE TABLE æŒ‡å®š LOCATIONï¼Œè³‡æ–™èˆ‡ metadata åˆ†é–‹ç®¡ç†ï¼Œåˆªè¡¨ä¸æœƒç§»é™¤å¯¦é«”è³‡æ–™ã€‚

Database LOCATIONï¼šè¨­å®š database æ™‚çš„è·¯å¾‘ï¼Œæ‰€æœ‰ä¸æŒ‡å®š LOCATION çš„ managed table éƒ½æœƒå­˜æ–¼æ­¤è·¯å¾‘ã€‚

Mount Pointï¼ˆæ›è¼‰é»ï¼‰ï¼šå°‡é›²ç«¯å„²å­˜æ¡¶æ›è¼‰åˆ° Databricks æª”æ¡ˆç³»çµ±ï¼Œè®“è·¯å¾‘å¦‚ /mnt/... å¯ç›´æ¥å­˜å–ã€‚

CREATE TABLE AS SELECTï¼šæ ¹æ“šæŸ¥è©¢çµæœå»ºç«‹æ–°è¡¨ï¼Œæ²’æŒ‡å®š LOCATION å°±æ˜¯ managed tableã€‚